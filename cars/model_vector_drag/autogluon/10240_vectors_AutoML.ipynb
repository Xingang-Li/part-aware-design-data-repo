{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: ./10240_vectors_drags.csv | Columns = 10243 / 10243 | Rows = 439 -> 439\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_10232</th>\n",
       "      <th>dim_10233</th>\n",
       "      <th>dim_10234</th>\n",
       "      <th>dim_10235</th>\n",
       "      <th>dim_10236</th>\n",
       "      <th>dim_10237</th>\n",
       "      <th>dim_10238</th>\n",
       "      <th>dim_10239</th>\n",
       "      <th>dim_10240</th>\n",
       "      <th>drag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dim_1  dim_2  dim_3  dim_4  dim_5  dim_6  dim_7  dim_8  dim_9  dim_10  \\\n",
       "61     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "354    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "358    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "275    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "18     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "\n",
       "     ...  dim_10232  dim_10233  dim_10234  dim_10235  dim_10236  dim_10237  \\\n",
       "61   ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "354  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "358  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "275  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "18   ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "     dim_10238  dim_10239  dim_10240   drag  \n",
       "61         0.0        0.0        0.0  0.375  \n",
       "354        0.0        0.0        0.0  0.374  \n",
       "358        0.0        0.0        0.0  0.435  \n",
       "275        0.0        0.0        0.0  0.437  \n",
       "18         0.0        0.0        0.0  0.367  \n",
       "\n",
       "[5 rows x 10241 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#surrogate models\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_file = './10240_vectors_drags.csv'\n",
    "df = TabularDataset(data_file)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=777)\n",
    "\n",
    "#exclue the first two columns of train data\n",
    "train_data = train_df.drop(columns=['i', 'name'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count    351.000000\n",
      "mean       0.398513\n",
      "std        0.060013\n",
      "min        0.278000\n",
      "25%        0.353000\n",
      "50%        0.394000\n",
      "75%        0.435000\n",
      "max        0.598000\n",
      "Name: drag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label = 'drag'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./agModels-10240\"\n",
      "Presets specified: ['best_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': 'True',\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 3,\n",
      " 'num_stack_levels': 3,\n",
      " 'verbosity': 4}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': 'True',\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': 3,\n",
      " 'num_stack_levels': 3,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 4}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=5, num_bag_sets=3\n",
      "Saving ./agModels-10240/learner.pkl\n",
      "Saving ./agModels-10240/predictor.pkl\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"./agModels-10240/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #51-Ubuntu SMP Mon Jul 4 06:41:22 UTC 2022\n",
      "Train Data Rows:    351\n",
      "Train Data Columns: 10240\n",
      "Label Column: drag\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (0.598, 0.278, 0.39851, 0.06001)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    250808.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 28.75 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4199 features to boolean dtype as they only contain 2 unique values.\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('int8', 'int') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\t13.6s = Fit runtime\n",
      "\t\t\t4199 features in original data used to generate 4199 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('int8', 'int') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\t1.5s = Fit runtime\n",
      "\t\t\t4199 features in original data used to generate 4199 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('int8', 'int') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\t1.0s = Fit runtime\n",
      "\t\t\t4199 features in original data used to generate 4199 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('int8', 'int') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t\t\t1.4s = Fit runtime\n",
      "\t\t\t4199 features in original data used to generate 4199 features in processed data.\n",
      "\tUseless Original Features (Count: 6041): ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', 'dim_6', 'dim_7', 'dim_8', 'dim_9', 'dim_10', 'dim_11', 'dim_12', 'dim_13', 'dim_14', 'dim_15', 'dim_16', 'dim_17', 'dim_18', 'dim_19', 'dim_20', 'dim_21', 'dim_22', 'dim_23', 'dim_24', 'dim_25', 'dim_26', 'dim_27', 'dim_28', 'dim_29', 'dim_30', 'dim_31', 'dim_32', 'dim_33', 'dim_34', 'dim_35', 'dim_36', 'dim_37', 'dim_38', 'dim_39', 'dim_40', 'dim_41', 'dim_42', 'dim_43', 'dim_44', 'dim_45', 'dim_46', 'dim_47', 'dim_48', 'dim_49', 'dim_50', 'dim_51', 'dim_52', 'dim_53', 'dim_54', 'dim_55', 'dim_56', 'dim_57', 'dim_58', 'dim_59', 'dim_60', 'dim_61', 'dim_62', 'dim_63', 'dim_64', 'dim_65', 'dim_66', 'dim_67', 'dim_68', 'dim_69', 'dim_70', 'dim_71', 'dim_72', 'dim_73', 'dim_74', 'dim_75', 'dim_76', 'dim_77', 'dim_78', 'dim_79', 'dim_80', 'dim_81', 'dim_82', 'dim_83', 'dim_84', 'dim_85', 'dim_86', 'dim_87', 'dim_88', 'dim_89', 'dim_90', 'dim_91', 'dim_92', 'dim_93', 'dim_94', 'dim_95', 'dim_96', 'dim_97', 'dim_98', 'dim_99', 'dim_100', 'dim_101', 'dim_102', 'dim_103', 'dim_106', 'dim_107', 'dim_108', 'dim_109', 'dim_110', 'dim_111', 'dim_112', 'dim_113', 'dim_114', 'dim_115', 'dim_116', 'dim_117', 'dim_118', 'dim_119', 'dim_120', 'dim_122', 'dim_123', 'dim_124', 'dim_125', 'dim_126', 'dim_127', 'dim_128', 'dim_129', 'dim_130', 'dim_131', 'dim_132', 'dim_134', 'dim_135', 'dim_136', 'dim_139', 'dim_140', 'dim_141', 'dim_142', 'dim_143', 'dim_144', 'dim_145', 'dim_146', 'dim_147', 'dim_148', 'dim_151', 'dim_152', 'dim_155', 'dim_156', 'dim_157', 'dim_158', 'dim_159', 'dim_160', 'dim_161', 'dim_162', 'dim_163', 'dim_168', 'dim_171', 'dim_172', 'dim_173', 'dim_174', 'dim_175', 'dim_176', 'dim_177', 'dim_178', 'dim_179', 'dim_183', 'dim_184', 'dim_187', 'dim_188', 'dim_189', 'dim_190', 'dim_191', 'dim_192', 'dim_193', 'dim_194', 'dim_195', 'dim_196', 'dim_198', 'dim_203', 'dim_204', 'dim_205', 'dim_206', 'dim_207', 'dim_208', 'dim_209', 'dim_210', 'dim_211', 'dim_212', 'dim_219', 'dim_220', 'dim_221', 'dim_222', 'dim_223', 'dim_224', 'dim_225', 'dim_226', 'dim_227', 'dim_228', 'dim_234', 'dim_235', 'dim_236', 'dim_237', 'dim_238', 'dim_239', 'dim_240', 'dim_241', 'dim_242', 'dim_243', 'dim_244', 'dim_245', 'dim_247', 'dim_248', 'dim_250', 'dim_251', 'dim_252', 'dim_253', 'dim_254', 'dim_255', 'dim_256', 'dim_257', 'dim_258', 'dim_259', 'dim_260', 'dim_261', 'dim_263', 'dim_264', 'dim_266', 'dim_267', 'dim_268', 'dim_269', 'dim_270', 'dim_271', 'dim_272', 'dim_273', 'dim_274', 'dim_275', 'dim_276', 'dim_277', 'dim_278', 'dim_279', 'dim_280', 'dim_281', 'dim_282', 'dim_283', 'dim_284', 'dim_285', 'dim_286', 'dim_287', 'dim_288', 'dim_289', 'dim_290', 'dim_291', 'dim_292', 'dim_293', 'dim_294', 'dim_295', 'dim_296', 'dim_297', 'dim_298', 'dim_299', 'dim_300', 'dim_301', 'dim_302', 'dim_303', 'dim_304', 'dim_305', 'dim_306', 'dim_307', 'dim_308', 'dim_309', 'dim_310', 'dim_311', 'dim_312', 'dim_313', 'dim_314', 'dim_315', 'dim_316', 'dim_317', 'dim_318', 'dim_319', 'dim_320', 'dim_321', 'dim_322', 'dim_323', 'dim_324', 'dim_325', 'dim_326', 'dim_327', 'dim_328', 'dim_329', 'dim_330', 'dim_331', 'dim_332', 'dim_333', 'dim_334', 'dim_335', 'dim_336', 'dim_337', 'dim_338', 'dim_339', 'dim_340', 'dim_341', 'dim_342', 'dim_343', 'dim_344', 'dim_345', 'dim_346', 'dim_347', 'dim_348', 'dim_349', 'dim_350', 'dim_351', 'dim_352', 'dim_353', 'dim_354', 'dim_355', 'dim_356', 'dim_357', 'dim_358', 'dim_359', 'dim_360', 'dim_361', 'dim_362', 'dim_363', 'dim_364', 'dim_365', 'dim_366', 'dim_367', 'dim_368', 'dim_369', 'dim_370', 'dim_371', 'dim_372', 'dim_373', 'dim_374', 'dim_375', 'dim_376', 'dim_377', 'dim_378', 'dim_379', 'dim_380', 'dim_381', 'dim_382', 'dim_383', 'dim_384', 'dim_385', 'dim_386', 'dim_387', 'dim_388', 'dim_389', 'dim_390', 'dim_391', 'dim_392', 'dim_393', 'dim_394', 'dim_395', 'dim_396', 'dim_397', 'dim_398', 'dim_399', 'dim_400', 'dim_401', 'dim_402', 'dim_403', 'dim_404', 'dim_405', 'dim_406', 'dim_407', 'dim_408', 'dim_409', 'dim_412', 'dim_413', 'dim_414', 'dim_415', 'dim_416', 'dim_417', 'dim_418', 'dim_419', 'dim_420', 'dim_421', 'dim_423', 'dim_424', 'dim_425', 'dim_426', 'dim_427', 'dim_428', 'dim_429', 'dim_430', 'dim_431', 'dim_432', 'dim_433', 'dim_434', 'dim_435', 'dim_436', 'dim_438', 'dim_439', 'dim_440', 'dim_441', 'dim_442', 'dim_443', 'dim_444', 'dim_445', 'dim_446', 'dim_447', 'dim_448', 'dim_449', 'dim_450', 'dim_451', 'dim_452', 'dim_453', 'dim_454', 'dim_457', 'dim_458', 'dim_459', 'dim_460', 'dim_461', 'dim_462', 'dim_463', 'dim_464', 'dim_465', 'dim_466', 'dim_467', 'dim_469', 'dim_474', 'dim_475', 'dim_476', 'dim_477', 'dim_478', 'dim_479', 'dim_480', 'dim_481', 'dim_482', 'dim_483', 'dim_491', 'dim_492', 'dim_493', 'dim_494', 'dim_495', 'dim_496', 'dim_497', 'dim_498', 'dim_499', 'dim_500', 'dim_501', 'dim_502', 'dim_503', 'dim_504', 'dim_505', 'dim_507', 'dim_508', 'dim_509', 'dim_510', 'dim_511', 'dim_512', 'dim_513', 'dim_514', 'dim_515', 'dim_516', 'dim_517', 'dim_518', 'dim_519', 'dim_520', 'dim_521', 'dim_522', 'dim_523', 'dim_524', 'dim_525', 'dim_526', 'dim_527', 'dim_528', 'dim_529', 'dim_530', 'dim_531', 'dim_532', 'dim_533', 'dim_534', 'dim_535', 'dim_536', 'dim_537', 'dim_538', 'dim_539', 'dim_540', 'dim_541', 'dim_542', 'dim_543', 'dim_544', 'dim_545', 'dim_546', 'dim_547', 'dim_548', 'dim_549', 'dim_550', 'dim_551', 'dim_552', 'dim_553', 'dim_554', 'dim_555', 'dim_556', 'dim_557', 'dim_558', 'dim_559', 'dim_560', 'dim_561', 'dim_562', 'dim_563', 'dim_564', 'dim_565', 'dim_566', 'dim_567', 'dim_568', 'dim_569', 'dim_570', 'dim_571', 'dim_572', 'dim_573', 'dim_574', 'dim_575', 'dim_576', 'dim_577', 'dim_578', 'dim_579', 'dim_580', 'dim_581', 'dim_582', 'dim_583', 'dim_584', 'dim_585', 'dim_586', 'dim_587', 'dim_588', 'dim_589', 'dim_590', 'dim_591', 'dim_592', 'dim_593', 'dim_594', 'dim_595', 'dim_596', 'dim_597', 'dim_598', 'dim_599', 'dim_600', 'dim_601', 'dim_602', 'dim_603', 'dim_604', 'dim_605', 'dim_606', 'dim_607', 'dim_608', 'dim_609', 'dim_610', 'dim_611', 'dim_612', 'dim_613', 'dim_614', 'dim_615', 'dim_616', 'dim_617', 'dim_618', 'dim_619', 'dim_620', 'dim_621', 'dim_622', 'dim_623', 'dim_624', 'dim_625', 'dim_626', 'dim_627', 'dim_628', 'dim_629', 'dim_630', 'dim_631', 'dim_632', 'dim_633', 'dim_634', 'dim_635', 'dim_636', 'dim_637', 'dim_638', 'dim_639', 'dim_640', 'dim_641', 'dim_642', 'dim_643', 'dim_644', 'dim_645', 'dim_646', 'dim_647', 'dim_648', 'dim_649', 'dim_650', 'dim_651', 'dim_652', 'dim_653', 'dim_654', 'dim_655', 'dim_656', 'dim_657', 'dim_658', 'dim_659', 'dim_660', 'dim_661', 'dim_662', 'dim_663', 'dim_664', 'dim_665', 'dim_666', 'dim_667', 'dim_668', 'dim_669', 'dim_670', 'dim_671', 'dim_672', 'dim_673', 'dim_674', 'dim_675', 'dim_676', 'dim_677', 'dim_678', 'dim_679', 'dim_680', 'dim_681', 'dim_682', 'dim_683', 'dim_684', 'dim_685', 'dim_686', 'dim_687', 'dim_688', 'dim_689', 'dim_690', 'dim_691', 'dim_692', 'dim_693', 'dim_694', 'dim_696', 'dim_697', 'dim_698', 'dim_699', 'dim_700', 'dim_701', 'dim_702', 'dim_703', 'dim_704', 'dim_705', 'dim_706', 'dim_707', 'dim_708', 'dim_709', 'dim_713', 'dim_714', 'dim_715', 'dim_716', 'dim_718', 'dim_719', 'dim_720', 'dim_721', 'dim_722', 'dim_723', 'dim_724', 'dim_725', 'dim_731', 'dim_734', 'dim_735', 'dim_736', 'dim_737', 'dim_738', 'dim_739', 'dim_740', 'dim_750', 'dim_751', 'dim_752', 'dim_753', 'dim_754', 'dim_764', 'dim_765', 'dim_766', 'dim_767', 'dim_768', 'dim_769', 'dim_770', 'dim_780', 'dim_781', 'dim_782', 'dim_783', 'dim_784', 'dim_785', 'dim_786', 'dim_796', 'dim_797', 'dim_798', 'dim_799', 'dim_800', 'dim_801', 'dim_802', 'dim_812', 'dim_813', 'dim_814', 'dim_815', 'dim_816', 'dim_817', 'dim_818', 'dim_828', 'dim_829', 'dim_830', 'dim_831', 'dim_832', 'dim_833', 'dim_844', 'dim_845', 'dim_846', 'dim_847', 'dim_848', 'dim_849', 'dim_850', 'dim_860', 'dim_861', 'dim_862', 'dim_863', 'dim_864', 'dim_865', 'dim_866', 'dim_876', 'dim_877', 'dim_878', 'dim_879', 'dim_880', 'dim_881', 'dim_882', 'dim_883', 'dim_892', 'dim_893', 'dim_894', 'dim_895', 'dim_896', 'dim_897', 'dim_898', 'dim_899', 'dim_908', 'dim_909', 'dim_910', 'dim_911', 'dim_912', 'dim_913', 'dim_914', 'dim_915', 'dim_924', 'dim_925', 'dim_926', 'dim_927', 'dim_928', 'dim_929', 'dim_930', 'dim_931', 'dim_940', 'dim_941', 'dim_942', 'dim_943', 'dim_944', 'dim_945', 'dim_946', 'dim_947', 'dim_956', 'dim_957', 'dim_958', 'dim_959', 'dim_960', 'dim_961', 'dim_962', 'dim_963', 'dim_972', 'dim_973', 'dim_974', 'dim_975', 'dim_976', 'dim_977', 'dim_978', 'dim_979', 'dim_988', 'dim_989', 'dim_990', 'dim_991', 'dim_992', 'dim_993', 'dim_994', 'dim_995', 'dim_1004', 'dim_1005', 'dim_1006', 'dim_1007', 'dim_1008', 'dim_1009', 'dim_1010', 'dim_1011', 'dim_1020', 'dim_1021', 'dim_1022', 'dim_1023', 'dim_1024', 'dim_1025', 'dim_1026', 'dim_1027', 'dim_1036', 'dim_1037', 'dim_1038', 'dim_1039', 'dim_1040', 'dim_1041', 'dim_1042', 'dim_1043', 'dim_1053', 'dim_1054', 'dim_1055', 'dim_1056', 'dim_1057', 'dim_1058', 'dim_1059', 'dim_1068', 'dim_1069', 'dim_1070', 'dim_1071', 'dim_1072', 'dim_1073', 'dim_1074', 'dim_1075', 'dim_1083', 'dim_1084', 'dim_1085', 'dim_1086', 'dim_1087', 'dim_1088', 'dim_1089', 'dim_1090', 'dim_1091', 'dim_1099', 'dim_1100', 'dim_1101', 'dim_1102', 'dim_1103', 'dim_1104', 'dim_1105', 'dim_1106', 'dim_1115', 'dim_1116', 'dim_1117', 'dim_1118', 'dim_1119', 'dim_1120', 'dim_1121', 'dim_1122', 'dim_1131', 'dim_1132', 'dim_1133', 'dim_1134', 'dim_1135', 'dim_1136', 'dim_1137', 'dim_1147', 'dim_1148', 'dim_1149', 'dim_1150', 'dim_1151', 'dim_1152', 'dim_1153', 'dim_1154', 'dim_1163', 'dim_1164', 'dim_1165', 'dim_1166', 'dim_1167', 'dim_1168', 'dim_1169', 'dim_1170', 'dim_1179', 'dim_1180', 'dim_1181', 'dim_1182', 'dim_1183', 'dim_1184', 'dim_1185', 'dim_1186', 'dim_1195', 'dim_1196', 'dim_1197', 'dim_1198', 'dim_1199', 'dim_1200', 'dim_1201', 'dim_1202', 'dim_1203', 'dim_1210', 'dim_1211', 'dim_1212', 'dim_1213', 'dim_1214', 'dim_1215', 'dim_1216', 'dim_1217', 'dim_1218', 'dim_1219', 'dim_1220', 'dim_1225', 'dim_1226', 'dim_1227', 'dim_1228', 'dim_1229', 'dim_1230', 'dim_1231', 'dim_1232', 'dim_1233', 'dim_1234', 'dim_1235', 'dim_1236', 'dim_1237', 'dim_1238', 'dim_1239', 'dim_1240', 'dim_1241', 'dim_1242', 'dim_1243', 'dim_1244', 'dim_1245', 'dim_1246', 'dim_1247', 'dim_1248', 'dim_1249', 'dim_1250', 'dim_1251', 'dim_1252', 'dim_1253', 'dim_1254', 'dim_1255', 'dim_1256', 'dim_1257', 'dim_1258', 'dim_1259', 'dim_1260', 'dim_1261', 'dim_1262', 'dim_1263', 'dim_1264', 'dim_1265', 'dim_1266', 'dim_1267', 'dim_1268', 'dim_1269', 'dim_1270', 'dim_1271', 'dim_1272', 'dim_1273', 'dim_1274', 'dim_1275', 'dim_1276', 'dim_1277', 'dim_1278', 'dim_1279', 'dim_1280', 'dim_1281', 'dim_1282', 'dim_1283', 'dim_1284', 'dim_1285', 'dim_1286', 'dim_1287', 'dim_1288', 'dim_1289', 'dim_1290', 'dim_1291', 'dim_1292', 'dim_1293', 'dim_1294', 'dim_1295', 'dim_1296', 'dim_1297', 'dim_1298', 'dim_1299', 'dim_1300', 'dim_1301', 'dim_1302', 'dim_1303', 'dim_1304', 'dim_1305', 'dim_1306', 'dim_1307', 'dim_1308', 'dim_1309', 'dim_1310', 'dim_1311', 'dim_1312', 'dim_1313', 'dim_1314', 'dim_1315', 'dim_1316', 'dim_1317', 'dim_1318', 'dim_1320', 'dim_1321', 'dim_1322', 'dim_1323', 'dim_1324', 'dim_1325', 'dim_1326', 'dim_1327', 'dim_1328', 'dim_1329', 'dim_1330', 'dim_1331', 'dim_1332', 'dim_1333', 'dim_1339', 'dim_1342', 'dim_1343', 'dim_1344', 'dim_1345', 'dim_1346', 'dim_1347', 'dim_1348', 'dim_1349', 'dim_1355', 'dim_1358', 'dim_1359', 'dim_1360', 'dim_1361', 'dim_1362', 'dim_1363', 'dim_1374', 'dim_1375', 'dim_1376', 'dim_1377', 'dim_1378', 'dim_1379', 'dim_1390', 'dim_1391', 'dim_1392', 'dim_1393', 'dim_1394', 'dim_1406', 'dim_1407', 'dim_1408', 'dim_1409', 'dim_1422', 'dim_1423', 'dim_1424', 'dim_1438', 'dim_1439', 'dim_1440', 'dim_1442', 'dim_1454', 'dim_1455', 'dim_1456', 'dim_1458', 'dim_1470', 'dim_1471', 'dim_1472', 'dim_1473', 'dim_1486', 'dim_1487', 'dim_1488', 'dim_1489', 'dim_1501', 'dim_1502', 'dim_1503', 'dim_1504', 'dim_1505', 'dim_1518', 'dim_1519', 'dim_1520', 'dim_1521', 'dim_1522', 'dim_1534', 'dim_1535', 'dim_1536', 'dim_1537', 'dim_1538', 'dim_1539', 'dim_1550', 'dim_1551', 'dim_1552', 'dim_1553', 'dim_1554', 'dim_1555', 'dim_1566', 'dim_1567', 'dim_1568', 'dim_1569', 'dim_1570', 'dim_1571', 'dim_1582', 'dim_1583', 'dim_1584', 'dim_1585', 'dim_1586', 'dim_1587', 'dim_1598', 'dim_1599', 'dim_1600', 'dim_1601', 'dim_1602', 'dim_1603', 'dim_1614', 'dim_1615', 'dim_1616', 'dim_1617', 'dim_1618', 'dim_1619', 'dim_1630', 'dim_1631', 'dim_1632', 'dim_1633', 'dim_1634', 'dim_1635', 'dim_1646', 'dim_1647', 'dim_1648', 'dim_1649', 'dim_1650', 'dim_1651', 'dim_1661', 'dim_1662', 'dim_1663', 'dim_1664', 'dim_1665', 'dim_1666', 'dim_1667', 'dim_1677', 'dim_1678', 'dim_1679', 'dim_1680', 'dim_1681', 'dim_1682', 'dim_1683', 'dim_1693', 'dim_1694', 'dim_1695', 'dim_1696', 'dim_1697', 'dim_1698', 'dim_1699', 'dim_1709', 'dim_1710', 'dim_1711', 'dim_1712', 'dim_1713', 'dim_1714', 'dim_1715', 'dim_1724', 'dim_1725', 'dim_1726', 'dim_1727', 'dim_1728', 'dim_1729', 'dim_1730', 'dim_1731', 'dim_1740', 'dim_1741', 'dim_1742', 'dim_1743', 'dim_1744', 'dim_1745', 'dim_1746', 'dim_1756', 'dim_1757', 'dim_1758', 'dim_1759', 'dim_1760', 'dim_1761', 'dim_1771', 'dim_1772', 'dim_1773', 'dim_1774', 'dim_1775', 'dim_1776', 'dim_1787', 'dim_1788', 'dim_1789', 'dim_1790', 'dim_1791', 'dim_1792', 'dim_1803', 'dim_1804', 'dim_1805', 'dim_1806', 'dim_1807', 'dim_1808', 'dim_1819', 'dim_1820', 'dim_1821', 'dim_1822', 'dim_1823', 'dim_1824', 'dim_1825', 'dim_1835', 'dim_1836', 'dim_1837', 'dim_1838', 'dim_1839', 'dim_1840', 'dim_1841', 'dim_1842', 'dim_1851', 'dim_1852', 'dim_1853', 'dim_1854', 'dim_1855', 'dim_1856', 'dim_1857', 'dim_1858', 'dim_1859', 'dim_1867', 'dim_1868', 'dim_1869', 'dim_1870', 'dim_1871', 'dim_1872', 'dim_1873', 'dim_1874', 'dim_1875', 'dim_1876', 'dim_1882', 'dim_1883', 'dim_1884', 'dim_1885', 'dim_1886', 'dim_1887', 'dim_1888', 'dim_1889', 'dim_1890', 'dim_1891', 'dim_1892', 'dim_1893', 'dim_1894', 'dim_1895', 'dim_1896', 'dim_1897', 'dim_1898', 'dim_1899', 'dim_1900', 'dim_1901', 'dim_1902', 'dim_1903', 'dim_1904', 'dim_1905', 'dim_1906', 'dim_1907', 'dim_1908', 'dim_1909', 'dim_1910', 'dim_1911', 'dim_1912', 'dim_1913', 'dim_1914', 'dim_1915', 'dim_1916', 'dim_1917', 'dim_1918', 'dim_1919', 'dim_1920', 'dim_1921', 'dim_1922', 'dim_1923', 'dim_1924', 'dim_1925', 'dim_1926', 'dim_1927', 'dim_1928', 'dim_1929', 'dim_1930', 'dim_1931', 'dim_1932', 'dim_1933', 'dim_1934', 'dim_1935', 'dim_1936', 'dim_1937', 'dim_1938', 'dim_1939', 'dim_1940', 'dim_1941', 'dim_1942', 'dim_1943', 'dim_1944', 'dim_1945', 'dim_1946', 'dim_1947', 'dim_1948', 'dim_1949', 'dim_1950', 'dim_1951', 'dim_1952', 'dim_1953', 'dim_1954', 'dim_1955', 'dim_1956', 'dim_1957', 'dim_1958', 'dim_1960', 'dim_1961', 'dim_1962', 'dim_1963', 'dim_1964', 'dim_1965', 'dim_1966', 'dim_1967', 'dim_1968', 'dim_1969', 'dim_1970', 'dim_1971', 'dim_1972', 'dim_1973', 'dim_1979', 'dim_1980', 'dim_1981', 'dim_1982', 'dim_1983', 'dim_1984', 'dim_1985', 'dim_1986', 'dim_1987', 'dim_1988', 'dim_1998', 'dim_1999', 'dim_2000', 'dim_2001', 'dim_2002', 'dim_2014', 'dim_2015', 'dim_2016', 'dim_2017', 'dim_2018', 'dim_2030', 'dim_2031', 'dim_2032', 'dim_2033', 'dim_2034', 'dim_2048', 'dim_2049', 'dim_2057', 'dim_2064', 'dim_2073', 'dim_2079', 'dim_2080', 'dim_2089', 'dim_2095', 'dim_2096', 'dim_2105', 'dim_2111', 'dim_2112', 'dim_2113', 'dim_2121', 'dim_2127', 'dim_2128', 'dim_2129', 'dim_2130', 'dim_2137', 'dim_2143', 'dim_2144', 'dim_2145', 'dim_2146', 'dim_2153', 'dim_2159', 'dim_2160', 'dim_2161', 'dim_2162', 'dim_2163', 'dim_2167', 'dim_2168', 'dim_2169', 'dim_2175', 'dim_2176', 'dim_2177', 'dim_2178', 'dim_2179', 'dim_2183', 'dim_2184', 'dim_2185', 'dim_2191', 'dim_2192', 'dim_2193', 'dim_2194', 'dim_2195', 'dim_2199', 'dim_2200', 'dim_2201', 'dim_2207', 'dim_2208', 'dim_2209', 'dim_2210', 'dim_2211', 'dim_2215', 'dim_2216', 'dim_2217', 'dim_2223', 'dim_2224', 'dim_2225', 'dim_2226', 'dim_2227', 'dim_2231', 'dim_2232', 'dim_2233', 'dim_2239', 'dim_2240', 'dim_2241', 'dim_2242', 'dim_2243', 'dim_2247', 'dim_2248', 'dim_2249', 'dim_2255', 'dim_2256', 'dim_2257', 'dim_2258', 'dim_2259', 'dim_2263', 'dim_2264', 'dim_2265', 'dim_2271', 'dim_2272', 'dim_2273', 'dim_2274', 'dim_2275', 'dim_2279', 'dim_2280', 'dim_2281', 'dim_2287', 'dim_2288', 'dim_2289', 'dim_2290', 'dim_2291', 'dim_2295', 'dim_2296', 'dim_2297', 'dim_2303', 'dim_2304', 'dim_2305', 'dim_2306', 'dim_2307', 'dim_2311', 'dim_2312', 'dim_2313', 'dim_2319', 'dim_2320', 'dim_2321', 'dim_2322', 'dim_2323', 'dim_2327', 'dim_2328', 'dim_2329', 'dim_2335', 'dim_2336', 'dim_2337', 'dim_2338', 'dim_2339', 'dim_2343', 'dim_2344', 'dim_2351', 'dim_2352', 'dim_2353', 'dim_2354', 'dim_2355', 'dim_2359', 'dim_2360', 'dim_2367', 'dim_2368', 'dim_2369', 'dim_2370', 'dim_2371', 'dim_2375', 'dim_2376', 'dim_2377', 'dim_2382', 'dim_2383', 'dim_2384', 'dim_2385', 'dim_2386', 'dim_2392', 'dim_2393', 'dim_2397', 'dim_2398', 'dim_2399', 'dim_2400', 'dim_2401', 'dim_2408', 'dim_2409', 'dim_2413', 'dim_2414', 'dim_2415', 'dim_2416', 'dim_2424', 'dim_2425', 'dim_2427', 'dim_2428', 'dim_2429', 'dim_2430', 'dim_2431', 'dim_2432', 'dim_2439', 'dim_2440', 'dim_2441', 'dim_2443', 'dim_2444', 'dim_2445', 'dim_2446', 'dim_2447', 'dim_2448', 'dim_2455', 'dim_2456', 'dim_2459', 'dim_2460', 'dim_2461', 'dim_2462', 'dim_2463', 'dim_2464', 'dim_2465', 'dim_2471', 'dim_2472', 'dim_2475', 'dim_2476', 'dim_2477', 'dim_2478', 'dim_2479', 'dim_2480', 'dim_2481', 'dim_2482', 'dim_2491', 'dim_2492', 'dim_2493', 'dim_2494', 'dim_2495', 'dim_2496', 'dim_2497', 'dim_2498', 'dim_2499', 'dim_2507', 'dim_2508', 'dim_2509', 'dim_2510', 'dim_2511', 'dim_2512', 'dim_2513', 'dim_2514', 'dim_2515', 'dim_2516', 'dim_2522', 'dim_2523', 'dim_2524', 'dim_2525', 'dim_2526', 'dim_2527', 'dim_2528', 'dim_2529', 'dim_2530', 'dim_2531', 'dim_2532', 'dim_2533', 'dim_2535', 'dim_2536', 'dim_2537', 'dim_2538', 'dim_2539', 'dim_2540', 'dim_2541', 'dim_2542', 'dim_2543', 'dim_2544', 'dim_2545', 'dim_2546', 'dim_2547', 'dim_2548', 'dim_2549', 'dim_2550', 'dim_2551', 'dim_2552', 'dim_2553', 'dim_2554', 'dim_2555', 'dim_2556', 'dim_2557', 'dim_2558', 'dim_2559', 'dim_2560', 'dim_2561', 'dim_2562', 'dim_2563', 'dim_2564', 'dim_2565', 'dim_2566', 'dim_2567', 'dim_2568', 'dim_2569', 'dim_2570', 'dim_2571', 'dim_2572', 'dim_2573', 'dim_2574', 'dim_2575', 'dim_2576', 'dim_2577', 'dim_2578', 'dim_2579', 'dim_2580', 'dim_2581', 'dim_2582', 'dim_2583', 'dim_2584', 'dim_2585', 'dim_2586', 'dim_2587', 'dim_2588', 'dim_2589', 'dim_2590', 'dim_2591', 'dim_2592', 'dim_2593', 'dim_2594', 'dim_2595', 'dim_2596', 'dim_2597', 'dim_2598', 'dim_2600', 'dim_2601', 'dim_2602', 'dim_2603', 'dim_2604', 'dim_2605', 'dim_2606', 'dim_2607', 'dim_2608', 'dim_2609', 'dim_2610', 'dim_2611', 'dim_2612', 'dim_2613', 'dim_2619', 'dim_2620', 'dim_2621', 'dim_2622', 'dim_2623', 'dim_2624', 'dim_2625', 'dim_2626', 'dim_2627', 'dim_2628', 'dim_2638', 'dim_2639', 'dim_2640', 'dim_2641', 'dim_2642', 'dim_2654', 'dim_2655', 'dim_2656', 'dim_2657', 'dim_2658', 'dim_2659', 'dim_2671', 'dim_2672', 'dim_2673', 'dim_2674', 'dim_2675', 'dim_2688', 'dim_2689', 'dim_2690', 'dim_2691', 'dim_2696', 'dim_2697', 'dim_2704', 'dim_2705', 'dim_2706', 'dim_2707', 'dim_2712', 'dim_2713', 'dim_2720', 'dim_2721', 'dim_2722', 'dim_2727', 'dim_2728', 'dim_2729', 'dim_2730', 'dim_2736', 'dim_2737', 'dim_2738', 'dim_2743', 'dim_2744', 'dim_2745', 'dim_2746', 'dim_2752', 'dim_2753', 'dim_2754', 'dim_2755', 'dim_2759', 'dim_2760', 'dim_2761', 'dim_2762', 'dim_2768', 'dim_2769', 'dim_2770', 'dim_2771', 'dim_2775', 'dim_2776', 'dim_2777', 'dim_2778', 'dim_2784', 'dim_2785', 'dim_2786', 'dim_2787', 'dim_2791', 'dim_2792', 'dim_2793', 'dim_2794', 'dim_2800', 'dim_2801', 'dim_2802', 'dim_2803', 'dim_2807', 'dim_2808', 'dim_2809', 'dim_2810', 'dim_2816', 'dim_2817', 'dim_2818', 'dim_2819', 'dim_2823', 'dim_2824', 'dim_2825', 'dim_2826', 'dim_2832', 'dim_2833', 'dim_2834', 'dim_2835', 'dim_2839', 'dim_2840', 'dim_2841', 'dim_2842', 'dim_2843', 'dim_2848', 'dim_2849', 'dim_2850', 'dim_2851', 'dim_2855', 'dim_2856', 'dim_2857', 'dim_2858', 'dim_2859', 'dim_2865', 'dim_2866', 'dim_2867', 'dim_2871', 'dim_2872', 'dim_2873', 'dim_2874', 'dim_2875', 'dim_2880', 'dim_2881', 'dim_2882', 'dim_2883', 'dim_2887', 'dim_2888', 'dim_2889', 'dim_2890', 'dim_2891', 'dim_2896', 'dim_2897', 'dim_2898', 'dim_2899', 'dim_2903', 'dim_2904', 'dim_2905', 'dim_2906', 'dim_2912', 'dim_2913', 'dim_2914', 'dim_2915', 'dim_2919', 'dim_2920', 'dim_2921', 'dim_2922', 'dim_2927', 'dim_2928', 'dim_2929', 'dim_2930', 'dim_2931', 'dim_2935', 'dim_2936', 'dim_2937', 'dim_2938', 'dim_2943', 'dim_2944', 'dim_2945', 'dim_2946', 'dim_2947', 'dim_2951', 'dim_2952', 'dim_2953', 'dim_2959', 'dim_2960', 'dim_2961', 'dim_2962', 'dim_2963', 'dim_2967', 'dim_2968', 'dim_2969', 'dim_2975', 'dim_2976', 'dim_2977', 'dim_2978', 'dim_2979', 'dim_2983', 'dim_2984', 'dim_2985', 'dim_2991', 'dim_2992', 'dim_2993', 'dim_2994', 'dim_2995', 'dim_2999', 'dim_3000', 'dim_3001', 'dim_3007', 'dim_3008', 'dim_3009', 'dim_3010', 'dim_3011', 'dim_3015', 'dim_3016', 'dim_3017', 'dim_3022', 'dim_3023', 'dim_3024', 'dim_3025', 'dim_3026', 'dim_3027', 'dim_3031', 'dim_3032', 'dim_3033', 'dim_3038', 'dim_3039', 'dim_3040', 'dim_3041', 'dim_3042', 'dim_3047', 'dim_3048', 'dim_3049', 'dim_3053', 'dim_3054', 'dim_3055', 'dim_3056', 'dim_3057', 'dim_3058', 'dim_3063', 'dim_3064', 'dim_3065', 'dim_3068', 'dim_3069', 'dim_3070', 'dim_3071', 'dim_3072', 'dim_3073', 'dim_3074', 'dim_3075', 'dim_3079', 'dim_3080', 'dim_3083', 'dim_3084', 'dim_3085', 'dim_3086', 'dim_3087', 'dim_3088', 'dim_3089', 'dim_3090', 'dim_3091', 'dim_3095', 'dim_3096', 'dim_3099', 'dim_3100', 'dim_3101', 'dim_3102', 'dim_3103', 'dim_3104', 'dim_3105', 'dim_3106', 'dim_3107', 'dim_3111', 'dim_3115', 'dim_3116', 'dim_3117', 'dim_3118', 'dim_3119', 'dim_3120', 'dim_3121', 'dim_3122', 'dim_3131', 'dim_3132', 'dim_3133', 'dim_3134', 'dim_3135', 'dim_3136', 'dim_3137', 'dim_3138', 'dim_3139', 'dim_3147', 'dim_3148', 'dim_3149', 'dim_3150', 'dim_3151', 'dim_3152', 'dim_3153', 'dim_3154', 'dim_3155', 'dim_3156', 'dim_3162', 'dim_3163', 'dim_3164', 'dim_3165', 'dim_3166', 'dim_3167', 'dim_3168', 'dim_3169', 'dim_3170', 'dim_3171', 'dim_3172', 'dim_3173', 'dim_3175', 'dim_3176', 'dim_3177', 'dim_3178', 'dim_3179', 'dim_3180', 'dim_3181', 'dim_3182', 'dim_3183', 'dim_3184', 'dim_3185', 'dim_3186', 'dim_3187', 'dim_3188', 'dim_3189', 'dim_3190', 'dim_3191', 'dim_3192', 'dim_3193', 'dim_3194', 'dim_3195', 'dim_3196', 'dim_3197', 'dim_3198', 'dim_3199', 'dim_3200', 'dim_3201', 'dim_3202', 'dim_3203', 'dim_3204', 'dim_3205', 'dim_3206', 'dim_3207', 'dim_3208', 'dim_3209', 'dim_3210', 'dim_3211', 'dim_3212', 'dim_3213', 'dim_3214', 'dim_3215', 'dim_3216', 'dim_3217', 'dim_3218', 'dim_3219', 'dim_3220', 'dim_3221', 'dim_3222', 'dim_3223', 'dim_3224', 'dim_3225', 'dim_3226', 'dim_3227', 'dim_3228', 'dim_3229', 'dim_3230', 'dim_3231', 'dim_3232', 'dim_3233', 'dim_3234', 'dim_3235', 'dim_3236', 'dim_3237', 'dim_3238', 'dim_3240', 'dim_3241', 'dim_3242', 'dim_3243', 'dim_3244', 'dim_3245', 'dim_3246', 'dim_3247', 'dim_3248', 'dim_3249', 'dim_3250', 'dim_3251', 'dim_3252', 'dim_3253', 'dim_3259', 'dim_3261', 'dim_3262', 'dim_3263', 'dim_3264', 'dim_3265', 'dim_3266', 'dim_3267', 'dim_3268', 'dim_3278', 'dim_3279', 'dim_3280', 'dim_3281', 'dim_3282', 'dim_3294', 'dim_3295', 'dim_3296', 'dim_3297', 'dim_3298', 'dim_3299', 'dim_3311', 'dim_3312', 'dim_3313', 'dim_3314', 'dim_3315', 'dim_3321', 'dim_3328', 'dim_3329', 'dim_3330', 'dim_3331', 'dim_3336', 'dim_3337', 'dim_3344', 'dim_3345', 'dim_3346', 'dim_3347', 'dim_3352', 'dim_3353', 'dim_3360', 'dim_3361', 'dim_3362', 'dim_3363', 'dim_3368', 'dim_3369', 'dim_3370', 'dim_3376', 'dim_3377', 'dim_3378', 'dim_3379', 'dim_3384', 'dim_3385', 'dim_3386', 'dim_3393', 'dim_3394', 'dim_3395', 'dim_3400', 'dim_3401', 'dim_3402', 'dim_3409', 'dim_3410', 'dim_3411', 'dim_3416', 'dim_3417', 'dim_3418', 'dim_3424', 'dim_3425', 'dim_3426', 'dim_3427', 'dim_3431', 'dim_3432', 'dim_3433', 'dim_3434', 'dim_3440', 'dim_3441', 'dim_3442', 'dim_3443', 'dim_3447', 'dim_3448', 'dim_3449', 'dim_3450', 'dim_3456', 'dim_3457', 'dim_3458', 'dim_3459', 'dim_3463', 'dim_3464', 'dim_3465', 'dim_3466', 'dim_3472', 'dim_3473', 'dim_3474', 'dim_3475', 'dim_3479', 'dim_3480', 'dim_3481', 'dim_3482', 'dim_3483', 'dim_3488', 'dim_3489', 'dim_3490', 'dim_3491', 'dim_3495', 'dim_3496', 'dim_3497', 'dim_3498', 'dim_3499', 'dim_3504', 'dim_3505', 'dim_3506', 'dim_3507', 'dim_3511', 'dim_3512', 'dim_3513', 'dim_3514', 'dim_3515', 'dim_3520', 'dim_3521', 'dim_3522', 'dim_3523', 'dim_3527', 'dim_3528', 'dim_3529', 'dim_3530', 'dim_3531', 'dim_3536', 'dim_3537', 'dim_3538', 'dim_3539', 'dim_3543', 'dim_3544', 'dim_3545', 'dim_3546', 'dim_3547', 'dim_3552', 'dim_3553', 'dim_3554', 'dim_3555', 'dim_3559', 'dim_3560', 'dim_3561', 'dim_3562', 'dim_3563', 'dim_3568', 'dim_3569', 'dim_3570', 'dim_3571', 'dim_3576', 'dim_3577', 'dim_3578', 'dim_3583', 'dim_3584', 'dim_3585', 'dim_3586', 'dim_3587', 'dim_3591', 'dim_3592', 'dim_3593', 'dim_3599', 'dim_3600', 'dim_3601', 'dim_3602', 'dim_3603', 'dim_3607', 'dim_3608', 'dim_3609', 'dim_3615', 'dim_3616', 'dim_3617', 'dim_3618', 'dim_3619', 'dim_3623', 'dim_3624', 'dim_3625', 'dim_3631', 'dim_3632', 'dim_3633', 'dim_3634', 'dim_3635', 'dim_3639', 'dim_3640', 'dim_3641', 'dim_3647', 'dim_3648', 'dim_3649', 'dim_3650', 'dim_3651', 'dim_3655', 'dim_3656', 'dim_3657', 'dim_3662', 'dim_3663', 'dim_3664', 'dim_3665', 'dim_3666', 'dim_3667', 'dim_3671', 'dim_3672', 'dim_3673', 'dim_3678', 'dim_3679', 'dim_3680', 'dim_3681', 'dim_3682', 'dim_3683', 'dim_3687', 'dim_3688', 'dim_3689', 'dim_3693', 'dim_3694', 'dim_3695', 'dim_3696', 'dim_3697', 'dim_3698', 'dim_3699', 'dim_3703', 'dim_3704', 'dim_3705', 'dim_3708', 'dim_3709', 'dim_3710', 'dim_3711', 'dim_3712', 'dim_3713', 'dim_3714', 'dim_3715', 'dim_3719', 'dim_3720', 'dim_3724', 'dim_3725', 'dim_3726', 'dim_3727', 'dim_3728', 'dim_3729', 'dim_3730', 'dim_3731', 'dim_3735', 'dim_3736', 'dim_3740', 'dim_3741', 'dim_3742', 'dim_3743', 'dim_3744', 'dim_3745', 'dim_3746', 'dim_3747', 'dim_3751', 'dim_3752', 'dim_3755', 'dim_3756', 'dim_3757', 'dim_3758', 'dim_3759', 'dim_3760', 'dim_3761', 'dim_3762', 'dim_3771', 'dim_3772', 'dim_3773', 'dim_3774', 'dim_3775', 'dim_3776', 'dim_3777', 'dim_3778', 'dim_3779', 'dim_3787', 'dim_3788', 'dim_3789', 'dim_3790', 'dim_3791', 'dim_3792', 'dim_3793', 'dim_3794', 'dim_3795', 'dim_3802', 'dim_3803', 'dim_3804', 'dim_3805', 'dim_3806', 'dim_3807', 'dim_3808', 'dim_3809', 'dim_3810', 'dim_3811', 'dim_3812', 'dim_3813', 'dim_3815', 'dim_3816', 'dim_3817', 'dim_3818', 'dim_3819', 'dim_3820', 'dim_3821', 'dim_3822', 'dim_3823', 'dim_3824', 'dim_3825', 'dim_3826', 'dim_3827', 'dim_3828', 'dim_3829', 'dim_3830', 'dim_3831', 'dim_3832', 'dim_3833', 'dim_3834', 'dim_3835', 'dim_3836', 'dim_3837', 'dim_3838', 'dim_3839', 'dim_3840', 'dim_3841', 'dim_3842', 'dim_3843', 'dim_3844', 'dim_3845', 'dim_3846', 'dim_3847', 'dim_3848', 'dim_3849', 'dim_3850', 'dim_3851', 'dim_3852', 'dim_3853', 'dim_3854', 'dim_3855', 'dim_3856', 'dim_3857', 'dim_3858', 'dim_3859', 'dim_3860', 'dim_3861', 'dim_3862', 'dim_3863', 'dim_3864', 'dim_3865', 'dim_3866', 'dim_3867', 'dim_3868', 'dim_3869', 'dim_3870', 'dim_3871', 'dim_3872', 'dim_3873', 'dim_3874', 'dim_3875', 'dim_3876', 'dim_3877', 'dim_3878', 'dim_3880', 'dim_3881', 'dim_3882', 'dim_3883', 'dim_3884', 'dim_3885', 'dim_3886', 'dim_3887', 'dim_3888', 'dim_3889', 'dim_3890', 'dim_3891', 'dim_3892', 'dim_3893', 'dim_3900', 'dim_3901', 'dim_3902', 'dim_3903', 'dim_3904', 'dim_3905', 'dim_3906', 'dim_3907', 'dim_3908', 'dim_3918', 'dim_3919', 'dim_3920', 'dim_3921', 'dim_3922', 'dim_3923', 'dim_3934', 'dim_3935', 'dim_3936', 'dim_3937', 'dim_3938', 'dim_3939', 'dim_3951', 'dim_3952', 'dim_3953', 'dim_3954', 'dim_3955', 'dim_3960', 'dim_3961', 'dim_3968', 'dim_3969', 'dim_3970', 'dim_3971', 'dim_3976', 'dim_3977', 'dim_3984', 'dim_3985', 'dim_3986', 'dim_3987', 'dim_3993', 'dim_4000', 'dim_4001', 'dim_4002', 'dim_4003', 'dim_4008', 'dim_4009', 'dim_4010', 'dim_4016', 'dim_4017', 'dim_4018', 'dim_4019', 'dim_4024', 'dim_4025', 'dim_4026', 'dim_4033', 'dim_4034', 'dim_4035', 'dim_4040', 'dim_4041', 'dim_4042', 'dim_4049', 'dim_4050', 'dim_4051', 'dim_4056', 'dim_4057', 'dim_4058', 'dim_4064', 'dim_4065', 'dim_4066', 'dim_4067', 'dim_4071', 'dim_4072', 'dim_4073', 'dim_4074', 'dim_4080', 'dim_4081', 'dim_4082', 'dim_4083', 'dim_4087', 'dim_4088', 'dim_4089', 'dim_4090', 'dim_4096', 'dim_4097', 'dim_4098', 'dim_4099', 'dim_4103', 'dim_4104', 'dim_4105', 'dim_4106', 'dim_4112', 'dim_4113', 'dim_4114', 'dim_4115', 'dim_4119', 'dim_4120', 'dim_4121', 'dim_4122', 'dim_4128', 'dim_4129', 'dim_4130', 'dim_4131', 'dim_4135', 'dim_4136', 'dim_4137', 'dim_4138', 'dim_4144', 'dim_4145', 'dim_4146', 'dim_4147', 'dim_4151', 'dim_4152', 'dim_4153', 'dim_4154', 'dim_4155', 'dim_4161', 'dim_4162', 'dim_4163', 'dim_4167', 'dim_4168', 'dim_4169', 'dim_4170', 'dim_4171', 'dim_4176', 'dim_4177', 'dim_4178', 'dim_4179', 'dim_4183', 'dim_4184', 'dim_4185', 'dim_4186', 'dim_4187', 'dim_4192', 'dim_4193', 'dim_4194', 'dim_4195', 'dim_4199', 'dim_4200', 'dim_4201', 'dim_4202', 'dim_4203', 'dim_4208', 'dim_4209', 'dim_4210', 'dim_4211', 'dim_4216', 'dim_4217', 'dim_4218', 'dim_4223', 'dim_4224', 'dim_4225', 'dim_4226', 'dim_4227', 'dim_4231', 'dim_4232', 'dim_4233', 'dim_4234', 'dim_4239', 'dim_4240', 'dim_4241', 'dim_4242', 'dim_4243', 'dim_4247', 'dim_4248', 'dim_4249', 'dim_4255', 'dim_4256', 'dim_4257', 'dim_4258', 'dim_4259', 'dim_4263', 'dim_4264', 'dim_4265', 'dim_4271', 'dim_4272', 'dim_4273', 'dim_4274', 'dim_4275', 'dim_4279', 'dim_4280', 'dim_4281', 'dim_4287', 'dim_4288', 'dim_4289', 'dim_4290', 'dim_4291', 'dim_4295', 'dim_4296', 'dim_4297', 'dim_4302', 'dim_4303', 'dim_4304', 'dim_4305', 'dim_4306', 'dim_4307', 'dim_4311', 'dim_4312', 'dim_4313', 'dim_4318', 'dim_4319', 'dim_4320', 'dim_4321', 'dim_4322', 'dim_4323', 'dim_4327', 'dim_4328', 'dim_4329', 'dim_4333', 'dim_4334', 'dim_4335', 'dim_4336', 'dim_4337', 'dim_4338', 'dim_4339', 'dim_4343', 'dim_4344', 'dim_4345', 'dim_4348', 'dim_4349', 'dim_4350', 'dim_4351', 'dim_4352', 'dim_4353', 'dim_4354', 'dim_4355', 'dim_4359', 'dim_4360', 'dim_4364', 'dim_4365', 'dim_4366', 'dim_4367', 'dim_4368', 'dim_4369', 'dim_4370', 'dim_4371', 'dim_4375', 'dim_4376', 'dim_4380', 'dim_4381', 'dim_4382', 'dim_4383', 'dim_4384', 'dim_4385', 'dim_4386', 'dim_4387', 'dim_4391', 'dim_4392', 'dim_4395', 'dim_4396', 'dim_4397', 'dim_4398', 'dim_4399', 'dim_4400', 'dim_4401', 'dim_4402', 'dim_4411', 'dim_4412', 'dim_4413', 'dim_4414', 'dim_4415', 'dim_4416', 'dim_4417', 'dim_4418', 'dim_4419', 'dim_4427', 'dim_4428', 'dim_4429', 'dim_4430', 'dim_4431', 'dim_4432', 'dim_4433', 'dim_4434', 'dim_4435', 'dim_4442', 'dim_4443', 'dim_4444', 'dim_4445', 'dim_4446', 'dim_4447', 'dim_4448', 'dim_4449', 'dim_4450', 'dim_4451', 'dim_4452', 'dim_4457', 'dim_4458', 'dim_4459', 'dim_4460', 'dim_4461', 'dim_4462', 'dim_4463', 'dim_4464', 'dim_4465', 'dim_4466', 'dim_4467', 'dim_4468', 'dim_4469', 'dim_4470', 'dim_4471', 'dim_4472', 'dim_4473', 'dim_4474', 'dim_4475', 'dim_4476', 'dim_4477', 'dim_4478', 'dim_4479', 'dim_4480', 'dim_4481', 'dim_4482', 'dim_4483', 'dim_4484', 'dim_4485', 'dim_4486', 'dim_4487', 'dim_4488', 'dim_4489', 'dim_4490', 'dim_4491', 'dim_4492', 'dim_4493', 'dim_4494', 'dim_4495', 'dim_4496', 'dim_4497', 'dim_4498', 'dim_4499', 'dim_4500', 'dim_4501', 'dim_4502', 'dim_4503', 'dim_4504', 'dim_4505', 'dim_4506', 'dim_4507', 'dim_4508', 'dim_4509', 'dim_4510', 'dim_4511', 'dim_4512', 'dim_4513', 'dim_4514', 'dim_4515', 'dim_4516', 'dim_4517', 'dim_4518', 'dim_4520', 'dim_4521', 'dim_4522', 'dim_4523', 'dim_4524', 'dim_4525', 'dim_4526', 'dim_4527', 'dim_4528', 'dim_4529', 'dim_4530', 'dim_4531', 'dim_4532', 'dim_4540', 'dim_4541', 'dim_4542', 'dim_4543', 'dim_4544', 'dim_4545', 'dim_4546', 'dim_4547', 'dim_4548', 'dim_4558', 'dim_4559', 'dim_4560', 'dim_4561', 'dim_4562', 'dim_4563', 'dim_4564', 'dim_4574', 'dim_4575', 'dim_4576', 'dim_4577', 'dim_4578', 'dim_4579', 'dim_4583', 'dim_4591', 'dim_4592', 'dim_4593', 'dim_4594', 'dim_4595', 'dim_4600', 'dim_4601', 'dim_4608', 'dim_4609', 'dim_4610', 'dim_4611', 'dim_4616', 'dim_4617', 'dim_4624', 'dim_4625', 'dim_4626', 'dim_4627', 'dim_4633', 'dim_4640', 'dim_4641', 'dim_4642', 'dim_4643', 'dim_4649', 'dim_4650', 'dim_4656', 'dim_4657', 'dim_4658', 'dim_4659', 'dim_4665', 'dim_4666', 'dim_4673', 'dim_4674', 'dim_4675', 'dim_4680', 'dim_4681', 'dim_4682', 'dim_4689', 'dim_4690', 'dim_4691', 'dim_4696', 'dim_4697', 'dim_4704', 'dim_4705', 'dim_4706', 'dim_4707', 'dim_4711', 'dim_4712', 'dim_4713', 'dim_4720', 'dim_4721', 'dim_4722', 'dim_4723', 'dim_4727', 'dim_4728', 'dim_4729', 'dim_4730', 'dim_4736', 'dim_4737', 'dim_4738', 'dim_4739', 'dim_4743', 'dim_4744', 'dim_4745', 'dim_4746', 'dim_4752', 'dim_4753', 'dim_4754', 'dim_4755', 'dim_4759', 'dim_4760', 'dim_4761', 'dim_4762', 'dim_4768', 'dim_4769', 'dim_4770', 'dim_4771', 'dim_4775', 'dim_4776', 'dim_4777', 'dim_4778', 'dim_4784', 'dim_4785', 'dim_4786', 'dim_4787', 'dim_4791', 'dim_4792', 'dim_4793', 'dim_4794', 'dim_4801', 'dim_4802', 'dim_4803', 'dim_4807', 'dim_4808', 'dim_4809', 'dim_4810', 'dim_4811', 'dim_4816', 'dim_4817', 'dim_4818', 'dim_4819', 'dim_4823', 'dim_4824', 'dim_4825', 'dim_4826', 'dim_4827', 'dim_4832', 'dim_4833', 'dim_4834', 'dim_4835', 'dim_4839', 'dim_4840', 'dim_4841', 'dim_4842', 'dim_4843', 'dim_4848', 'dim_4849', 'dim_4850', 'dim_4851', 'dim_4856', 'dim_4857', 'dim_4858', 'dim_4863', 'dim_4864', 'dim_4865', 'dim_4866', 'dim_4867', 'dim_4871', 'dim_4872', 'dim_4873', 'dim_4874', 'dim_4879', 'dim_4880', 'dim_4881', 'dim_4882', 'dim_4883', 'dim_4887', 'dim_4888', 'dim_4889', 'dim_4890', 'dim_4895', 'dim_4896', 'dim_4897', 'dim_4898', 'dim_4899', 'dim_4903', 'dim_4904', 'dim_4905', 'dim_4906', 'dim_4911', 'dim_4912', 'dim_4913', 'dim_4914', 'dim_4915', 'dim_4919', 'dim_4920', 'dim_4921', 'dim_4927', 'dim_4928', 'dim_4929', 'dim_4930', 'dim_4931', 'dim_4935', 'dim_4936', 'dim_4937', 'dim_4942', 'dim_4943', 'dim_4944', 'dim_4945', 'dim_4946', 'dim_4947', 'dim_4951', 'dim_4952', 'dim_4953', 'dim_4958', 'dim_4959', 'dim_4960', 'dim_4961', 'dim_4962', 'dim_4963', 'dim_4967', 'dim_4968', 'dim_4969', 'dim_4973', 'dim_4974', 'dim_4975', 'dim_4976', 'dim_4977', 'dim_4978', 'dim_4979', 'dim_4983', 'dim_4984', 'dim_4985', 'dim_4988', 'dim_4989', 'dim_4990', 'dim_4991', 'dim_4992', 'dim_4993', 'dim_4994', 'dim_4995', 'dim_4999', 'dim_5000', 'dim_5004', 'dim_5005', 'dim_5006', 'dim_5007', 'dim_5008', 'dim_5009', 'dim_5010', 'dim_5011', 'dim_5015', 'dim_5016', 'dim_5020', 'dim_5021', 'dim_5022', 'dim_5023', 'dim_5024', 'dim_5025', 'dim_5026', 'dim_5027', 'dim_5031', 'dim_5032', 'dim_5035', 'dim_5036', 'dim_5037', 'dim_5038', 'dim_5039', 'dim_5040', 'dim_5041', 'dim_5042', 'dim_5051', 'dim_5052', 'dim_5053', 'dim_5054', 'dim_5055', 'dim_5056', 'dim_5057', 'dim_5058', 'dim_5059', 'dim_5067', 'dim_5068', 'dim_5069', 'dim_5070', 'dim_5071', 'dim_5072', 'dim_5073', 'dim_5074', 'dim_5075', 'dim_5083', 'dim_5084', 'dim_5085', 'dim_5086', 'dim_5087', 'dim_5088', 'dim_5089', 'dim_5090', 'dim_5091', 'dim_5092', 'dim_5098', 'dim_5099', 'dim_5100', 'dim_5101', 'dim_5102', 'dim_5103', 'dim_5104', 'dim_5105', 'dim_5106', 'dim_5107', 'dim_5108', 'dim_5109', 'dim_5110', 'dim_5111', 'dim_5112', 'dim_5113', 'dim_5114', 'dim_5115', 'dim_5116', 'dim_5117', 'dim_5118', 'dim_5119', 'dim_5120', 'dim_5121', 'dim_5122', 'dim_5123', 'dim_5124', 'dim_5125', 'dim_5126', 'dim_5127', 'dim_5128', 'dim_5129', 'dim_5130', 'dim_5131', 'dim_5132', 'dim_5133', 'dim_5134', 'dim_5135', 'dim_5136', 'dim_5137', 'dim_5138', 'dim_5139', 'dim_5140', 'dim_5141', 'dim_5142', 'dim_5143', 'dim_5144', 'dim_5145', 'dim_5146', 'dim_5147', 'dim_5148', 'dim_5149', 'dim_5150', 'dim_5151', 'dim_5152', 'dim_5153', 'dim_5154', 'dim_5155', 'dim_5156', 'dim_5157', 'dim_5158', 'dim_5160', 'dim_5161', 'dim_5162', 'dim_5163', 'dim_5164', 'dim_5165', 'dim_5166', 'dim_5167', 'dim_5168', 'dim_5169', 'dim_5170', 'dim_5171', 'dim_5172', 'dim_5180', 'dim_5181', 'dim_5182', 'dim_5183', 'dim_5184', 'dim_5185', 'dim_5186', 'dim_5187', 'dim_5188', 'dim_5198', 'dim_5199', 'dim_5200', 'dim_5201', 'dim_5202', 'dim_5203', 'dim_5214', 'dim_5215', 'dim_5216', 'dim_5217', 'dim_5218', 'dim_5219', 'dim_5231', 'dim_5232', 'dim_5233', 'dim_5234', 'dim_5235', 'dim_5240', 'dim_5241', 'dim_5248', 'dim_5249', 'dim_5250', 'dim_5251', 'dim_5256', 'dim_5257', 'dim_5264', 'dim_5265', 'dim_5266', 'dim_5267', 'dim_5273', 'dim_5280', 'dim_5281', 'dim_5282', 'dim_5283', 'dim_5289', 'dim_5290', 'dim_5296', 'dim_5297', 'dim_5298', 'dim_5299', 'dim_5305', 'dim_5306', 'dim_5313', 'dim_5314', 'dim_5315', 'dim_5320', 'dim_5321', 'dim_5322', 'dim_5329', 'dim_5330', 'dim_5331', 'dim_5336', 'dim_5337', 'dim_5344', 'dim_5345', 'dim_5346', 'dim_5347', 'dim_5351', 'dim_5352', 'dim_5353', 'dim_5360', 'dim_5361', 'dim_5362', 'dim_5363', 'dim_5367', 'dim_5368', 'dim_5369', 'dim_5370', 'dim_5376', 'dim_5377', 'dim_5378', 'dim_5379', 'dim_5383', 'dim_5384', 'dim_5385', 'dim_5386', 'dim_5392', 'dim_5393', 'dim_5394', 'dim_5395', 'dim_5399', 'dim_5400', 'dim_5401', 'dim_5402', 'dim_5408', 'dim_5409', 'dim_5410', 'dim_5411', 'dim_5415', 'dim_5416', 'dim_5417', 'dim_5418', 'dim_5424', 'dim_5425', 'dim_5426', 'dim_5427', 'dim_5431', 'dim_5432', 'dim_5433', 'dim_5434', 'dim_5440', 'dim_5441', 'dim_5442', 'dim_5443', 'dim_5447', 'dim_5448', 'dim_5449', 'dim_5450', 'dim_5451', 'dim_5456', 'dim_5457', 'dim_5458', 'dim_5459', 'dim_5463', 'dim_5464', 'dim_5465', 'dim_5466', 'dim_5467', 'dim_5472', 'dim_5473', 'dim_5474', 'dim_5475', 'dim_5479', 'dim_5480', 'dim_5481', 'dim_5482', 'dim_5483', 'dim_5488', 'dim_5489', 'dim_5490', 'dim_5491', 'dim_5496', 'dim_5497', 'dim_5498', 'dim_5503', 'dim_5504', 'dim_5505', 'dim_5506', 'dim_5507', 'dim_5511', 'dim_5512', 'dim_5513', 'dim_5514', 'dim_5519', 'dim_5520', 'dim_5521', 'dim_5522', 'dim_5523', 'dim_5527', 'dim_5528', 'dim_5529', 'dim_5530', 'dim_5535', 'dim_5536', 'dim_5537', 'dim_5538', 'dim_5539', 'dim_5543', 'dim_5544', 'dim_5545', 'dim_5546', 'dim_5551', 'dim_5552', 'dim_5553', 'dim_5554', 'dim_5555', 'dim_5559', 'dim_5560', 'dim_5561', 'dim_5567', 'dim_5568', 'dim_5569', 'dim_5570', 'dim_5571', 'dim_5575', 'dim_5576', 'dim_5577', 'dim_5582', 'dim_5583', 'dim_5584', 'dim_5585', 'dim_5586', 'dim_5587', 'dim_5591', 'dim_5592', 'dim_5593', 'dim_5598', 'dim_5599', 'dim_5600', 'dim_5601', 'dim_5602', 'dim_5603', 'dim_5607', 'dim_5608', 'dim_5609', 'dim_5613', 'dim_5614', 'dim_5615', 'dim_5616', 'dim_5617', 'dim_5618', 'dim_5619', 'dim_5623', 'dim_5624', 'dim_5625', 'dim_5628', 'dim_5629', 'dim_5630', 'dim_5631', 'dim_5632', 'dim_5633', 'dim_5634', 'dim_5635', 'dim_5639', 'dim_5640', 'dim_5644', 'dim_5645', 'dim_5646', 'dim_5647', 'dim_5648', 'dim_5649', 'dim_5650', 'dim_5651', 'dim_5655', 'dim_5656', 'dim_5660', 'dim_5661', 'dim_5662', 'dim_5663', 'dim_5664', 'dim_5665', 'dim_5666', 'dim_5667', 'dim_5671', 'dim_5672', 'dim_5675', 'dim_5676', 'dim_5677', 'dim_5678', 'dim_5679', 'dim_5680', 'dim_5681', 'dim_5682', 'dim_5691', 'dim_5692', 'dim_5693', 'dim_5694', 'dim_5695', 'dim_5696', 'dim_5697', 'dim_5698', 'dim_5699', 'dim_5707', 'dim_5708', 'dim_5709', 'dim_5710', 'dim_5711', 'dim_5712', 'dim_5713', 'dim_5714', 'dim_5715', 'dim_5723', 'dim_5724', 'dim_5725', 'dim_5726', 'dim_5727', 'dim_5728', 'dim_5729', 'dim_5730', 'dim_5731', 'dim_5732', 'dim_5738', 'dim_5739', 'dim_5740', 'dim_5741', 'dim_5742', 'dim_5743', 'dim_5744', 'dim_5745', 'dim_5746', 'dim_5747', 'dim_5748', 'dim_5749', 'dim_5750', 'dim_5751', 'dim_5752', 'dim_5753', 'dim_5754', 'dim_5755', 'dim_5756', 'dim_5757', 'dim_5758', 'dim_5759', 'dim_5760', 'dim_5761', 'dim_5762', 'dim_5763', 'dim_5764', 'dim_5765', 'dim_5766', 'dim_5767', 'dim_5768', 'dim_5769', 'dim_5770', 'dim_5771', 'dim_5772', 'dim_5773', 'dim_5774', 'dim_5775', 'dim_5776', 'dim_5777', 'dim_5778', 'dim_5779', 'dim_5780', 'dim_5781', 'dim_5782', 'dim_5783', 'dim_5784', 'dim_5785', 'dim_5786', 'dim_5787', 'dim_5788', 'dim_5789', 'dim_5790', 'dim_5791', 'dim_5792', 'dim_5793', 'dim_5794', 'dim_5795', 'dim_5796', 'dim_5797', 'dim_5798', 'dim_5800', 'dim_5801', 'dim_5802', 'dim_5803', 'dim_5804', 'dim_5805', 'dim_5806', 'dim_5807', 'dim_5808', 'dim_5809', 'dim_5810', 'dim_5811', 'dim_5812', 'dim_5813', 'dim_5820', 'dim_5821', 'dim_5822', 'dim_5823', 'dim_5824', 'dim_5825', 'dim_5826', 'dim_5827', 'dim_5828', 'dim_5838', 'dim_5839', 'dim_5840', 'dim_5841', 'dim_5842', 'dim_5843', 'dim_5854', 'dim_5855', 'dim_5856', 'dim_5857', 'dim_5858', 'dim_5859', 'dim_5871', 'dim_5872', 'dim_5873', 'dim_5874', 'dim_5875', 'dim_5881', 'dim_5888', 'dim_5889', 'dim_5890', 'dim_5891', 'dim_5896', 'dim_5897', 'dim_5904', 'dim_5905', 'dim_5906', 'dim_5907', 'dim_5913', 'dim_5920', 'dim_5921', 'dim_5922', 'dim_5923', 'dim_5929', 'dim_5930', 'dim_5936', 'dim_5937', 'dim_5938', 'dim_5939', 'dim_5944', 'dim_5945', 'dim_5946', 'dim_5953', 'dim_5954', 'dim_5955', 'dim_5960', 'dim_5961', 'dim_5962', 'dim_5969', 'dim_5970', 'dim_5971', 'dim_5976', 'dim_5977', 'dim_5978', 'dim_5984', 'dim_5985', 'dim_5986', 'dim_5987', 'dim_5991', 'dim_5992', 'dim_5993', 'dim_5994', 'dim_6000', 'dim_6001', 'dim_6002', 'dim_6003', 'dim_6007', 'dim_6008', 'dim_6009', 'dim_6010', 'dim_6016', 'dim_6017', 'dim_6018', 'dim_6019', 'dim_6023', 'dim_6024', 'dim_6025', 'dim_6026', 'dim_6032', 'dim_6033', 'dim_6034', 'dim_6035', 'dim_6039', 'dim_6040', 'dim_6041', 'dim_6042', 'dim_6048', 'dim_6049', 'dim_6050', 'dim_6051', 'dim_6055', 'dim_6056', 'dim_6057', 'dim_6058', 'dim_6065', 'dim_6066', 'dim_6067', 'dim_6071', 'dim_6072', 'dim_6073', 'dim_6074', 'dim_6075', 'dim_6080', 'dim_6081', 'dim_6082', 'dim_6083', 'dim_6087', 'dim_6088', 'dim_6089', 'dim_6090', 'dim_6091', 'dim_6096', 'dim_6097', 'dim_6098', 'dim_6099', 'dim_6103', 'dim_6104', 'dim_6105', 'dim_6106', 'dim_6107', 'dim_6112', 'dim_6113', 'dim_6114', 'dim_6115', 'dim_6119', 'dim_6120', 'dim_6121', 'dim_6122', 'dim_6123', 'dim_6128', 'dim_6129', 'dim_6130', 'dim_6131', 'dim_6135', 'dim_6136', 'dim_6137', 'dim_6138', 'dim_6143', 'dim_6144', 'dim_6145', 'dim_6146', 'dim_6147', 'dim_6151', 'dim_6152', 'dim_6153', 'dim_6154', 'dim_6159', 'dim_6160', 'dim_6161', 'dim_6162', 'dim_6163', 'dim_6167', 'dim_6168', 'dim_6169', 'dim_6175', 'dim_6176', 'dim_6177', 'dim_6178', 'dim_6179', 'dim_6183', 'dim_6184', 'dim_6185', 'dim_6191', 'dim_6192', 'dim_6193', 'dim_6194', 'dim_6195', 'dim_6199', 'dim_6200', 'dim_6201', 'dim_6207', 'dim_6208', 'dim_6209', 'dim_6210', 'dim_6211', 'dim_6215', 'dim_6216', 'dim_6217', 'dim_6222', 'dim_6223', 'dim_6224', 'dim_6225', 'dim_6226', 'dim_6227', 'dim_6231', 'dim_6232', 'dim_6233', 'dim_6238', 'dim_6239', 'dim_6240', 'dim_6241', 'dim_6242', 'dim_6243', 'dim_6247', 'dim_6248', 'dim_6249', 'dim_6253', 'dim_6254', 'dim_6255', 'dim_6256', 'dim_6257', 'dim_6258', 'dim_6259', 'dim_6263', 'dim_6264', 'dim_6265', 'dim_6268', 'dim_6269', 'dim_6270', 'dim_6271', 'dim_6272', 'dim_6273', 'dim_6274', 'dim_6275', 'dim_6279', 'dim_6280', 'dim_6284', 'dim_6285', 'dim_6286', 'dim_6287', 'dim_6288', 'dim_6289', 'dim_6290', 'dim_6291', 'dim_6295', 'dim_6296', 'dim_6300', 'dim_6301', 'dim_6302', 'dim_6303', 'dim_6304', 'dim_6305', 'dim_6306', 'dim_6307', 'dim_6311', 'dim_6312', 'dim_6315', 'dim_6316', 'dim_6317', 'dim_6318', 'dim_6319', 'dim_6320', 'dim_6321', 'dim_6322', 'dim_6331', 'dim_6332', 'dim_6333', 'dim_6334', 'dim_6335', 'dim_6336', 'dim_6337', 'dim_6338', 'dim_6339', 'dim_6347', 'dim_6348', 'dim_6349', 'dim_6350', 'dim_6351', 'dim_6352', 'dim_6353', 'dim_6354', 'dim_6355', 'dim_6362', 'dim_6363', 'dim_6364', 'dim_6365', 'dim_6366', 'dim_6367', 'dim_6368', 'dim_6369', 'dim_6370', 'dim_6371', 'dim_6372', 'dim_6377', 'dim_6378', 'dim_6379', 'dim_6380', 'dim_6381', 'dim_6382', 'dim_6383', 'dim_6384', 'dim_6385', 'dim_6386', 'dim_6387', 'dim_6388', 'dim_6389', 'dim_6390', 'dim_6391', 'dim_6392', 'dim_6393', 'dim_6394', 'dim_6395', 'dim_6396', 'dim_6397', 'dim_6398', 'dim_6399', 'dim_6400', 'dim_6401', 'dim_6402', 'dim_6403', 'dim_6404', 'dim_6405', 'dim_6406', 'dim_6407', 'dim_6408', 'dim_6409', 'dim_6410', 'dim_6411', 'dim_6412', 'dim_6413', 'dim_6414', 'dim_6415', 'dim_6416', 'dim_6417', 'dim_6418', 'dim_6419', 'dim_6420', 'dim_6421', 'dim_6422', 'dim_6423', 'dim_6424', 'dim_6425', 'dim_6426', 'dim_6427', 'dim_6428', 'dim_6429', 'dim_6430', 'dim_6431', 'dim_6432', 'dim_6433', 'dim_6434', 'dim_6435', 'dim_6436', 'dim_6437', 'dim_6438', 'dim_6440', 'dim_6441', 'dim_6442', 'dim_6443', 'dim_6444', 'dim_6445', 'dim_6446', 'dim_6447', 'dim_6448', 'dim_6449', 'dim_6450', 'dim_6451', 'dim_6452', 'dim_6453', 'dim_6459', 'dim_6460', 'dim_6461', 'dim_6462', 'dim_6463', 'dim_6464', 'dim_6465', 'dim_6466', 'dim_6467', 'dim_6468', 'dim_6478', 'dim_6479', 'dim_6480', 'dim_6481', 'dim_6482', 'dim_6483', 'dim_6494', 'dim_6495', 'dim_6496', 'dim_6497', 'dim_6498', 'dim_6499', 'dim_6511', 'dim_6512', 'dim_6513', 'dim_6514', 'dim_6515', 'dim_6521', 'dim_6528', 'dim_6529', 'dim_6530', 'dim_6531', 'dim_6536', 'dim_6537', 'dim_6544', 'dim_6545', 'dim_6546', 'dim_6547', 'dim_6552', 'dim_6553', 'dim_6560', 'dim_6561', 'dim_6562', 'dim_6563', 'dim_6568', 'dim_6569', 'dim_6570', 'dim_6576', 'dim_6577', 'dim_6578', 'dim_6579', 'dim_6584', 'dim_6585', 'dim_6586', 'dim_6593', 'dim_6594', 'dim_6595', 'dim_6600', 'dim_6601', 'dim_6602', 'dim_6609', 'dim_6610', 'dim_6611', 'dim_6616', 'dim_6617', 'dim_6618', 'dim_6625', 'dim_6626', 'dim_6627', 'dim_6631', 'dim_6632', 'dim_6633', 'dim_6634', 'dim_6640', 'dim_6641', 'dim_6642', 'dim_6643', 'dim_6647', 'dim_6648', 'dim_6649', 'dim_6650', 'dim_6656', 'dim_6657', 'dim_6658', 'dim_6659', 'dim_6663', 'dim_6664', 'dim_6665', 'dim_6666', 'dim_6672', 'dim_6673', 'dim_6674', 'dim_6675', 'dim_6679', 'dim_6680', 'dim_6681', 'dim_6682', 'dim_6683', 'dim_6688', 'dim_6689', 'dim_6690', 'dim_6691', 'dim_6695', 'dim_6696', 'dim_6697', 'dim_6698', 'dim_6699', 'dim_6704', 'dim_6705', 'dim_6706', 'dim_6707', 'dim_6711', 'dim_6712', 'dim_6713', 'dim_6714', 'dim_6715', 'dim_6720', 'dim_6721', 'dim_6722', 'dim_6723', 'dim_6727', 'dim_6728', 'dim_6729', 'dim_6730', 'dim_6731', 'dim_6736', 'dim_6737', 'dim_6738', 'dim_6739', 'dim_6743', 'dim_6744', 'dim_6745', 'dim_6746', 'dim_6747', 'dim_6752', 'dim_6753', 'dim_6754', 'dim_6755', 'dim_6759', 'dim_6760', 'dim_6761', 'dim_6762', 'dim_6763', 'dim_6768', 'dim_6769', 'dim_6770', 'dim_6771', 'dim_6775', 'dim_6776', 'dim_6777', 'dim_6778', 'dim_6783', 'dim_6784', 'dim_6785', 'dim_6786', 'dim_6787', 'dim_6791', 'dim_6792', 'dim_6793', 'dim_6799', 'dim_6800', 'dim_6801', 'dim_6802', 'dim_6803', 'dim_6807', 'dim_6808', 'dim_6809', 'dim_6815', 'dim_6816', 'dim_6817', 'dim_6818', 'dim_6819', 'dim_6823', 'dim_6824', 'dim_6825', 'dim_6831', 'dim_6832', 'dim_6833', 'dim_6834', 'dim_6835', 'dim_6839', 'dim_6840', 'dim_6841', 'dim_6847', 'dim_6848', 'dim_6849', 'dim_6850', 'dim_6851', 'dim_6855', 'dim_6856', 'dim_6857', 'dim_6862', 'dim_6863', 'dim_6864', 'dim_6865', 'dim_6866', 'dim_6867', 'dim_6871', 'dim_6872', 'dim_6873', 'dim_6878', 'dim_6879', 'dim_6880', 'dim_6881', 'dim_6882', 'dim_6883', 'dim_6887', 'dim_6888', 'dim_6889', 'dim_6893', 'dim_6894', 'dim_6895', 'dim_6896', 'dim_6897', 'dim_6898', 'dim_6899', 'dim_6903', 'dim_6904', 'dim_6905', 'dim_6908', 'dim_6909', 'dim_6910', 'dim_6911', 'dim_6912', 'dim_6913', 'dim_6914', 'dim_6915', 'dim_6919', 'dim_6920', 'dim_6924', 'dim_6925', 'dim_6926', 'dim_6927', 'dim_6928', 'dim_6929', 'dim_6930', 'dim_6931', 'dim_6935', 'dim_6936', 'dim_6940', 'dim_6941', 'dim_6942', 'dim_6943', 'dim_6944', 'dim_6945', 'dim_6946', 'dim_6947', 'dim_6951', 'dim_6952', 'dim_6955', 'dim_6956', 'dim_6957', 'dim_6958', 'dim_6959', 'dim_6960', 'dim_6961', 'dim_6962', 'dim_6971', 'dim_6972', 'dim_6973', 'dim_6974', 'dim_6975', 'dim_6976', 'dim_6977', 'dim_6978', 'dim_6979', 'dim_6987', 'dim_6988', 'dim_6989', 'dim_6990', 'dim_6991', 'dim_6992', 'dim_6993', 'dim_6994', 'dim_6995', 'dim_7003', 'dim_7004', 'dim_7005', 'dim_7006', 'dim_7007', 'dim_7008', 'dim_7009', 'dim_7010', 'dim_7011', 'dim_7012', 'dim_7013', 'dim_7015', 'dim_7016', 'dim_7017', 'dim_7018', 'dim_7019', 'dim_7020', 'dim_7021', 'dim_7022', 'dim_7023', 'dim_7024', 'dim_7025', 'dim_7026', 'dim_7027', 'dim_7028', 'dim_7029', 'dim_7030', 'dim_7031', 'dim_7032', 'dim_7033', 'dim_7034', 'dim_7035', 'dim_7036', 'dim_7037', 'dim_7038', 'dim_7039', 'dim_7040', 'dim_7041', 'dim_7042', 'dim_7043', 'dim_7044', 'dim_7045', 'dim_7046', 'dim_7047', 'dim_7048', 'dim_7049', 'dim_7050', 'dim_7051', 'dim_7052', 'dim_7053', 'dim_7054', 'dim_7055', 'dim_7056', 'dim_7057', 'dim_7058', 'dim_7059', 'dim_7060', 'dim_7061', 'dim_7062', 'dim_7063', 'dim_7064', 'dim_7065', 'dim_7066', 'dim_7067', 'dim_7068', 'dim_7069', 'dim_7070', 'dim_7071', 'dim_7072', 'dim_7073', 'dim_7074', 'dim_7075', 'dim_7076', 'dim_7077', 'dim_7078', 'dim_7080', 'dim_7081', 'dim_7082', 'dim_7083', 'dim_7084', 'dim_7085', 'dim_7086', 'dim_7087', 'dim_7088', 'dim_7089', 'dim_7090', 'dim_7091', 'dim_7092', 'dim_7093', 'dim_7099', 'dim_7100', 'dim_7101', 'dim_7102', 'dim_7103', 'dim_7104', 'dim_7105', 'dim_7106', 'dim_7107', 'dim_7108', 'dim_7118', 'dim_7119', 'dim_7120', 'dim_7121', 'dim_7122', 'dim_7134', 'dim_7135', 'dim_7136', 'dim_7137', 'dim_7138', 'dim_7139', 'dim_7151', 'dim_7152', 'dim_7153', 'dim_7154', 'dim_7155', 'dim_7168', 'dim_7169', 'dim_7170', 'dim_7171', 'dim_7176', 'dim_7177', 'dim_7184', 'dim_7185', 'dim_7186', 'dim_7187', 'dim_7192', 'dim_7193', 'dim_7200', 'dim_7201', 'dim_7202', 'dim_7207', 'dim_7208', 'dim_7209', 'dim_7210', 'dim_7216', 'dim_7217', 'dim_7218', 'dim_7223', 'dim_7224', 'dim_7225', 'dim_7226', 'dim_7231', 'dim_7232', 'dim_7233', 'dim_7234', 'dim_7239', 'dim_7240', 'dim_7241', 'dim_7242', 'dim_7248', 'dim_7249', 'dim_7250', 'dim_7255', 'dim_7256', 'dim_7257', 'dim_7258', 'dim_7264', 'dim_7265', 'dim_7266', 'dim_7267', 'dim_7271', 'dim_7272', 'dim_7273', 'dim_7274', 'dim_7280', 'dim_7281', 'dim_7282', 'dim_7283', 'dim_7287', 'dim_7288', 'dim_7289', 'dim_7290', 'dim_7296', 'dim_7297', 'dim_7298', 'dim_7299', 'dim_7303', 'dim_7304', 'dim_7305', 'dim_7306', 'dim_7313', 'dim_7314', 'dim_7315', 'dim_7319', 'dim_7320', 'dim_7321', 'dim_7322', 'dim_7323', 'dim_7328', 'dim_7329', 'dim_7330', 'dim_7331', 'dim_7335', 'dim_7336', 'dim_7337', 'dim_7338', 'dim_7339', 'dim_7344', 'dim_7345', 'dim_7346', 'dim_7347', 'dim_7351', 'dim_7352', 'dim_7353', 'dim_7354', 'dim_7355', 'dim_7360', 'dim_7361', 'dim_7362', 'dim_7363', 'dim_7367', 'dim_7368', 'dim_7369', 'dim_7370', 'dim_7371', 'dim_7376', 'dim_7377', 'dim_7378', 'dim_7379', 'dim_7383', 'dim_7384', 'dim_7385', 'dim_7386', 'dim_7392', 'dim_7393', 'dim_7394', 'dim_7395', 'dim_7399', 'dim_7400', 'dim_7401', 'dim_7402', 'dim_7407', 'dim_7408', 'dim_7409', 'dim_7410', 'dim_7411', 'dim_7415', 'dim_7416', 'dim_7417', 'dim_7418', 'dim_7423', 'dim_7424', 'dim_7425', 'dim_7426', 'dim_7427', 'dim_7431', 'dim_7432', 'dim_7433', 'dim_7439', 'dim_7440', 'dim_7441', 'dim_7442', 'dim_7443', 'dim_7447', 'dim_7448', 'dim_7449', 'dim_7455', 'dim_7456', 'dim_7457', 'dim_7458', 'dim_7459', 'dim_7463', 'dim_7464', 'dim_7465', 'dim_7471', 'dim_7472', 'dim_7473', 'dim_7474', 'dim_7475', 'dim_7479', 'dim_7480', 'dim_7481', 'dim_7487', 'dim_7488', 'dim_7489', 'dim_7490', 'dim_7491', 'dim_7495', 'dim_7496', 'dim_7497', 'dim_7502', 'dim_7503', 'dim_7504', 'dim_7505', 'dim_7506', 'dim_7507', 'dim_7511', 'dim_7512', 'dim_7513', 'dim_7517', 'dim_7518', 'dim_7519', 'dim_7520', 'dim_7521', 'dim_7522', 'dim_7527', 'dim_7528', 'dim_7529', 'dim_7533', 'dim_7534', 'dim_7535', 'dim_7536', 'dim_7537', 'dim_7538', 'dim_7543', 'dim_7544', 'dim_7545', 'dim_7548', 'dim_7549', 'dim_7550', 'dim_7551', 'dim_7552', 'dim_7553', 'dim_7554', 'dim_7559', 'dim_7560', 'dim_7563', 'dim_7564', 'dim_7565', 'dim_7566', 'dim_7567', 'dim_7568', 'dim_7569', 'dim_7570', 'dim_7571', 'dim_7575', 'dim_7576', 'dim_7579', 'dim_7580', 'dim_7581', 'dim_7582', 'dim_7583', 'dim_7584', 'dim_7585', 'dim_7586', 'dim_7591', 'dim_7595', 'dim_7596', 'dim_7597', 'dim_7598', 'dim_7599', 'dim_7600', 'dim_7601', 'dim_7602', 'dim_7611', 'dim_7612', 'dim_7613', 'dim_7614', 'dim_7615', 'dim_7616', 'dim_7617', 'dim_7618', 'dim_7619', 'dim_7627', 'dim_7628', 'dim_7629', 'dim_7630', 'dim_7631', 'dim_7632', 'dim_7633', 'dim_7634', 'dim_7635', 'dim_7636', 'dim_7642', 'dim_7643', 'dim_7644', 'dim_7645', 'dim_7646', 'dim_7647', 'dim_7648', 'dim_7649', 'dim_7650', 'dim_7651', 'dim_7652', 'dim_7653', 'dim_7654', 'dim_7655', 'dim_7656', 'dim_7657', 'dim_7658', 'dim_7659', 'dim_7660', 'dim_7661', 'dim_7662', 'dim_7663', 'dim_7664', 'dim_7665', 'dim_7666', 'dim_7667', 'dim_7668', 'dim_7669', 'dim_7670', 'dim_7671', 'dim_7672', 'dim_7673', 'dim_7674', 'dim_7675', 'dim_7676', 'dim_7677', 'dim_7678', 'dim_7679', 'dim_7680', 'dim_7681', 'dim_7682', 'dim_7683', 'dim_7684', 'dim_7685', 'dim_7686', 'dim_7687', 'dim_7688', 'dim_7689', 'dim_7690', 'dim_7691', 'dim_7692', 'dim_7693', 'dim_7694', 'dim_7695', 'dim_7696', 'dim_7697', 'dim_7698', 'dim_7699', 'dim_7700', 'dim_7701', 'dim_7702', 'dim_7703', 'dim_7704', 'dim_7705', 'dim_7706', 'dim_7707', 'dim_7708', 'dim_7709', 'dim_7710', 'dim_7711', 'dim_7712', 'dim_7713', 'dim_7714', 'dim_7715', 'dim_7716', 'dim_7717', 'dim_7718', 'dim_7720', 'dim_7721', 'dim_7722', 'dim_7723', 'dim_7724', 'dim_7725', 'dim_7726', 'dim_7727', 'dim_7728', 'dim_7729', 'dim_7730', 'dim_7731', 'dim_7732', 'dim_7733', 'dim_7739', 'dim_7740', 'dim_7741', 'dim_7742', 'dim_7743', 'dim_7744', 'dim_7745', 'dim_7746', 'dim_7747', 'dim_7748', 'dim_7758', 'dim_7759', 'dim_7760', 'dim_7761', 'dim_7762', 'dim_7775', 'dim_7776', 'dim_7777', 'dim_7778', 'dim_7790', 'dim_7791', 'dim_7792', 'dim_7793', 'dim_7794', 'dim_7808', 'dim_7809', 'dim_7810', 'dim_7817', 'dim_7824', 'dim_7825', 'dim_7832', 'dim_7833', 'dim_7839', 'dim_7840', 'dim_7849', 'dim_7855', 'dim_7856', 'dim_7865', 'dim_7871', 'dim_7872', 'dim_7873', 'dim_7881', 'dim_7887', 'dim_7888', 'dim_7889', 'dim_7890', 'dim_7897', 'dim_7903', 'dim_7904', 'dim_7905', 'dim_7906', 'dim_7913', 'dim_7919', 'dim_7920', 'dim_7921', 'dim_7922', 'dim_7923', 'dim_7927', 'dim_7928', 'dim_7929', 'dim_7935', 'dim_7936', 'dim_7937', 'dim_7938', 'dim_7939', 'dim_7943', 'dim_7944', 'dim_7945', 'dim_7951', 'dim_7952', 'dim_7953', 'dim_7954', 'dim_7955', 'dim_7959', 'dim_7960', 'dim_7961', 'dim_7967', 'dim_7968', 'dim_7969', 'dim_7970', 'dim_7971', 'dim_7975', 'dim_7976', 'dim_7977', 'dim_7983', 'dim_7984', 'dim_7985', 'dim_7986', 'dim_7987', 'dim_7991', 'dim_7992', 'dim_7993', 'dim_7999', 'dim_8000', 'dim_8001', 'dim_8002', 'dim_8003', 'dim_8007', 'dim_8008', 'dim_8009', 'dim_8015', 'dim_8016', 'dim_8017', 'dim_8018', 'dim_8019', 'dim_8023', 'dim_8024', 'dim_8025', 'dim_8031', 'dim_8032', 'dim_8033', 'dim_8034', 'dim_8035', 'dim_8039', 'dim_8040', 'dim_8041', 'dim_8047', 'dim_8048', 'dim_8049', 'dim_8050', 'dim_8051', 'dim_8055', 'dim_8056', 'dim_8057', 'dim_8063', 'dim_8064', 'dim_8065', 'dim_8066', 'dim_8067', 'dim_8071', 'dim_8072', 'dim_8073', 'dim_8079', 'dim_8080', 'dim_8081', 'dim_8082', 'dim_8083', 'dim_8087', 'dim_8088', 'dim_8089', 'dim_8095', 'dim_8096', 'dim_8097', 'dim_8098', 'dim_8099', 'dim_8103', 'dim_8104', 'dim_8111', 'dim_8112', 'dim_8113', 'dim_8114', 'dim_8115', 'dim_8119', 'dim_8120', 'dim_8127', 'dim_8128', 'dim_8129', 'dim_8130', 'dim_8131', 'dim_8135', 'dim_8136', 'dim_8137', 'dim_8142', 'dim_8143', 'dim_8144', 'dim_8145', 'dim_8146', 'dim_8151', 'dim_8152', 'dim_8153', 'dim_8157', 'dim_8158', 'dim_8159', 'dim_8160', 'dim_8161', 'dim_8168', 'dim_8169', 'dim_8173', 'dim_8174', 'dim_8175', 'dim_8176', 'dim_8184', 'dim_8185', 'dim_8187', 'dim_8188', 'dim_8189', 'dim_8190', 'dim_8191', 'dim_8192', 'dim_8201', 'dim_8203', 'dim_8204', 'dim_8205', 'dim_8206', 'dim_8207', 'dim_8208', 'dim_8216', 'dim_8219', 'dim_8220', 'dim_8221', 'dim_8222', 'dim_8223', 'dim_8224', 'dim_8225', 'dim_8226', 'dim_8231', 'dim_8232', 'dim_8235', 'dim_8236', 'dim_8237', 'dim_8238', 'dim_8239', 'dim_8240', 'dim_8241', 'dim_8242', 'dim_8251', 'dim_8252', 'dim_8253', 'dim_8254', 'dim_8255', 'dim_8256', 'dim_8257', 'dim_8258', 'dim_8259', 'dim_8267', 'dim_8268', 'dim_8269', 'dim_8270', 'dim_8271', 'dim_8272', 'dim_8273', 'dim_8274', 'dim_8275', 'dim_8276', 'dim_8282', 'dim_8283', 'dim_8284', 'dim_8285', 'dim_8286', 'dim_8287', 'dim_8288', 'dim_8289', 'dim_8290', 'dim_8291', 'dim_8292', 'dim_8293', 'dim_8294', 'dim_8295', 'dim_8296', 'dim_8297', 'dim_8298', 'dim_8299', 'dim_8300', 'dim_8301', 'dim_8302', 'dim_8303', 'dim_8304', 'dim_8305', 'dim_8306', 'dim_8307', 'dim_8308', 'dim_8309', 'dim_8310', 'dim_8311', 'dim_8312', 'dim_8313', 'dim_8314', 'dim_8315', 'dim_8316', 'dim_8317', 'dim_8318', 'dim_8319', 'dim_8320', 'dim_8321', 'dim_8322', 'dim_8323', 'dim_8324', 'dim_8325', 'dim_8326', 'dim_8327', 'dim_8328', 'dim_8329', 'dim_8330', 'dim_8331', 'dim_8332', 'dim_8333', 'dim_8334', 'dim_8335', 'dim_8336', 'dim_8337', 'dim_8338', 'dim_8339', 'dim_8340', 'dim_8341', 'dim_8342', 'dim_8343', 'dim_8344', 'dim_8345', 'dim_8346', 'dim_8347', 'dim_8348', 'dim_8349', 'dim_8350', 'dim_8351', 'dim_8352', 'dim_8353', 'dim_8354', 'dim_8355', 'dim_8356', 'dim_8357', 'dim_8358', 'dim_8360', 'dim_8361', 'dim_8362', 'dim_8363', 'dim_8364', 'dim_8365', 'dim_8366', 'dim_8367', 'dim_8368', 'dim_8369', 'dim_8370', 'dim_8371', 'dim_8372', 'dim_8373', 'dim_8379', 'dim_8380', 'dim_8381', 'dim_8382', 'dim_8383', 'dim_8384', 'dim_8385', 'dim_8386', 'dim_8387', 'dim_8388', 'dim_8389', 'dim_8395', 'dim_8398', 'dim_8399', 'dim_8400', 'dim_8401', 'dim_8402', 'dim_8403', 'dim_8414', 'dim_8415', 'dim_8416', 'dim_8417', 'dim_8418', 'dim_8419', 'dim_8430', 'dim_8431', 'dim_8432', 'dim_8433', 'dim_8434', 'dim_8446', 'dim_8447', 'dim_8448', 'dim_8449', 'dim_8462', 'dim_8463', 'dim_8464', 'dim_8478', 'dim_8479', 'dim_8480', 'dim_8482', 'dim_8494', 'dim_8495', 'dim_8496', 'dim_8510', 'dim_8511', 'dim_8512', 'dim_8513', 'dim_8525', 'dim_8526', 'dim_8527', 'dim_8528', 'dim_8529', 'dim_8541', 'dim_8542', 'dim_8543', 'dim_8544', 'dim_8545', 'dim_8558', 'dim_8559', 'dim_8560', 'dim_8561', 'dim_8574', 'dim_8575', 'dim_8576', 'dim_8577', 'dim_8578', 'dim_8579', 'dim_8590', 'dim_8591', 'dim_8592', 'dim_8593', 'dim_8594', 'dim_8595', 'dim_8606', 'dim_8607', 'dim_8608', 'dim_8609', 'dim_8610', 'dim_8611', 'dim_8622', 'dim_8623', 'dim_8624', 'dim_8625', 'dim_8626', 'dim_8627', 'dim_8638', 'dim_8639', 'dim_8640', 'dim_8641', 'dim_8642', 'dim_8643', 'dim_8654', 'dim_8655', 'dim_8656', 'dim_8657', 'dim_8658', 'dim_8659', 'dim_8670', 'dim_8671', 'dim_8672', 'dim_8673', 'dim_8674', 'dim_8675', 'dim_8686', 'dim_8687', 'dim_8688', 'dim_8689', 'dim_8690', 'dim_8691', 'dim_8701', 'dim_8702', 'dim_8703', 'dim_8704', 'dim_8705', 'dim_8706', 'dim_8707', 'dim_8717', 'dim_8718', 'dim_8719', 'dim_8720', 'dim_8721', 'dim_8722', 'dim_8723', 'dim_8733', 'dim_8734', 'dim_8735', 'dim_8736', 'dim_8737', 'dim_8738', 'dim_8739', 'dim_8749', 'dim_8750', 'dim_8751', 'dim_8752', 'dim_8753', 'dim_8754', 'dim_8755', 'dim_8765', 'dim_8766', 'dim_8767', 'dim_8768', 'dim_8769', 'dim_8770', 'dim_8771', 'dim_8780', 'dim_8781', 'dim_8782', 'dim_8783', 'dim_8784', 'dim_8785', 'dim_8786', 'dim_8796', 'dim_8797', 'dim_8798', 'dim_8799', 'dim_8800', 'dim_8811', 'dim_8812', 'dim_8813', 'dim_8814', 'dim_8815', 'dim_8816', 'dim_8827', 'dim_8828', 'dim_8829', 'dim_8830', 'dim_8831', 'dim_8832', 'dim_8843', 'dim_8844', 'dim_8845', 'dim_8846', 'dim_8847', 'dim_8848', 'dim_8859', 'dim_8860', 'dim_8861', 'dim_8862', 'dim_8863', 'dim_8864', 'dim_8865', 'dim_8875', 'dim_8876', 'dim_8877', 'dim_8878', 'dim_8879', 'dim_8880', 'dim_8881', 'dim_8882', 'dim_8891', 'dim_8892', 'dim_8893', 'dim_8894', 'dim_8895', 'dim_8896', 'dim_8897', 'dim_8898', 'dim_8899', 'dim_8907', 'dim_8908', 'dim_8909', 'dim_8910', 'dim_8911', 'dim_8912', 'dim_8913', 'dim_8914', 'dim_8915', 'dim_8916', 'dim_8922', 'dim_8923', 'dim_8924', 'dim_8925', 'dim_8926', 'dim_8927', 'dim_8928', 'dim_8929', 'dim_8930', 'dim_8931', 'dim_8932', 'dim_8933', 'dim_8934', 'dim_8935', 'dim_8936', 'dim_8937', 'dim_8938', 'dim_8939', 'dim_8940', 'dim_8941', 'dim_8942', 'dim_8943', 'dim_8944', 'dim_8945', 'dim_8946', 'dim_8947', 'dim_8948', 'dim_8949', 'dim_8950', 'dim_8951', 'dim_8952', 'dim_8953', 'dim_8954', 'dim_8955', 'dim_8956', 'dim_8957', 'dim_8958', 'dim_8959', 'dim_8960', 'dim_8961', 'dim_8962', 'dim_8963', 'dim_8964', 'dim_8965', 'dim_8966', 'dim_8967', 'dim_8968', 'dim_8969', 'dim_8970', 'dim_8971', 'dim_8972', 'dim_8973', 'dim_8974', 'dim_8975', 'dim_8976', 'dim_8977', 'dim_8978', 'dim_8979', 'dim_8980', 'dim_8981', 'dim_8982', 'dim_8983', 'dim_8984', 'dim_8985', 'dim_8986', 'dim_8987', 'dim_8988', 'dim_8989', 'dim_8990', 'dim_8991', 'dim_8992', 'dim_8993', 'dim_8994', 'dim_8995', 'dim_8996', 'dim_8997', 'dim_8998', 'dim_8999', 'dim_9000', 'dim_9001', 'dim_9002', 'dim_9003', 'dim_9004', 'dim_9005', 'dim_9006', 'dim_9007', 'dim_9008', 'dim_9009', 'dim_9010', 'dim_9011', 'dim_9012', 'dim_9013', 'dim_9014', 'dim_9016', 'dim_9017', 'dim_9018', 'dim_9019', 'dim_9020', 'dim_9021', 'dim_9022', 'dim_9023', 'dim_9024', 'dim_9025', 'dim_9026', 'dim_9027', 'dim_9028', 'dim_9029', 'dim_9033', 'dim_9034', 'dim_9035', 'dim_9036', 'dim_9038', 'dim_9039', 'dim_9040', 'dim_9041', 'dim_9042', 'dim_9043', 'dim_9044', 'dim_9045', 'dim_9046', 'dim_9051', 'dim_9054', 'dim_9055', 'dim_9056', 'dim_9057', 'dim_9058', 'dim_9059', 'dim_9060', 'dim_9070', 'dim_9071', 'dim_9072', 'dim_9073', 'dim_9074', 'dim_9084', 'dim_9085', 'dim_9086', 'dim_9087', 'dim_9088', 'dim_9089', 'dim_9090', 'dim_9100', 'dim_9101', 'dim_9102', 'dim_9103', 'dim_9104', 'dim_9105', 'dim_9106', 'dim_9116', 'dim_9117', 'dim_9118', 'dim_9119', 'dim_9120', 'dim_9121', 'dim_9122', 'dim_9132', 'dim_9133', 'dim_9134', 'dim_9135', 'dim_9136', 'dim_9137', 'dim_9138', 'dim_9148', 'dim_9149', 'dim_9150', 'dim_9151', 'dim_9152', 'dim_9153', 'dim_9154', 'dim_9164', 'dim_9165', 'dim_9166', 'dim_9167', 'dim_9168', 'dim_9169', 'dim_9170', 'dim_9180', 'dim_9181', 'dim_9182', 'dim_9183', 'dim_9184', 'dim_9185', 'dim_9186', 'dim_9196', 'dim_9197', 'dim_9198', 'dim_9199', 'dim_9200', 'dim_9201', 'dim_9202', 'dim_9203', 'dim_9212', 'dim_9213', 'dim_9214', 'dim_9215', 'dim_9216', 'dim_9217', 'dim_9218', 'dim_9219', 'dim_9228', 'dim_9229', 'dim_9230', 'dim_9231', 'dim_9232', 'dim_9233', 'dim_9234', 'dim_9235', 'dim_9244', 'dim_9245', 'dim_9246', 'dim_9247', 'dim_9248', 'dim_9249', 'dim_9250', 'dim_9251', 'dim_9260', 'dim_9261', 'dim_9262', 'dim_9263', 'dim_9264', 'dim_9265', 'dim_9266', 'dim_9267', 'dim_9276', 'dim_9277', 'dim_9278', 'dim_9279', 'dim_9280', 'dim_9281', 'dim_9282', 'dim_9283', 'dim_9292', 'dim_9293', 'dim_9294', 'dim_9295', 'dim_9296', 'dim_9297', 'dim_9298', 'dim_9299', 'dim_9308', 'dim_9309', 'dim_9310', 'dim_9311', 'dim_9312', 'dim_9313', 'dim_9314', 'dim_9315', 'dim_9324', 'dim_9325', 'dim_9326', 'dim_9327', 'dim_9328', 'dim_9329', 'dim_9330', 'dim_9331', 'dim_9340', 'dim_9341', 'dim_9342', 'dim_9343', 'dim_9344', 'dim_9345', 'dim_9346', 'dim_9347', 'dim_9357', 'dim_9358', 'dim_9359', 'dim_9360', 'dim_9361', 'dim_9362', 'dim_9363', 'dim_9372', 'dim_9373', 'dim_9374', 'dim_9375', 'dim_9376', 'dim_9377', 'dim_9378', 'dim_9379', 'dim_9388', 'dim_9389', 'dim_9390', 'dim_9391', 'dim_9392', 'dim_9393', 'dim_9394', 'dim_9395', 'dim_9403', 'dim_9404', 'dim_9405', 'dim_9406', 'dim_9407', 'dim_9408', 'dim_9409', 'dim_9410', 'dim_9419', 'dim_9420', 'dim_9421', 'dim_9422', 'dim_9423', 'dim_9424', 'dim_9425', 'dim_9426', 'dim_9435', 'dim_9436', 'dim_9437', 'dim_9438', 'dim_9439', 'dim_9440', 'dim_9441', 'dim_9442', 'dim_9451', 'dim_9452', 'dim_9453', 'dim_9454', 'dim_9455', 'dim_9456', 'dim_9457', 'dim_9467', 'dim_9468', 'dim_9469', 'dim_9470', 'dim_9471', 'dim_9472', 'dim_9473', 'dim_9474', 'dim_9483', 'dim_9484', 'dim_9485', 'dim_9486', 'dim_9487', 'dim_9488', 'dim_9489', 'dim_9490', 'dim_9499', 'dim_9500', 'dim_9501', 'dim_9502', 'dim_9503', 'dim_9504', 'dim_9505', 'dim_9506', 'dim_9515', 'dim_9516', 'dim_9517', 'dim_9518', 'dim_9519', 'dim_9520', 'dim_9521', 'dim_9522', 'dim_9523', 'dim_9524', 'dim_9530', 'dim_9531', 'dim_9532', 'dim_9533', 'dim_9534', 'dim_9535', 'dim_9536', 'dim_9537', 'dim_9538', 'dim_9539', 'dim_9540', 'dim_9545', 'dim_9546', 'dim_9547', 'dim_9548', 'dim_9549', 'dim_9550', 'dim_9551', 'dim_9552', 'dim_9553', 'dim_9554', 'dim_9555', 'dim_9556', 'dim_9557', 'dim_9558', 'dim_9559', 'dim_9560', 'dim_9561', 'dim_9562', 'dim_9563', 'dim_9564', 'dim_9565', 'dim_9566', 'dim_9567', 'dim_9568', 'dim_9569', 'dim_9570', 'dim_9571', 'dim_9572', 'dim_9573', 'dim_9574', 'dim_9575', 'dim_9576', 'dim_9577', 'dim_9578', 'dim_9579', 'dim_9580', 'dim_9581', 'dim_9582', 'dim_9583', 'dim_9584', 'dim_9585', 'dim_9586', 'dim_9587', 'dim_9588', 'dim_9589', 'dim_9590', 'dim_9591', 'dim_9592', 'dim_9593', 'dim_9594', 'dim_9595', 'dim_9596', 'dim_9597', 'dim_9598', 'dim_9599', 'dim_9600', 'dim_9601', 'dim_9602', 'dim_9603', 'dim_9604', 'dim_9605', 'dim_9606', 'dim_9607', 'dim_9608', 'dim_9609', 'dim_9610', 'dim_9611', 'dim_9612', 'dim_9613', 'dim_9614', 'dim_9615', 'dim_9616', 'dim_9617', 'dim_9618', 'dim_9619', 'dim_9620', 'dim_9621', 'dim_9622', 'dim_9623', 'dim_9624', 'dim_9625', 'dim_9626', 'dim_9627', 'dim_9628', 'dim_9629', 'dim_9630', 'dim_9631', 'dim_9632', 'dim_9633', 'dim_9634', 'dim_9635', 'dim_9636', 'dim_9637', 'dim_9638', 'dim_9639', 'dim_9640', 'dim_9641', 'dim_9642', 'dim_9643', 'dim_9644', 'dim_9645', 'dim_9646', 'dim_9647', 'dim_9648', 'dim_9649', 'dim_9650', 'dim_9651', 'dim_9652', 'dim_9653', 'dim_9654', 'dim_9655', 'dim_9656', 'dim_9657', 'dim_9658', 'dim_9659', 'dim_9660', 'dim_9661', 'dim_9662', 'dim_9663', 'dim_9664', 'dim_9665', 'dim_9666', 'dim_9667', 'dim_9668', 'dim_9669', 'dim_9670', 'dim_9671', 'dim_9672', 'dim_9673', 'dim_9674', 'dim_9675', 'dim_9676', 'dim_9677', 'dim_9678', 'dim_9679', 'dim_9680', 'dim_9681', 'dim_9682', 'dim_9683', 'dim_9684', 'dim_9685', 'dim_9686', 'dim_9687', 'dim_9688', 'dim_9689', 'dim_9690', 'dim_9691', 'dim_9692', 'dim_9693', 'dim_9694', 'dim_9695', 'dim_9696', 'dim_9697', 'dim_9698', 'dim_9699', 'dim_9700', 'dim_9701', 'dim_9702', 'dim_9706', 'dim_9707', 'dim_9708', 'dim_9709', 'dim_9710', 'dim_9711', 'dim_9712', 'dim_9713', 'dim_9714', 'dim_9715', 'dim_9716', 'dim_9717', 'dim_9719', 'dim_9721', 'dim_9722', 'dim_9723', 'dim_9724', 'dim_9725', 'dim_9726', 'dim_9727', 'dim_9728', 'dim_9729', 'dim_9730', 'dim_9731', 'dim_9732', 'dim_9734', 'dim_9735', 'dim_9737', 'dim_9739', 'dim_9740', 'dim_9741', 'dim_9742', 'dim_9743', 'dim_9744', 'dim_9745', 'dim_9746', 'dim_9747', 'dim_9748', 'dim_9751', 'dim_9753', 'dim_9755', 'dim_9756', 'dim_9757', 'dim_9758', 'dim_9759', 'dim_9760', 'dim_9761', 'dim_9762', 'dim_9763', 'dim_9769', 'dim_9771', 'dim_9772', 'dim_9773', 'dim_9774', 'dim_9775', 'dim_9776', 'dim_9777', 'dim_9778', 'dim_9779', 'dim_9781', 'dim_9787', 'dim_9788', 'dim_9789', 'dim_9790', 'dim_9791', 'dim_9792', 'dim_9793', 'dim_9794', 'dim_9795', 'dim_9796', 'dim_9797', 'dim_9803', 'dim_9804', 'dim_9805', 'dim_9806', 'dim_9807', 'dim_9808', 'dim_9809', 'dim_9810', 'dim_9811', 'dim_9812', 'dim_9819', 'dim_9820', 'dim_9821', 'dim_9822', 'dim_9823', 'dim_9824', 'dim_9825', 'dim_9826', 'dim_9827', 'dim_9828', 'dim_9834', 'dim_9835', 'dim_9836', 'dim_9837', 'dim_9838', 'dim_9839', 'dim_9840', 'dim_9841', 'dim_9842', 'dim_9843', 'dim_9844', 'dim_9845', 'dim_9847', 'dim_9848', 'dim_9850', 'dim_9851', 'dim_9852', 'dim_9853', 'dim_9854', 'dim_9855', 'dim_9856', 'dim_9857', 'dim_9858', 'dim_9859', 'dim_9860', 'dim_9861', 'dim_9863', 'dim_9864', 'dim_9866', 'dim_9867', 'dim_9868', 'dim_9869', 'dim_9870', 'dim_9871', 'dim_9872', 'dim_9873', 'dim_9874', 'dim_9875', 'dim_9876', 'dim_9877', 'dim_9879', 'dim_9880', 'dim_9881', 'dim_9882', 'dim_9883', 'dim_9884', 'dim_9885', 'dim_9886', 'dim_9887', 'dim_9888', 'dim_9889', 'dim_9890', 'dim_9891', 'dim_9892', 'dim_9893', 'dim_9894', 'dim_9895', 'dim_9896', 'dim_9897', 'dim_9898', 'dim_9899', 'dim_9900', 'dim_9901', 'dim_9902', 'dim_9903', 'dim_9904', 'dim_9905', 'dim_9906', 'dim_9907', 'dim_9908', 'dim_9909', 'dim_9910', 'dim_9911', 'dim_9912', 'dim_9913', 'dim_9914', 'dim_9915', 'dim_9916', 'dim_9917', 'dim_9918', 'dim_9919', 'dim_9920', 'dim_9921', 'dim_9922', 'dim_9923', 'dim_9924', 'dim_9925', 'dim_9926', 'dim_9927', 'dim_9928', 'dim_9929', 'dim_9930', 'dim_9931', 'dim_9932', 'dim_9933', 'dim_9934', 'dim_9935', 'dim_9936', 'dim_9937', 'dim_9938', 'dim_9939', 'dim_9940', 'dim_9941', 'dim_9942', 'dim_9943', 'dim_9944', 'dim_9945', 'dim_9946', 'dim_9947', 'dim_9948', 'dim_9949', 'dim_9950', 'dim_9951', 'dim_9952', 'dim_9953', 'dim_9954', 'dim_9955', 'dim_9956', 'dim_9957', 'dim_9958', 'dim_9959', 'dim_9960', 'dim_9961', 'dim_9962', 'dim_9963', 'dim_9964', 'dim_9965', 'dim_9966', 'dim_9967', 'dim_9968', 'dim_9969', 'dim_9970', 'dim_9971', 'dim_9972', 'dim_9973', 'dim_9974', 'dim_9975', 'dim_9976', 'dim_9977', 'dim_9978', 'dim_9979', 'dim_9980', 'dim_9981', 'dim_9982', 'dim_9983', 'dim_9984', 'dim_9985', 'dim_9986', 'dim_9987', 'dim_9988', 'dim_9989', 'dim_9990', 'dim_9991', 'dim_9992', 'dim_9993', 'dim_9994', 'dim_9995', 'dim_9996', 'dim_9997', 'dim_9998', 'dim_9999', 'dim_10000', 'dim_10001', 'dim_10002', 'dim_10003', 'dim_10004', 'dim_10005', 'dim_10006', 'dim_10007', 'dim_10008', 'dim_10009', 'dim_10010', 'dim_10013', 'dim_10014', 'dim_10015', 'dim_10016', 'dim_10017', 'dim_10018', 'dim_10019', 'dim_10020', 'dim_10021', 'dim_10022', 'dim_10023', 'dim_10024', 'dim_10025', 'dim_10026', 'dim_10027', 'dim_10028', 'dim_10029', 'dim_10030', 'dim_10031', 'dim_10032', 'dim_10033', 'dim_10034', 'dim_10035', 'dim_10036', 'dim_10038', 'dim_10039', 'dim_10040', 'dim_10041', 'dim_10042', 'dim_10043', 'dim_10044', 'dim_10045', 'dim_10046', 'dim_10047', 'dim_10048', 'dim_10049', 'dim_10050', 'dim_10051', 'dim_10052', 'dim_10054', 'dim_10055', 'dim_10056', 'dim_10057', 'dim_10058', 'dim_10059', 'dim_10060', 'dim_10061', 'dim_10062', 'dim_10063', 'dim_10064', 'dim_10065', 'dim_10066', 'dim_10067', 'dim_10069', 'dim_10070', 'dim_10071', 'dim_10072', 'dim_10073', 'dim_10074', 'dim_10075', 'dim_10076', 'dim_10077', 'dim_10078', 'dim_10079', 'dim_10080', 'dim_10081', 'dim_10082', 'dim_10083', 'dim_10085', 'dim_10086', 'dim_10087', 'dim_10088', 'dim_10089', 'dim_10091', 'dim_10092', 'dim_10093', 'dim_10094', 'dim_10095', 'dim_10096', 'dim_10097', 'dim_10098', 'dim_10099', 'dim_10100', 'dim_10101', 'dim_10107', 'dim_10108', 'dim_10109', 'dim_10110', 'dim_10111', 'dim_10112', 'dim_10113', 'dim_10114', 'dim_10115', 'dim_10116', 'dim_10117', 'dim_10122', 'dim_10123', 'dim_10124', 'dim_10125', 'dim_10126', 'dim_10127', 'dim_10128', 'dim_10129', 'dim_10130', 'dim_10131', 'dim_10132', 'dim_10133', 'dim_10137', 'dim_10138', 'dim_10139', 'dim_10140', 'dim_10141', 'dim_10142', 'dim_10143', 'dim_10144', 'dim_10145', 'dim_10146', 'dim_10147', 'dim_10148', 'dim_10149', 'dim_10150', 'dim_10151', 'dim_10152', 'dim_10153', 'dim_10154', 'dim_10155', 'dim_10156', 'dim_10157', 'dim_10158', 'dim_10159', 'dim_10160', 'dim_10161', 'dim_10162', 'dim_10163', 'dim_10164', 'dim_10165', 'dim_10166', 'dim_10167', 'dim_10168', 'dim_10169', 'dim_10170', 'dim_10171', 'dim_10172', 'dim_10173', 'dim_10174', 'dim_10175', 'dim_10176', 'dim_10177', 'dim_10178', 'dim_10179', 'dim_10180', 'dim_10181', 'dim_10182', 'dim_10183', 'dim_10184', 'dim_10185', 'dim_10186', 'dim_10187', 'dim_10188', 'dim_10189', 'dim_10190', 'dim_10191', 'dim_10192', 'dim_10193', 'dim_10194', 'dim_10195', 'dim_10196', 'dim_10197', 'dim_10198', 'dim_10199', 'dim_10200', 'dim_10201', 'dim_10202', 'dim_10203', 'dim_10204', 'dim_10205', 'dim_10206', 'dim_10207', 'dim_10208', 'dim_10209', 'dim_10210', 'dim_10211', 'dim_10212', 'dim_10213', 'dim_10214', 'dim_10215', 'dim_10216', 'dim_10217', 'dim_10218', 'dim_10219', 'dim_10220', 'dim_10221', 'dim_10222', 'dim_10223', 'dim_10224', 'dim_10225', 'dim_10226', 'dim_10227', 'dim_10228', 'dim_10229', 'dim_10230', 'dim_10231', 'dim_10232', 'dim_10233', 'dim_10234', 'dim_10235', 'dim_10236', 'dim_10237', 'dim_10238', 'dim_10239', 'dim_10240']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('int8', 'int') : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n",
      "\t44.4s = Fit runtime\n",
      "\t4199 features in original data used to generate 4199 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.47 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 46.02s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving ./agModels-10240/learner.pkl\n",
      "Saving ./agModels-10240/utils/data/X.pkl\n",
      "Saving ./agModels-10240/utils/data/y.pkl\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\tDropped 4199 of 4199 features.\n",
      "\tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\tDropped 4199 of 4199 features.\n",
      "\tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting LightGBMXT_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "\t-0.0468\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.82s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting LightGBM_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L1/model.pkl\n",
      "\t-0.0468\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.96s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting RandomForestMSE_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/utils/model_template.pkl\n",
      "\tDropped 0 of 4199 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "\t-0.0478\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.84s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting CatBoost_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L1/model.pkl\n",
      "\t-0.0467\t = Validation score   (-root_mean_squared_error)\n",
      "\t2646.87s\t = Training   runtime\n",
      "\t8.69s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting ExtraTreesMSE_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/utils/model_template.pkl\n",
      "\tDropped 0 of 4199 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "\t-0.048\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting NeuralNetFastAI_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "\t-0.0456\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.05s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting XGBoost_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L1/model.pkl\n",
      "\t-0.0485\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.33s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting NeuralNetTorch_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "\t-0.0458\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.69s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tDropped 0 of 4199 features.\n",
      "\tDropped 0 of 4199 features.\n",
      "\tFitting LightGBMLarge_BAG_L1 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4199 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "\t-0.048\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.26s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tDropped 0 of 9 features.\n",
      "\tDropped 0 of 9 features.\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "\tDropped 0 of 9 features.\n",
      "Ensemble size: 16\n",
      "Ensemble indices: [5, 7, 0, 5, 7, 5, 0, 5, 7, 0, 5, 7, 5, 0, 5, 7]\n",
      "Ensemble weights: \n",
      "[0.25   0.     0.     0.     0.     0.4375 0.     0.3125 0.    ]\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L2/model.pkl\n",
      "\t-0.0445\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L2 models ...\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/utils/oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMXT_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L2/model.pkl\n",
      "\t-0.0465\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.13s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBM_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L2/model.pkl\n",
      "\t-0.0465\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.78s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting RandomForestMSE_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "\t-0.0475\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.82s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting CatBoost_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L2/model.pkl\n",
      "\t-0.0472\t = Validation score   (-root_mean_squared_error)\n",
      "\t134.51s\t = Training   runtime\n",
      "\t4.03s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting ExtraTreesMSE_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "\t-0.0472\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetFastAI_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "\t-0.0459\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.81s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting XGBoost_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L2/model.pkl\n",
      "\t-0.0487\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.02s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetTorch_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "\t-0.0455\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.06s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMLarge_BAG_L2 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "\t-0.0482\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.24s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\tDropped 0 of 9 features.\n",
      "\tDropped 0 of 9 features.\n",
      "\tFitting WeightedEnsemble_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "\tDropped 0 of 9 features.\n",
      "Ensemble size: 36\n",
      "Ensemble indices: [7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 1, 5, 7, 5, 7, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 7, 5, 7, 5, 7, 5, 7]\n",
      "Ensemble weights: \n",
      "[0.         0.02777778 0.         0.         0.         0.44444444\n",
      " 0.         0.52777778 0.        ]\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L3/model.pkl\n",
      "\t-0.0446\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L3: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L3: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L3: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L3: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L3: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L3: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L3: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L3: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L3: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L3 models ...\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/utils/oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMXT_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L3/model.pkl\n",
      "\t-0.0472\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.18s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBM_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L3/model.pkl\n",
      "\t-0.0475\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.57s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting RandomForestMSE_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L3/model.pkl\n",
      "\t-0.0478\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting CatBoost_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L3/model.pkl\n",
      "\t-0.0476\t = Validation score   (-root_mean_squared_error)\n",
      "\t263.56s\t = Training   runtime\n",
      "\t6.55s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting ExtraTreesMSE_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L3/model.pkl\n",
      "\t-0.0481\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetFastAI_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L3/model.pkl\n",
      "\t-0.0459\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.97s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting XGBoost_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L3/model.pkl\n",
      "\t-0.0482\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.6s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetTorch_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L3/model.pkl\n",
      "\t-0.046\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.57s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L3 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMLarge_BAG_L3 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L3/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L3/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L3/model.pkl\n",
      "\t-0.0474\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.04s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L4: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L4 ...\n",
      "\tDropped 0 of 9 features.\n",
      "\tDropped 0 of 9 features.\n",
      "\tFitting WeightedEnsemble_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L4/utils/model_template.pkl\n",
      "\tDropped 0 of 9 features.\n",
      "Ensemble size: 23\n",
      "Ensemble indices: [5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5, 7, 5]\n",
      "Ensemble weights: \n",
      "[0.         0.         0.         0.         0.         0.52173913\n",
      " 0.         0.47826087 0.        ]\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L4/model.pkl\n",
      "\t-0.0449\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L4: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tLightGBM_BAG_L4: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
      "\tRandomForestMSE_BAG_L4: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L4: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
      "\tExtraTreesMSE_BAG_L4: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L4: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
      "\tXGBoost_BAG_L4: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
      "\tNeuralNetTorch_BAG_L4: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
      "\tLightGBMLarge_BAG_L4: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 5, 'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'name_suffix': 'Large', 'hyperparameter_tune_kwargs': None, 'priority': 0}}\n",
      "Fitting 9 L4 models ...\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/utils/oof.pkl\n",
      "Fitting model: LightGBMXT_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMXT_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMXT_BAG_L4/model.pkl\n",
      "\t-0.047\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.95s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBM_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBM_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBM_BAG_L4/model.pkl\n",
      "\t-0.0461\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.06s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: RandomForestMSE_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting RandomForestMSE_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L4/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/RandomForestMSE_BAG_L4/model.pkl\n",
      "\t-0.0476\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.99s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: CatBoost_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting CatBoost_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/CatBoost_BAG_L4/model.pkl\n",
      "\t-0.0476\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.46s\t = Training   runtime\n",
      "\t3.63s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: ExtraTreesMSE_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting ExtraTreesMSE_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L4/utils/model_template.pkl\n",
      "\tDropped 0 of 4208 features.\n",
      "\t`use_child_oof` was specified for this model. It will function similarly to a bagged model, but will only fit one child model.\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/ExtraTreesMSE_BAG_L4/model.pkl\n",
      "\t-0.0482\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.74s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetFastAI_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetFastAI_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetFastAI_BAG_L4/model.pkl\n",
      "\t-0.0449\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.08s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: XGBoost_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting XGBoost_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/XGBoost_BAG_L4/model.pkl\n",
      "\t-0.0472\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.09s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: NeuralNetTorch_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting NeuralNetTorch_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/NeuralNetTorch_BAG_L4/model.pkl\n",
      "\t-0.0454\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.5s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Fitting model: LightGBMLarge_BAG_L4 ...\n",
      "\tDropped 0 of 4208 features.\n",
      "\tDropped 0 of 4208 features.\n",
      "\tFitting LightGBMLarge_BAG_L4 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L4/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L4/utils/model_template.pkl\n",
      "Upper level total_num_cpus, num_gpus 40 | 0\n",
      "\tDropped 0 of 4208 features.\n",
      "minimum_model_resources: {'num_cpus': 1}\n",
      "user_cpu_per_job, user_gpu_per_job None | None\n",
      "user_ensemble_cpu, user_ensemble_gpu None | None\n",
      "Resources info for CpuResourceCalculator: {'resources_per_job': {'cpu': 2}, 'num_parallel_jobs': 15, 'batches': 1, 'cpu_per_job': 2}\n",
      "\tFitting 15 child models (S1F1 - S3F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Folding resources per job {'num_gpus': 0, 'num_cpus': 2}\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L4/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/LightGBMLarge_BAG_L4/model.pkl\n",
      "\t-0.0468\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.2s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L4/utils/oof.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L4/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L5: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L5 ...\n",
      "\tDropped 0 of 9 features.\n",
      "\tDropped 0 of 9 features.\n",
      "\tFitting WeightedEnsemble_L5 with 'num_gpus': 0, 'num_cpus': 40\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L5/utils/model_template.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L5/utils/model_template.pkl\n",
      "\tDropped 0 of 9 features.\n",
      "Ensemble size: 30\n",
      "Ensemble indices: [5, 7, 5, 6, 7, 5, 5, 7, 5, 8, 7, 5, 5, 7, 5, 6, 7, 5, 5, 7, 5, 7, 8, 5, 7, 5, 6, 5, 7, 5]\n",
      "Ensemble weights: \n",
      "[0.         0.         0.         0.         0.         0.5\n",
      " 0.1        0.33333333 0.06666667]\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L5/utils/oof.pkl\n",
      "Saving ./agModels-10240/models/WeightedEnsemble_L5/model.pkl\n",
      "\t-0.0442\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 3843.82s ... Best model: \"WeightedEnsemble_L5\"\n",
      "Loading: ./agModels-10240/models/trainer.pkl\n",
      "Saving ./agModels-10240/models/trainer.pkl\n",
      "Saving ./agModels-10240/learner.pkl\n",
      "Saving ./agModels-10240/predictor.pkl\n",
      "Saving ./agModels-10240/__version__ with contents \"0.7.0\"\n",
      "Saving ./agModels-10240/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./agModels-10240/\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = './agModels-10240'  # specifies folder to store trained models\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "bag_folds = 5 #suggestion range [5, 10]\n",
    "bag_sets = 3 #suggestion range [1, 20]\n",
    "stack_levels = 3 #suggestion range [0, 3]\n",
    "metric = 'root_mean_squared_error' #Regression:mean_absolute_error, mean_squared_error,root_mean_squared_error (default), r2\n",
    "predictor = TabularPredictor(label=label, path=save_path, eval_metric=metric).fit(train_data, \n",
    "                                                                                  presets='best_quality', \n",
    "                                                                                  auto_stack=\"True\", \n",
    "                                                                                  num_bag_folds=bag_folds, \n",
    "                                                                                  num_bag_sets=bag_sets,\n",
    "                                                                                  num_stack_levels=stack_levels,\n",
    "                                                                                  verbosity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_10231</th>\n",
       "      <th>dim_10232</th>\n",
       "      <th>dim_10233</th>\n",
       "      <th>dim_10234</th>\n",
       "      <th>dim_10235</th>\n",
       "      <th>dim_10236</th>\n",
       "      <th>dim_10237</th>\n",
       "      <th>dim_10238</th>\n",
       "      <th>dim_10239</th>\n",
       "      <th>dim_10240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dim_1  dim_2  dim_3  dim_4  dim_5  dim_6  dim_7  dim_8  dim_9  dim_10  \\\n",
       "46     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "101    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "175    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "9      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "136    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0   \n",
       "\n",
       "     ...  dim_10231  dim_10232  dim_10233  dim_10234  dim_10235  dim_10236  \\\n",
       "46   ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "101  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "175  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "9    ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "136  ...        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "     dim_10237  dim_10238  dim_10239  dim_10240  \n",
       "46         0.0        0.0        0.0        0.0  \n",
       "101        0.0        0.0        0.0        0.0  \n",
       "175        0.0        0.0        0.0        0.0  \n",
       "9          0.0        0.0        0.0        0.0  \n",
       "136        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 10240 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_df.drop(columns=['i', 'name'])\n",
    "# val_data.head()\n",
    "y_val = test_data[label]\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: ./agModels-10240/predictor.pkl\n",
      "Loading: ./agModels-10240/learner.pkl\n",
      "Loading: ./agModels-10240/models/trainer.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L5/model.pkl\n",
      "Evaluation: root_mean_squared_error on test data: -0.044519757134338915\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.044519757134338915,\n",
      "    \"mean_squared_error\": -0.0019820087753005208,\n",
      "    \"mean_absolute_error\": -0.03124420763958585,\n",
      "    \"r2\": 0.3259874430541019,\n",
      "    \"pearsonr\": 0.5740160310699406,\n",
      "    \"median_absolute_error\": -0.01846248078346252\n",
      "}\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L5/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L2/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L3/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBM_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/RandomForestMSE_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/CatBoost_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/ExtraTreesMSE_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/XGBoost_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/LightGBMLarge_BAG_L4/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L5/model.pkl\n",
      "Model scores:\n",
      "{'LightGBMXT_BAG_L1': -0.04723866010638192, 'LightGBM_BAG_L1': -0.04723865991917106, 'RandomForestMSE_BAG_L1': -0.04738844494320194, 'CatBoost_BAG_L1': -0.04583979114737291, 'ExtraTreesMSE_BAG_L1': -0.04738031506874293, 'NeuralNetFastAI_BAG_L1': -0.046723811758777184, 'XGBoost_BAG_L1': -0.04753070011589677, 'NeuralNetTorch_BAG_L1': -0.045176397231886226, 'LightGBMLarge_BAG_L1': -0.046674700041324985, 'WeightedEnsemble_L2': -0.04512961328060986, 'LightGBMXT_BAG_L2': -0.0451079022069705, 'LightGBM_BAG_L2': -0.04487739684357689, 'RandomForestMSE_BAG_L2': -0.04445001715541401, 'CatBoost_BAG_L2': -0.04456276911792251, 'ExtraTreesMSE_BAG_L2': -0.045744793256192746, 'NeuralNetFastAI_BAG_L2': -0.049975086961960104, 'XGBoost_BAG_L2': -0.04566065626001358, 'NeuralNetTorch_BAG_L2': -0.045554955521697214, 'LightGBMLarge_BAG_L2': -0.044948625792828045, 'WeightedEnsemble_L3': -0.04625243040161791, 'LightGBMXT_BAG_L3': -0.04544838340751627, 'LightGBM_BAG_L3': -0.04531599157659767, 'RandomForestMSE_BAG_L3': -0.04639809156196842, 'CatBoost_BAG_L3': -0.044581292621408315, 'ExtraTreesMSE_BAG_L3': -0.04602345490923523, 'NeuralNetFastAI_BAG_L3': -0.04619105368775308, 'XGBoost_BAG_L3': -0.04725539449100333, 'NeuralNetTorch_BAG_L3': -0.0451340662054766, 'LightGBMLarge_BAG_L3': -0.045355505483927375, 'WeightedEnsemble_L4': -0.04470452538481377, 'LightGBMXT_BAG_L4': -0.045510080170101525, 'LightGBM_BAG_L4': -0.043787739305499555, 'RandomForestMSE_BAG_L4': -0.04456316340684032, 'CatBoost_BAG_L4': -0.044636518842603674, 'ExtraTreesMSE_BAG_L4': -0.045680622466155146, 'NeuralNetFastAI_BAG_L4': -0.0450719186311638, 'XGBoost_BAG_L4': -0.04548610837079528, 'NeuralNetTorch_BAG_L4': -0.04563092913536178, 'LightGBMLarge_BAG_L4': -0.0458999478741544, 'WeightedEnsemble_L5': -0.044519757134338915}\n"
     ]
    }
   ],
   "source": [
    "%%capture log_output\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config Application.log_level = 'DEBUG'\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "for item in y_pred:\n",
    "    print(item)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_val, y_pred=y_pred, auxiliary_metrics=True)\n",
    "print(perf)\n",
    "\n",
    "results = predictor.fit_summary(show_plot=True)\n",
    "print(results)\n",
    "print(predictor.leaderboard(test_data, silent=True))\n",
    "\n",
    "with open('./output_10240.log', 'w') as f:\n",
    "    f.write(log_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is:  regression\n",
      "AutoGluon identified the following types of features:\n",
      "('int', ['bool']) : 4199 | ['dim_104', 'dim_105', 'dim_121', 'dim_133', 'dim_137', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L2/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39684391021728516\n",
      "0.3913448750972748\n",
      "0.4223480820655823\n",
      "0.4116629958152771\n",
      "0.36983686685562134\n",
      "0.3777603209018707\n",
      "0.45314064621925354\n",
      "0.37940889596939087\n",
      "0.3578237295150757\n",
      "0.3551599979400635\n",
      "0.3765122890472412\n",
      "0.5027872920036316\n",
      "0.48429542779922485\n",
      "0.3819773197174072\n",
      "0.4310091733932495\n",
      "0.4659513235092163\n",
      "0.3352888226509094\n",
      "0.4014022946357727\n",
      "0.5015910863876343\n",
      "0.3721180260181427\n",
      "0.4296607971191406\n",
      "0.45741963386535645\n",
      "0.3608793318271637\n",
      "0.4460003972053528\n",
      "0.40174829959869385\n",
      "0.4364483058452606\n",
      "0.3774409890174866\n",
      "0.4127395451068878\n",
      "0.3348081111907959\n",
      "0.33224034309387207\n",
      "0.4371427595615387\n",
      "0.4403727650642395\n",
      "0.48169034719467163\n",
      "0.44448769092559814\n",
      "0.36699074506759644\n",
      "0.3769642114639282\n",
      "0.3950638175010681\n",
      "0.49207356572151184\n",
      "0.37753841280937195\n",
      "0.3444945514202118\n",
      "0.510535717010498\n",
      "0.3723331093788147\n",
      "0.3548129200935364\n",
      "0.3253961205482483\n",
      "0.3582330346107483\n",
      "0.38779133558273315\n",
      "0.41741833090782166\n",
      "0.4255271255970001\n",
      "0.3588879406452179\n",
      "0.39587604999542236\n",
      "0.390817254781723\n",
      "0.3706459403038025\n",
      "0.3815770447254181\n",
      "0.3540266752243042\n",
      "0.43190398812294006\n",
      "0.3844245672225952\n",
      "0.4207175374031067\n",
      "0.3748228847980499\n",
      "0.3506748676300049\n",
      "0.3775913119316101\n",
      "0.36617881059646606\n",
      "0.36308056116104126\n",
      "0.4502204358577728\n",
      "0.3664547801017761\n",
      "0.37049829959869385\n",
      "0.38924580812454224\n",
      "0.388399213552475\n",
      "0.33848756551742554\n",
      "0.43275970220565796\n",
      "0.3764336407184601\n",
      "0.39182180166244507\n",
      "0.3875674605369568\n",
      "0.38877183198928833\n",
      "0.3625359535217285\n",
      "0.40267127752304077\n",
      "0.39981162548065186\n",
      "0.4985840320587158\n",
      "0.41876351833343506\n",
      "0.36262986063957214\n",
      "0.42403197288513184\n",
      "0.41487616300582886\n",
      "0.49309399724006653\n",
      "0.35455819964408875\n",
      "0.347791850566864\n",
      "0.3657543659210205\n",
      "0.4231069087982178\n",
      "0.38423416018486023\n",
      "0.43932074308395386\n",
      "0.4204988181591034\n",
      "0.404541015625\n",
      "0.3675767183303833\n",
      "0.3428614139556885\n",
      "0.4343869388103485\n",
      "0.368053674697876\n",
      "0.4136015772819519\n",
      "0.3607230484485626\n",
      "0.3901141285896301\n",
      "0.36990952491760254\n",
      "0.5056619048118591\n",
      "0.34728196263313293\n",
      "0.4304957389831543\n",
      "0.40679898858070374\n",
      "0.4076460003852844\n",
      "0.4023314118385315\n",
      "0.4214048981666565\n",
      "0.34785792231559753\n",
      "0.354564368724823\n",
      "0.4218910038471222\n",
      "0.4171818196773529\n",
      "0.4932429790496826\n",
      "0.318304181098938\n",
      "0.42361098527908325\n",
      "0.36999866366386414\n",
      "0.39817965030670166\n",
      "0.44210702180862427\n",
      "0.3920445144176483\n",
      "0.35726508498191833\n",
      "0.3513410985469818\n",
      "0.5293217897415161\n",
      "0.3733004331588745\n",
      "0.38375338912010193\n",
      "0.39716148376464844\n",
      "0.3770299553871155\n",
      "0.4594687223434448\n",
      "0.46647608280181885\n",
      "0.37665995955467224\n",
      "0.4255206882953644\n",
      "0.454256534576416\n",
      "0.38346201181411743\n",
      "0.3644670844078064\n",
      "0.3811638653278351\n",
      "0.45855486392974854\n",
      "0.3651212453842163\n",
      "0.38857144117355347\n",
      "0.44984740018844604\n",
      "0.41730982065200806\n",
      "0.4058590233325958\n",
      "0.44672417640686035\n",
      "0.37337809801101685\n",
      "0.34960514307022095\n",
      "0.42864781618118286\n",
      "0.43285244703292847\n",
      "0.39797788858413696\n",
      "0.34256377816200256\n",
      "0.3949832022190094\n",
      "0.3821568191051483\n",
      "0.4044164717197418\n",
      "0.41656577587127686\n",
      "0.44225016236305237\n",
      "0.35577720403671265\n",
      "0.43682950735092163\n",
      "0.43324869871139526\n",
      "0.4779322147369385\n",
      "0.458032488822937\n",
      "0.37965378165245056\n",
      "0.3614014983177185\n",
      "0.38636475801467896\n",
      "0.4085158705711365\n",
      "0.4091064929962158\n",
      "0.37921759486198425\n",
      "0.46926960349082947\n",
      "0.35954877734184265\n",
      "0.44613826274871826\n",
      "0.41812586784362793\n",
      "0.3671847879886627\n",
      "0.3607879877090454\n",
      "0.4097103178501129\n",
      "0.4428294897079468\n",
      "0.3351787328720093\n",
      "0.42179206013679504\n",
      "0.38310903310775757\n",
      "0.40139806270599365\n",
      "0.41734635829925537\n",
      "0.39352184534072876\n",
      "0.36911529302597046\n",
      "0.4502411484718323\n",
      "0.41650816798210144\n",
      "0.3854735493659973\n",
      "0.43582481145858765\n",
      "0.41829854249954224\n",
      "0.3753190338611603\n",
      "0.34343069791793823\n",
      "0.408173531293869\n",
      "0.393862783908844\n",
      "0.3753816485404968\n",
      "0.3757905662059784\n",
      "0.4492731988430023\n",
      "0.35024258494377136\n",
      "0.44344842433929443\n",
      "0.36579710245132446\n",
      "0.4848976731300354\n",
      "0.37816986441612244\n",
      "0.3526243567466736\n",
      "0.529291033744812\n",
      "0.4754358232021332\n",
      "0.4222380518913269\n",
      "0.4088078737258911\n",
      "0.4466899037361145\n",
      "0.352027952671051\n",
      "0.4324955344200134\n",
      "0.41091346740722656\n",
      "0.4438427686691284\n",
      "0.367479532957077\n",
      "0.356967568397522\n",
      "0.4421093463897705\n",
      "0.3551580309867859\n",
      "0.40726810693740845\n",
      "0.41268569231033325\n",
      "0.40950411558151245\n",
      "0.36638766527175903\n",
      "0.4628555178642273\n",
      "0.3804134130477905\n",
      "0.33613070845603943\n",
      "0.421906977891922\n",
      "0.3759344816207886\n",
      "0.38768476247787476\n",
      "0.3933557868003845\n",
      "0.42476120591163635\n",
      "0.4160703718662262\n",
      "0.41628405451774597\n",
      "0.37179338932037354\n",
      "0.3492801785469055\n",
      "0.48180729150772095\n",
      "0.37121084332466125\n",
      "0.43736839294433594\n",
      "0.33976131677627563\n",
      "0.3545958399772644\n",
      "0.4217188358306885\n",
      "0.3796144425868988\n",
      "0.345183789730072\n",
      "0.38656601309776306\n",
      "0.3390214443206787\n",
      "0.40643763542175293\n",
      "0.36864525079727173\n",
      "0.3608063757419586\n",
      "0.34979933500289917\n",
      "0.4724956154823303\n",
      "0.3578512668609619\n",
      "0.49429649114608765\n",
      "0.3616073727607727\n",
      "0.39750319719314575\n",
      "0.38925763964653015\n",
      "0.3334582448005676\n",
      "0.39769741892814636\n",
      "0.4113866090774536\n",
      "0.3862571716308594\n",
      "0.3778088390827179\n",
      "0.4213080406188965\n",
      "0.37683096528053284\n",
      "0.3886955678462982\n",
      "0.370138943195343\n",
      "0.3733401596546173\n",
      "0.3776523470878601\n",
      "0.3878471851348877\n",
      "0.42236626148223877\n",
      "0.42555058002471924\n",
      "0.36183488368988037\n",
      "0.4260569214820862\n",
      "0.37453824281692505\n",
      "0.3580417037010193\n",
      "0.3583259880542755\n",
      "0.3619075417518616\n",
      "0.47274839878082275\n",
      "0.35104116797447205\n",
      "0.44831472635269165\n",
      "0.38346540927886963\n",
      "0.38826796412467957\n",
      "0.4750233292579651\n",
      "0.443672239780426\n",
      "0.39815205335617065\n",
      "0.47187650203704834\n",
      "0.433019757270813\n",
      "0.40879520773887634\n",
      "0.3484746813774109\n",
      "0.35777124762535095\n",
      "0.42747431993484497\n",
      "0.3409869074821472\n",
      "0.34812217950820923\n",
      "0.45403623580932617\n",
      "0.3878922164440155\n",
      "0.4234725534915924\n",
      "0.3382609486579895\n",
      "0.38978561758995056\n",
      "0.3966904580593109\n",
      "0.387422651052475\n",
      "0.36523115634918213\n",
      "0.33892375230789185\n",
      "0.43967926502227783\n",
      "0.4070580005645752\n",
      "0.5034397840499878\n",
      "0.4100548326969147\n",
      "0.3803771138191223\n",
      "0.3360348045825958\n",
      "0.36590099334716797\n",
      "0.3750779926776886\n",
      "0.4262612462043762\n",
      "0.3671853244304657\n",
      "0.5341804623603821\n",
      "0.3673771619796753\n",
      "0.44428014755249023\n",
      "0.3921399414539337\n",
      "0.3515301048755646\n",
      "0.42873114347457886\n",
      "0.355302095413208\n",
      "0.34361016750335693\n",
      "0.40012407302856445\n",
      "0.38210055232048035\n",
      "0.37496256828308105\n",
      "0.34739845991134644\n",
      "0.45197784900665283\n",
      "0.5048218965530396\n",
      "0.5123926997184753\n",
      "0.42853087186813354\n",
      "0.3959610164165497\n",
      "0.4878454804420471\n",
      "0.42875567078590393\n",
      "0.4062850773334503\n",
      "0.4255000650882721\n",
      "0.4023314118385315\n",
      "0.4014737904071808\n",
      "0.5333976745605469\n",
      "0.3578512668609619\n",
      "0.34949761629104614\n",
      "0.35865071415901184\n",
      "0.35386186838150024\n",
      "0.46226149797439575\n",
      "0.4055330455303192\n",
      "0.4023202061653137\n",
      "0.31089097261428833\n",
      "0.3829597234725952\n",
      "0.3962398171424866\n",
      "0.33635953068733215\n",
      "0.4423825740814209\n",
      "0.41739422082901\n",
      "0.38704395294189453\n",
      "0.45433157682418823\n",
      "0.3898024559020996\n",
      "0.4603695869445801\n",
      "0.34738582372665405\n",
      "0.37595993280410767\n",
      "0.37573760747909546\n",
      "0.3859826922416687\n",
      "0.3634797930717468\n",
      "0.4161427617073059\n",
      "0.42965513467788696\n",
      "0.4859081506729126\n",
      "0.34822696447372437\n",
      "0.4144881069660187\n",
      "0.3485623598098755\n",
      "0.3906865119934082\n",
      "0.3496039807796478\n"
     ]
    }
   ],
   "source": [
    "train_data_pred = predictor.predict(train_data, model='WeightedEnsemble_L2')\n",
    "for item in train_data_pred:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: ./agModels-10240/models/LightGBMXT_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetFastAI_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/NeuralNetTorch_BAG_L1/model.pkl\n",
      "Loading: ./agModels-10240/models/WeightedEnsemble_L2/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37721070647239685\n",
      "0.37889713048934937\n",
      "0.38901621103286743\n",
      "0.5264616012573242\n",
      "0.3876022696495056\n",
      "0.3983873128890991\n",
      "0.45048239827156067\n",
      "0.4745407700538635\n",
      "0.4069373607635498\n",
      "0.3362642526626587\n",
      "0.3589078187942505\n",
      "0.4085840582847595\n",
      "0.3843332827091217\n",
      "0.4127265512943268\n",
      "0.4385644793510437\n",
      "0.443266361951828\n",
      "0.4124796390533447\n",
      "0.40313225984573364\n",
      "0.37369030714035034\n",
      "0.3725566267967224\n",
      "0.428382933139801\n",
      "0.3845765292644501\n",
      "0.3717888593673706\n",
      "0.37263745069503784\n",
      "0.4637130796909332\n",
      "0.4064176082611084\n",
      "0.4418641924858093\n",
      "0.3945232033729553\n",
      "0.40603309869766235\n",
      "0.3750144839286804\n",
      "0.44224828481674194\n",
      "0.36285287141799927\n",
      "0.3838746249675751\n",
      "0.3949584364891052\n",
      "0.39330774545669556\n",
      "0.376933753490448\n",
      "0.36675000190734863\n",
      "0.42432475090026855\n",
      "0.3874284625053406\n",
      "0.39080744981765747\n",
      "0.4742080569267273\n",
      "0.3628019094467163\n",
      "0.4568457305431366\n",
      "0.3655886948108673\n",
      "0.44608891010284424\n",
      "0.3894349932670593\n",
      "0.45070159435272217\n",
      "0.38714098930358887\n",
      "0.39656826853752136\n",
      "0.39007216691970825\n",
      "0.39487722516059875\n",
      "0.36500486731529236\n",
      "0.42544257640838623\n",
      "0.42817676067352295\n",
      "0.3829643130302429\n",
      "0.3856255114078522\n",
      "0.4303073287010193\n",
      "0.4118459224700928\n",
      "0.3651316463947296\n",
      "0.43881165981292725\n",
      "0.4231892228126526\n",
      "0.4169766306877136\n",
      "0.35764116048812866\n",
      "0.4123832583427429\n",
      "0.4176274240016937\n",
      "0.3356815576553345\n",
      "0.40318334102630615\n",
      "0.35931915044784546\n",
      "0.38979244232177734\n",
      "0.40223968029022217\n",
      "0.39462101459503174\n",
      "0.3950197994709015\n",
      "0.40535029768943787\n",
      "0.3868292272090912\n",
      "0.4128679633140564\n",
      "0.37579023838043213\n",
      "0.3945937752723694\n",
      "0.38072946667671204\n",
      "0.3664056360721588\n",
      "0.42504453659057617\n",
      "0.3744089603424072\n",
      "0.44051966071128845\n",
      "0.35141026973724365\n",
      "0.38037967681884766\n",
      "0.3872028887271881\n",
      "0.44037508964538574\n",
      "0.3804081678390503\n",
      "0.37583187222480774\n"
     ]
    }
   ],
   "source": [
    "test_data_pred = predictor.predict(test_data, model='WeightedEnsemble_L2')\n",
    "for item in test_data_pred:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
