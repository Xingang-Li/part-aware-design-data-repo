{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/anaconda3/envs/surrogate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_440</th>\n",
       "      <th>dim_441</th>\n",
       "      <th>dim_442</th>\n",
       "      <th>dim_443</th>\n",
       "      <th>dim_444</th>\n",
       "      <th>dim_445</th>\n",
       "      <th>dim_446</th>\n",
       "      <th>dim_447</th>\n",
       "      <th>dim_448</th>\n",
       "      <th>drag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-6.639910</td>\n",
       "      <td>-2.575970</td>\n",
       "      <td>-4.333867</td>\n",
       "      <td>0.348634</td>\n",
       "      <td>-6.591602</td>\n",
       "      <td>4.137893</td>\n",
       "      <td>3.448946</td>\n",
       "      <td>9.801783</td>\n",
       "      <td>-4.944950</td>\n",
       "      <td>-1.340411</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.080956</td>\n",
       "      <td>0.063173</td>\n",
       "      <td>1.612214</td>\n",
       "      <td>1.418663</td>\n",
       "      <td>0.173674</td>\n",
       "      <td>0.923131</td>\n",
       "      <td>-0.423641</td>\n",
       "      <td>-2.278858</td>\n",
       "      <td>-0.536435</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>5.044733</td>\n",
       "      <td>2.543681</td>\n",
       "      <td>-3.247805</td>\n",
       "      <td>-5.378260</td>\n",
       "      <td>0.549456</td>\n",
       "      <td>-1.075264</td>\n",
       "      <td>6.259791</td>\n",
       "      <td>5.443143</td>\n",
       "      <td>1.828774</td>\n",
       "      <td>5.774058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829254</td>\n",
       "      <td>2.322891</td>\n",
       "      <td>0.782790</td>\n",
       "      <td>1.487128</td>\n",
       "      <td>-1.055578</td>\n",
       "      <td>2.655854</td>\n",
       "      <td>-0.162912</td>\n",
       "      <td>-0.385024</td>\n",
       "      <td>0.673573</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.939149</td>\n",
       "      <td>0.704528</td>\n",
       "      <td>0.749587</td>\n",
       "      <td>-1.643254</td>\n",
       "      <td>-3.556550</td>\n",
       "      <td>0.293420</td>\n",
       "      <td>3.039790</td>\n",
       "      <td>-3.084175</td>\n",
       "      <td>-3.810488</td>\n",
       "      <td>3.759489</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636529</td>\n",
       "      <td>10.484594</td>\n",
       "      <td>10.753080</td>\n",
       "      <td>8.283512</td>\n",
       "      <td>1.325511</td>\n",
       "      <td>6.276447</td>\n",
       "      <td>-0.932776</td>\n",
       "      <td>11.217846</td>\n",
       "      <td>1.199296</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-1.177986</td>\n",
       "      <td>1.175732</td>\n",
       "      <td>-1.834498</td>\n",
       "      <td>1.306601</td>\n",
       "      <td>-0.493694</td>\n",
       "      <td>1.298445</td>\n",
       "      <td>2.197183</td>\n",
       "      <td>4.541283</td>\n",
       "      <td>1.859666</td>\n",
       "      <td>3.284920</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.321944</td>\n",
       "      <td>1.785326</td>\n",
       "      <td>0.453086</td>\n",
       "      <td>0.186109</td>\n",
       "      <td>1.353902</td>\n",
       "      <td>2.105992</td>\n",
       "      <td>-0.312803</td>\n",
       "      <td>-1.399713</td>\n",
       "      <td>0.909484</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.390329</td>\n",
       "      <td>-1.530953</td>\n",
       "      <td>-2.521950</td>\n",
       "      <td>-4.864147</td>\n",
       "      <td>-0.742480</td>\n",
       "      <td>0.146031</td>\n",
       "      <td>-0.156536</td>\n",
       "      <td>1.740131</td>\n",
       "      <td>-2.943921</td>\n",
       "      <td>5.280754</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.205936</td>\n",
       "      <td>-0.899805</td>\n",
       "      <td>0.860134</td>\n",
       "      <td>1.484055</td>\n",
       "      <td>-2.426541</td>\n",
       "      <td>2.012407</td>\n",
       "      <td>-0.914859</td>\n",
       "      <td>-1.245621</td>\n",
       "      <td>0.841827</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 449 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "61  -6.639910 -2.575970 -4.333867  0.348634 -6.591602  4.137893  3.448946   \n",
       "354  5.044733  2.543681 -3.247805 -5.378260  0.549456 -1.075264  6.259791   \n",
       "358 -0.939149  0.704528  0.749587 -1.643254 -3.556550  0.293420  3.039790   \n",
       "275 -1.177986  1.175732 -1.834498  1.306601 -0.493694  1.298445  2.197183   \n",
       "18   0.390329 -1.530953 -2.521950 -4.864147 -0.742480  0.146031 -0.156536   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...   dim_440    dim_441    dim_442  \\\n",
       "61   9.801783 -4.944950 -1.340411  ... -1.080956   0.063173   1.612214   \n",
       "354  5.443143  1.828774  5.774058  ... -1.829254   2.322891   0.782790   \n",
       "358 -3.084175 -3.810488  3.759489  ... -0.636529  10.484594  10.753080   \n",
       "275  4.541283  1.859666  3.284920  ... -2.321944   1.785326   0.453086   \n",
       "18   1.740131 -2.943921  5.280754  ... -2.205936  -0.899805   0.860134   \n",
       "\n",
       "      dim_443   dim_444   dim_445   dim_446    dim_447   dim_448   drag  \n",
       "61   1.418663  0.173674  0.923131 -0.423641  -2.278858 -0.536435  0.375  \n",
       "354  1.487128 -1.055578  2.655854 -0.162912  -0.385024  0.673573  0.374  \n",
       "358  8.283512  1.325511  6.276447 -0.932776  11.217846  1.199296  0.435  \n",
       "275  0.186109  1.353902  2.105992 -0.312803  -1.399713  0.909484  0.437  \n",
       "18   1.484055 -2.426541  2.012407 -0.914859  -1.245621  0.841827  0.367  \n",
       "\n",
       "[5 rows x 449 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#surrogate models\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_file = './all_parts_vectors_drags.csv'\n",
    "df = TabularDataset(data_file)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=777)\n",
    "\n",
    "#exclue the first two columns of train data\n",
    "train_data = train_df.drop(columns=['i', 'name'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count    351.000000\n",
      "mean       0.398513\n",
      "std        0.060013\n",
      "min        0.278000\n",
      "25%        0.353000\n",
      "50%        0.394000\n",
      "75%        0.435000\n",
      "max        0.598000\n",
      "Name: drag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label = 'drag'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./agModels-all_parts\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"./agModels-all_parts/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #51-Ubuntu SMP Mon Jul 4 06:41:22 UTC 2022\n",
      "Train Data Rows:    351\n",
      "Train Data Columns: 448\n",
      "Label Column: drag\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (0.598, 0.278, 0.39851, 0.06001)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    234809.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.26 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 448 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 448 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t448 features in original data used to generate 448 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.26 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 280, Val Rows: 71\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.0584\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.0577\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-0.0516\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.0505\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.053\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.0515\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.0527\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.0543\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.0529\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.054\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.89s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-0.0534\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.0501\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 32.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./agModels-all_parts/\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = './agModels-all_parts'  # specifies folder to store trained models\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "metric = 'root_mean_squared_error' #Regression:mean_absolute_error, mean_squared_error,root_mean_squared_error (default), r2\n",
    "predictor = TabularPredictor(label=label, path=save_path, eval_metric=metric).fit(train_data, presets='best_quality', num_bag_folds=0, num_bag_sets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_439</th>\n",
       "      <th>dim_440</th>\n",
       "      <th>dim_441</th>\n",
       "      <th>dim_442</th>\n",
       "      <th>dim_443</th>\n",
       "      <th>dim_444</th>\n",
       "      <th>dim_445</th>\n",
       "      <th>dim_446</th>\n",
       "      <th>dim_447</th>\n",
       "      <th>dim_448</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.262418</td>\n",
       "      <td>4.623484</td>\n",
       "      <td>5.207058</td>\n",
       "      <td>1.474604</td>\n",
       "      <td>5.149589</td>\n",
       "      <td>1.366774</td>\n",
       "      <td>7.350364</td>\n",
       "      <td>4.988407</td>\n",
       "      <td>-4.451728</td>\n",
       "      <td>2.715955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493204</td>\n",
       "      <td>-1.530669</td>\n",
       "      <td>-1.038711</td>\n",
       "      <td>1.546212</td>\n",
       "      <td>0.701456</td>\n",
       "      <td>-0.417296</td>\n",
       "      <td>2.205014</td>\n",
       "      <td>-2.338213</td>\n",
       "      <td>-1.252558</td>\n",
       "      <td>-0.207984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.815653</td>\n",
       "      <td>0.467063</td>\n",
       "      <td>-0.713573</td>\n",
       "      <td>-2.519432</td>\n",
       "      <td>-6.833046</td>\n",
       "      <td>3.925573</td>\n",
       "      <td>3.131740</td>\n",
       "      <td>2.785505</td>\n",
       "      <td>-0.169789</td>\n",
       "      <td>3.797550</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.799442</td>\n",
       "      <td>-2.883593</td>\n",
       "      <td>1.992778</td>\n",
       "      <td>0.544198</td>\n",
       "      <td>1.757790</td>\n",
       "      <td>-0.963274</td>\n",
       "      <td>2.303173</td>\n",
       "      <td>-0.294457</td>\n",
       "      <td>-0.247084</td>\n",
       "      <td>2.795963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.128397</td>\n",
       "      <td>-0.347771</td>\n",
       "      <td>-4.491439</td>\n",
       "      <td>-0.143762</td>\n",
       "      <td>-1.139696</td>\n",
       "      <td>1.523660</td>\n",
       "      <td>4.283424</td>\n",
       "      <td>1.275272</td>\n",
       "      <td>-3.277825</td>\n",
       "      <td>-0.944104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>-2.087350</td>\n",
       "      <td>0.265765</td>\n",
       "      <td>1.710699</td>\n",
       "      <td>-0.205781</td>\n",
       "      <td>0.221282</td>\n",
       "      <td>2.097013</td>\n",
       "      <td>-2.484610</td>\n",
       "      <td>-1.032043</td>\n",
       "      <td>0.472173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-0.358754</td>\n",
       "      <td>-5.121314</td>\n",
       "      <td>-1.869275</td>\n",
       "      <td>0.053525</td>\n",
       "      <td>2.998930</td>\n",
       "      <td>-0.482418</td>\n",
       "      <td>1.283854</td>\n",
       "      <td>-1.355309</td>\n",
       "      <td>-0.988547</td>\n",
       "      <td>...</td>\n",
       "      <td>2.702447</td>\n",
       "      <td>-1.539050</td>\n",
       "      <td>-0.802389</td>\n",
       "      <td>1.228584</td>\n",
       "      <td>1.375461</td>\n",
       "      <td>-2.406342</td>\n",
       "      <td>0.712337</td>\n",
       "      <td>-1.958422</td>\n",
       "      <td>-1.374814</td>\n",
       "      <td>-1.371111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.339348</td>\n",
       "      <td>2.649365</td>\n",
       "      <td>-1.346519</td>\n",
       "      <td>0.872298</td>\n",
       "      <td>-2.672357</td>\n",
       "      <td>1.983254</td>\n",
       "      <td>2.075392</td>\n",
       "      <td>6.573480</td>\n",
       "      <td>-0.809723</td>\n",
       "      <td>-2.231186</td>\n",
       "      <td>...</td>\n",
       "      <td>4.364009</td>\n",
       "      <td>-0.522479</td>\n",
       "      <td>0.352676</td>\n",
       "      <td>0.769096</td>\n",
       "      <td>1.184941</td>\n",
       "      <td>2.701294</td>\n",
       "      <td>0.989609</td>\n",
       "      <td>-2.073087</td>\n",
       "      <td>-1.652956</td>\n",
       "      <td>-0.242727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 448 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "46   3.262418  4.623484  5.207058  1.474604  5.149589  1.366774  7.350364   \n",
       "101 -1.815653  0.467063 -0.713573 -2.519432 -6.833046  3.925573  3.131740   \n",
       "175 -0.128397 -0.347771 -4.491439 -0.143762 -1.139696  1.523660  4.283424   \n",
       "9   -0.334880 -0.358754 -5.121314 -1.869275  0.053525  2.998930 -0.482418   \n",
       "136  0.339348  2.649365 -1.346519  0.872298 -2.672357  1.983254  2.075392   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...   dim_439   dim_440   dim_441  \\\n",
       "46   4.988407 -4.451728  2.715955  ...  0.493204 -1.530669 -1.038711   \n",
       "101  2.785505 -0.169789  3.797550  ... -1.799442 -2.883593  1.992778   \n",
       "175  1.275272 -3.277825 -0.944104  ... -0.005584 -2.087350  0.265765   \n",
       "9    1.283854 -1.355309 -0.988547  ...  2.702447 -1.539050 -0.802389   \n",
       "136  6.573480 -0.809723 -2.231186  ...  4.364009 -0.522479  0.352676   \n",
       "\n",
       "      dim_442   dim_443   dim_444   dim_445   dim_446   dim_447   dim_448  \n",
       "46   1.546212  0.701456 -0.417296  2.205014 -2.338213 -1.252558 -0.207984  \n",
       "101  0.544198  1.757790 -0.963274  2.303173 -0.294457 -0.247084  2.795963  \n",
       "175  1.710699 -0.205781  0.221282  2.097013 -2.484610 -1.032043  0.472173  \n",
       "9    1.228584  1.375461 -2.406342  0.712337 -1.958422 -1.374814 -1.371111  \n",
       "136  0.769096  1.184941  2.701294  0.989609 -2.073087 -1.652956 -0.242727  \n",
       "\n",
       "[5 rows x 448 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_df.drop(columns=['i', 'name'])\n",
    "# val_data.head()\n",
    "y_val = test_data[label]\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.049616523059487175\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.049616523059487175,\n",
      "    \"mean_squared_error\": -0.0024617993605126226,\n",
      "    \"mean_absolute_error\": -0.03747519815239039,\n",
      "    \"r2\": 0.16282727788866536,\n",
      "    \"pearsonr\": 0.41014733083055,\n",
      "    \"median_absolute_error\": -0.032015099525451673\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%capture log_output\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config Application.log_level = 'DEBUG'\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "for item in y_pred:\n",
    "    print(item)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_val, y_pred=y_pred, auxiliary_metrics=True)\n",
    "print(perf)\n",
    "\n",
    "results = predictor.fit_summary(show_plot=True)\n",
    "print(results)\n",
    "print(predictor.leaderboard(test_data, silent=True))\n",
    "\n",
    "with open('./output_all_parts.log', 'w') as f:\n",
    "    f.write(log_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is:  regression\n",
      "AutoGluon identified the following types of features:\n",
      "('float', []) : 64 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37861838936805725\n",
      "0.38249272108078003\n",
      "0.4381767511367798\n",
      "0.43621888756752014\n",
      "0.36622968316078186\n",
      "0.366889089345932\n",
      "0.41771551966667175\n",
      "0.41200411319732666\n",
      "0.36638909578323364\n",
      "0.3539412021636963\n",
      "0.34941190481185913\n",
      "0.508400559425354\n",
      "0.4613805115222931\n",
      "0.37091490626335144\n",
      "0.42514297366142273\n",
      "0.4128950238227844\n",
      "0.30657824873924255\n",
      "0.4202014207839966\n",
      "0.49697691202163696\n",
      "0.35575976967811584\n",
      "0.3942566215991974\n",
      "0.46309420466423035\n",
      "0.3245896100997925\n",
      "0.43436768651008606\n",
      "0.4211915135383606\n",
      "0.4247615933418274\n",
      "0.3925668001174927\n",
      "0.4259239435195923\n",
      "0.32838764786720276\n",
      "0.3498603105545044\n",
      "0.4584028124809265\n",
      "0.42842620611190796\n",
      "0.491293728351593\n",
      "0.4479084610939026\n",
      "0.3599356710910797\n",
      "0.37358254194259644\n",
      "0.4128498136997223\n",
      "0.40936943888664246\n",
      "0.3293745517730713\n",
      "0.34855908155441284\n",
      "0.49886441230773926\n",
      "0.37742623686790466\n",
      "0.33937087655067444\n",
      "0.31430166959762573\n",
      "0.3420461118221283\n",
      "0.41306015849113464\n",
      "0.3899974226951599\n",
      "0.40397006273269653\n",
      "0.3414789140224457\n",
      "0.39428237080574036\n",
      "0.395708829164505\n",
      "0.3622297942638397\n",
      "0.36996662616729736\n",
      "0.32605189085006714\n",
      "0.4172266125679016\n",
      "0.37400785088539124\n",
      "0.421663761138916\n",
      "0.37521374225616455\n",
      "0.3210490345954895\n",
      "0.4180373251438141\n",
      "0.3828268051147461\n",
      "0.3548732399940491\n",
      "0.441588431596756\n",
      "0.40890440344810486\n",
      "0.3671486973762512\n",
      "0.4168619215488434\n",
      "0.42734289169311523\n",
      "0.33501070737838745\n",
      "0.3988898992538452\n",
      "0.3781370222568512\n",
      "0.43090590834617615\n",
      "0.3554213345050812\n",
      "0.3608669638633728\n",
      "0.3563440442085266\n",
      "0.397853821516037\n",
      "0.40618112683296204\n",
      "0.42533570528030396\n",
      "0.4329065680503845\n",
      "0.38398119807243347\n",
      "0.43928229808807373\n",
      "0.4231123924255371\n",
      "0.48768019676208496\n",
      "0.3152066767215729\n",
      "0.33327946066856384\n",
      "0.36041855812072754\n",
      "0.43187686800956726\n",
      "0.3813992142677307\n",
      "0.44616252183914185\n",
      "0.39566290378570557\n",
      "0.3726702332496643\n",
      "0.36202192306518555\n",
      "0.31249096989631653\n",
      "0.41026192903518677\n",
      "0.3462991714477539\n",
      "0.43565207719802856\n",
      "0.3553208112716675\n",
      "0.3831081986427307\n",
      "0.3326178789138794\n",
      "0.4925599694252014\n",
      "0.3372308611869812\n",
      "0.4203842878341675\n",
      "0.3855385482311249\n",
      "0.38937345147132874\n",
      "0.3785327970981598\n",
      "0.4379591643810272\n",
      "0.3402762711048126\n",
      "0.3856116235256195\n",
      "0.45170995593070984\n",
      "0.40474578738212585\n",
      "0.47896039485931396\n",
      "0.30372029542922974\n",
      "0.41849666833877563\n",
      "0.36797407269477844\n",
      "0.4067750573158264\n",
      "0.4006305932998657\n",
      "0.38781115412712097\n",
      "0.3386372923851013\n",
      "0.34279170632362366\n",
      "0.5078130960464478\n",
      "0.3585819602012634\n",
      "0.40611734986305237\n",
      "0.42636385560035706\n",
      "0.39003604650497437\n",
      "0.4184426963329315\n",
      "0.4049036204814911\n",
      "0.3481863737106323\n",
      "0.4369145929813385\n",
      "0.47436484694480896\n",
      "0.36766934394836426\n",
      "0.3365926444530487\n",
      "0.37630850076675415\n",
      "0.46120360493659973\n",
      "0.36309492588043213\n",
      "0.4113154411315918\n",
      "0.40029430389404297\n",
      "0.4294794201850891\n",
      "0.39105933904647827\n",
      "0.4623258113861084\n",
      "0.367614209651947\n",
      "0.34982597827911377\n",
      "0.4363081455230713\n",
      "0.42238834500312805\n",
      "0.37623727321624756\n",
      "0.3690257668495178\n",
      "0.40403103828430176\n",
      "0.3975490927696228\n",
      "0.40576207637786865\n",
      "0.3979496955871582\n",
      "0.4615098536014557\n",
      "0.33955106139183044\n",
      "0.4368826150894165\n",
      "0.44599390029907227\n",
      "0.4901018440723419\n",
      "0.4387262165546417\n",
      "0.3890027403831482\n",
      "0.3549541234970093\n",
      "0.3844561278820038\n",
      "0.455634742975235\n",
      "0.4326353669166565\n",
      "0.3799968957901001\n",
      "0.46023300290107727\n",
      "0.3513861894607544\n",
      "0.43991097807884216\n",
      "0.4075394570827484\n",
      "0.35118961334228516\n",
      "0.34626007080078125\n",
      "0.42834633588790894\n",
      "0.43491268157958984\n",
      "0.322027325630188\n",
      "0.42409199476242065\n",
      "0.3581962585449219\n",
      "0.424799382686615\n",
      "0.41013002395629883\n",
      "0.4028841555118561\n",
      "0.3945443034172058\n",
      "0.44578641653060913\n",
      "0.46802932024002075\n",
      "0.3857118487358093\n",
      "0.4390120208263397\n",
      "0.4269908368587494\n",
      "0.40734410285949707\n",
      "0.3769497871398926\n",
      "0.3925184905529022\n",
      "0.38239651918411255\n",
      "0.3879917562007904\n",
      "0.35163089632987976\n",
      "0.39703211188316345\n",
      "0.33853816986083984\n",
      "0.4505141079425812\n",
      "0.36472854018211365\n",
      "0.4359441101551056\n",
      "0.3488730490207672\n",
      "0.376503586769104\n",
      "0.49753305315971375\n",
      "0.47501689195632935\n",
      "0.4348123073577881\n",
      "0.4097827672958374\n",
      "0.461678147315979\n",
      "0.43235284090042114\n",
      "0.4488915801048279\n",
      "0.39235568046569824\n",
      "0.40626344084739685\n",
      "0.35439151525497437\n",
      "0.33804237842559814\n",
      "0.44255220890045166\n",
      "0.36214226484298706\n",
      "0.377564936876297\n",
      "0.4242174029350281\n",
      "0.40591493248939514\n",
      "0.3865261673927307\n",
      "0.46884632110595703\n",
      "0.3772900402545929\n",
      "0.3563401699066162\n",
      "0.3992050886154175\n",
      "0.3797133266925812\n",
      "0.3603857755661011\n",
      "0.3740524351596832\n",
      "0.41966962814331055\n",
      "0.4192713797092438\n",
      "0.42636632919311523\n",
      "0.4022558927536011\n",
      "0.31720781326293945\n",
      "0.4758116602897644\n",
      "0.4160221517086029\n",
      "0.44013315439224243\n",
      "0.370292991399765\n",
      "0.34361276030540466\n",
      "0.4011998772621155\n",
      "0.37065985798835754\n",
      "0.38719770312309265\n",
      "0.4005746841430664\n",
      "0.32517093420028687\n",
      "0.41131266951560974\n",
      "0.38827550411224365\n",
      "0.3403392434120178\n",
      "0.3410448729991913\n",
      "0.4810188412666321\n",
      "0.3513493835926056\n",
      "0.41554170846939087\n",
      "0.3657851815223694\n",
      "0.37804144620895386\n",
      "0.39775601029396057\n",
      "0.3369610905647278\n",
      "0.40327996015548706\n",
      "0.41314971446990967\n",
      "0.37390977144241333\n",
      "0.4156058132648468\n",
      "0.4319806396961212\n",
      "0.4240078926086426\n",
      "0.3920615315437317\n",
      "0.40674740076065063\n",
      "0.38138219714164734\n",
      "0.38356056809425354\n",
      "0.39531493186950684\n",
      "0.3952530324459076\n",
      "0.44844818115234375\n",
      "0.34738439321517944\n",
      "0.4496765732765198\n",
      "0.40555906295776367\n",
      "0.3834686875343323\n",
      "0.34042757749557495\n",
      "0.3602026104927063\n",
      "0.45723026990890503\n",
      "0.3287678360939026\n",
      "0.466122031211853\n",
      "0.38242432475090027\n",
      "0.35020092129707336\n",
      "0.4852704405784607\n",
      "0.4081525206565857\n",
      "0.3819294571876526\n",
      "0.4849777817726135\n",
      "0.4555373191833496\n",
      "0.39361101388931274\n",
      "0.31628769636154175\n",
      "0.3855014145374298\n",
      "0.4223787784576416\n",
      "0.30957257747650146\n",
      "0.34531402587890625\n",
      "0.4685843884944916\n",
      "0.38341742753982544\n",
      "0.4440537691116333\n",
      "0.36879634857177734\n",
      "0.39512449502944946\n",
      "0.4027697741985321\n",
      "0.38086575269699097\n",
      "0.33783581852912903\n",
      "0.3229997158050537\n",
      "0.4342353343963623\n",
      "0.39703837037086487\n",
      "0.5021942853927612\n",
      "0.4246428906917572\n",
      "0.37148404121398926\n",
      "0.30676645040512085\n",
      "0.3681071102619171\n",
      "0.36332815885543823\n",
      "0.4332050383090973\n",
      "0.3619530498981476\n",
      "0.4236973226070404\n",
      "0.34504756331443787\n",
      "0.43237927556037903\n",
      "0.37410131096839905\n",
      "0.33555200695991516\n",
      "0.40784192085266113\n",
      "0.34392687678337097\n",
      "0.3241313099861145\n",
      "0.412631630897522\n",
      "0.39367642998695374\n",
      "0.366481751203537\n",
      "0.33941933512687683\n",
      "0.4474071264266968\n",
      "0.5098754167556763\n",
      "0.4968710243701935\n",
      "0.4436592757701874\n",
      "0.38135480880737305\n",
      "0.4919639527797699\n",
      "0.43924689292907715\n",
      "0.41912657022476196\n",
      "0.420479416847229\n",
      "0.39482253789901733\n",
      "0.39988166093826294\n",
      "0.5108622908592224\n",
      "0.35189470648765564\n",
      "0.3247087299823761\n",
      "0.35508134961128235\n",
      "0.33900728821754456\n",
      "0.4599076211452484\n",
      "0.3962692618370056\n",
      "0.40300753712654114\n",
      "0.2962137758731842\n",
      "0.37457308173179626\n",
      "0.42063289880752563\n",
      "0.30796530842781067\n",
      "0.4210299551486969\n",
      "0.46793854236602783\n",
      "0.4303966462612152\n",
      "0.4579540491104126\n",
      "0.4421934485435486\n",
      "0.4625661373138428\n",
      "0.3482440114021301\n",
      "0.3840988278388977\n",
      "0.3564963936805725\n",
      "0.39105790853500366\n",
      "0.3726164996623993\n",
      "0.4252989888191223\n",
      "0.4221886396408081\n",
      "0.49061936140060425\n",
      "0.33273351192474365\n",
      "0.4340788722038269\n",
      "0.34766554832458496\n",
      "0.3896786868572235\n",
      "0.33697181940078735\n"
     ]
    }
   ],
   "source": [
    "train_data_pred = predictor.predict(train_data, model='WeightedEnsemble_L2')\n",
    "for item in train_data_pred:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37111416459083557\n",
      "0.3560630679130554\n",
      "0.4008951783180237\n",
      "0.41535258293151855\n",
      "0.4331023097038269\n",
      "0.37909644842147827\n",
      "0.398598313331604\n",
      "0.4546825885772705\n",
      "0.4231151342391968\n",
      "0.36642080545425415\n",
      "0.39534491300582886\n",
      "0.39965587854385376\n",
      "0.3610095977783203\n",
      "0.42284712195396423\n",
      "0.4054073095321655\n",
      "0.41756123304367065\n",
      "0.3687572479248047\n",
      "0.4054275155067444\n",
      "0.37956365942955017\n",
      "0.36853331327438354\n",
      "0.41840681433677673\n",
      "0.4042297601699829\n",
      "0.3840467929840088\n",
      "0.38012588024139404\n",
      "0.43543678522109985\n",
      "0.39734330773353577\n",
      "0.44037193059921265\n",
      "0.3608059883117676\n",
      "0.3968009352684021\n",
      "0.37854433059692383\n",
      "0.4255990982055664\n",
      "0.39983874559402466\n",
      "0.4111539125442505\n",
      "0.4331341087818146\n",
      "0.37638670206069946\n",
      "0.36026841402053833\n",
      "0.3626856207847595\n",
      "0.41154545545578003\n",
      "0.3939424753189087\n",
      "0.3762201964855194\n",
      "0.455719918012619\n",
      "0.37719839811325073\n",
      "0.42263007164001465\n",
      "0.37848570942878723\n",
      "0.3750516176223755\n",
      "0.4030238389968872\n",
      "0.39069393277168274\n",
      "0.35904914140701294\n",
      "0.400773823261261\n",
      "0.40457651019096375\n",
      "0.42658737301826477\n",
      "0.3409181833267212\n",
      "0.42102140188217163\n",
      "0.38900864124298096\n",
      "0.3417207598686218\n",
      "0.42450132966041565\n",
      "0.4222024381160736\n",
      "0.40444445610046387\n",
      "0.37917792797088623\n",
      "0.37281665205955505\n",
      "0.41742536425590515\n",
      "0.4042399823665619\n",
      "0.394340455532074\n",
      "0.41041237115859985\n",
      "0.3814655542373657\n",
      "0.38603878021240234\n",
      "0.40247440338134766\n",
      "0.3693678379058838\n",
      "0.3620762825012207\n",
      "0.3945832848548889\n",
      "0.4274894595146179\n",
      "0.3682887554168701\n",
      "0.38425689935684204\n",
      "0.4047880172729492\n",
      "0.36165255308151245\n",
      "0.43604594469070435\n",
      "0.4039037227630615\n",
      "0.37156054377555847\n",
      "0.42641448974609375\n",
      "0.4187307357788086\n",
      "0.3573049306869507\n",
      "0.40123260021209717\n",
      "0.3542708158493042\n",
      "0.38859376311302185\n",
      "0.43700647354125977\n",
      "0.4370720386505127\n",
      "0.3837008476257324\n",
      "0.42471808195114136\n"
     ]
    }
   ],
   "source": [
    "test_data_pred = predictor.predict(test_data, model='WeightedEnsemble_L2')\n",
    "for item in test_data_pred:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
