{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/anaconda3/envs/surrogate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_56</th>\n",
       "      <th>dim_57</th>\n",
       "      <th>dim_58</th>\n",
       "      <th>dim_59</th>\n",
       "      <th>dim_60</th>\n",
       "      <th>dim_61</th>\n",
       "      <th>dim_62</th>\n",
       "      <th>dim_63</th>\n",
       "      <th>dim_64</th>\n",
       "      <th>drag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-6.639910</td>\n",
       "      <td>-2.575970</td>\n",
       "      <td>-4.333867</td>\n",
       "      <td>0.348634</td>\n",
       "      <td>-6.591602</td>\n",
       "      <td>4.137893</td>\n",
       "      <td>3.448946</td>\n",
       "      <td>9.801783</td>\n",
       "      <td>-4.944950</td>\n",
       "      <td>-1.340411</td>\n",
       "      <td>...</td>\n",
       "      <td>4.211274</td>\n",
       "      <td>0.876610</td>\n",
       "      <td>2.535916</td>\n",
       "      <td>4.460216</td>\n",
       "      <td>7.625662</td>\n",
       "      <td>5.399125</td>\n",
       "      <td>2.165930</td>\n",
       "      <td>1.335564</td>\n",
       "      <td>-6.787674</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>5.044733</td>\n",
       "      <td>2.543681</td>\n",
       "      <td>-3.247805</td>\n",
       "      <td>-5.378260</td>\n",
       "      <td>0.549456</td>\n",
       "      <td>-1.075264</td>\n",
       "      <td>6.259791</td>\n",
       "      <td>5.443143</td>\n",
       "      <td>1.828774</td>\n",
       "      <td>5.774058</td>\n",
       "      <td>...</td>\n",
       "      <td>5.602760</td>\n",
       "      <td>-0.438914</td>\n",
       "      <td>-1.589318</td>\n",
       "      <td>4.962416</td>\n",
       "      <td>2.475589</td>\n",
       "      <td>-6.530942</td>\n",
       "      <td>-0.353212</td>\n",
       "      <td>1.797986</td>\n",
       "      <td>-2.464886</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-0.939149</td>\n",
       "      <td>0.704528</td>\n",
       "      <td>0.749587</td>\n",
       "      <td>-1.643254</td>\n",
       "      <td>-3.556550</td>\n",
       "      <td>0.293420</td>\n",
       "      <td>3.039790</td>\n",
       "      <td>-3.084175</td>\n",
       "      <td>-3.810488</td>\n",
       "      <td>3.759489</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.860520</td>\n",
       "      <td>-0.330673</td>\n",
       "      <td>2.862438</td>\n",
       "      <td>-0.245570</td>\n",
       "      <td>2.726983</td>\n",
       "      <td>-3.573323</td>\n",
       "      <td>-3.095024</td>\n",
       "      <td>0.883057</td>\n",
       "      <td>-0.973858</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-1.177986</td>\n",
       "      <td>1.175732</td>\n",
       "      <td>-1.834498</td>\n",
       "      <td>1.306601</td>\n",
       "      <td>-0.493694</td>\n",
       "      <td>1.298445</td>\n",
       "      <td>2.197183</td>\n",
       "      <td>4.541283</td>\n",
       "      <td>1.859666</td>\n",
       "      <td>3.284920</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.512394</td>\n",
       "      <td>-5.162504</td>\n",
       "      <td>0.902922</td>\n",
       "      <td>7.131485</td>\n",
       "      <td>-3.511894</td>\n",
       "      <td>4.393147</td>\n",
       "      <td>-5.885215</td>\n",
       "      <td>0.389388</td>\n",
       "      <td>5.135883</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.390329</td>\n",
       "      <td>-1.530953</td>\n",
       "      <td>-2.521950</td>\n",
       "      <td>-4.864147</td>\n",
       "      <td>-0.742480</td>\n",
       "      <td>0.146031</td>\n",
       "      <td>-0.156536</td>\n",
       "      <td>1.740131</td>\n",
       "      <td>-2.943921</td>\n",
       "      <td>5.280754</td>\n",
       "      <td>...</td>\n",
       "      <td>1.704322</td>\n",
       "      <td>5.339616</td>\n",
       "      <td>3.019771</td>\n",
       "      <td>6.336881</td>\n",
       "      <td>1.606373</td>\n",
       "      <td>-2.136702</td>\n",
       "      <td>-7.156015</td>\n",
       "      <td>3.795955</td>\n",
       "      <td>1.753440</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "61  -6.639910 -2.575970 -4.333867  0.348634 -6.591602  4.137893  3.448946   \n",
       "354  5.044733  2.543681 -3.247805 -5.378260  0.549456 -1.075264  6.259791   \n",
       "358 -0.939149  0.704528  0.749587 -1.643254 -3.556550  0.293420  3.039790   \n",
       "275 -1.177986  1.175732 -1.834498  1.306601 -0.493694  1.298445  2.197183   \n",
       "18   0.390329 -1.530953 -2.521950 -4.864147 -0.742480  0.146031 -0.156536   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...    dim_56    dim_57    dim_58  \\\n",
       "61   9.801783 -4.944950 -1.340411  ...  4.211274  0.876610  2.535916   \n",
       "354  5.443143  1.828774  5.774058  ...  5.602760 -0.438914 -1.589318   \n",
       "358 -3.084175 -3.810488  3.759489  ... -4.860520 -0.330673  2.862438   \n",
       "275  4.541283  1.859666  3.284920  ... -2.512394 -5.162504  0.902922   \n",
       "18   1.740131 -2.943921  5.280754  ...  1.704322  5.339616  3.019771   \n",
       "\n",
       "       dim_59    dim_60    dim_61    dim_62    dim_63    dim_64   drag  \n",
       "61   4.460216  7.625662  5.399125  2.165930  1.335564 -6.787674  0.375  \n",
       "354  4.962416  2.475589 -6.530942 -0.353212  1.797986 -2.464886  0.374  \n",
       "358 -0.245570  2.726983 -3.573323 -3.095024  0.883057 -0.973858  0.435  \n",
       "275  7.131485 -3.511894  4.393147 -5.885215  0.389388  5.135883  0.437  \n",
       "18   6.336881  1.606373 -2.136702 -7.156015  3.795955  1.753440  0.367  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#surrogate models\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_file = './body_vectors_drags.csv'\n",
    "df = TabularDataset(data_file)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=777)\n",
    "\n",
    "#exclue the first two columns of train data\n",
    "train_data = train_df.drop(columns=['i', 'name'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count    351.000000\n",
      "mean       0.398513\n",
      "std        0.060013\n",
      "min        0.278000\n",
      "25%        0.353000\n",
      "50%        0.394000\n",
      "75%        0.435000\n",
      "max        0.598000\n",
      "Name: drag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label = 'drag'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./agModels-body\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"./agModels-body/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #51-Ubuntu SMP Mon Jul 4 06:41:22 UTC 2022\n",
      "Train Data Rows:    351\n",
      "Train Data Columns: 64\n",
      "Label Column: drag\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (0.598, 0.278, 0.39851, 0.06001)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    235480.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 64 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 64 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t64 features in original data used to generate 64 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 280, Val Rows: 71\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.0513\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.0508\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-0.0527\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.0513\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.0526\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.0517\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.0523\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t-0.0543\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.0534\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.0473\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-0.0547\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.0458\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 18.51s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./agModels-body/\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = './agModels-body'  # specifies folder to store trained models\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "metric = 'root_mean_squared_error' #Regression:mean_absolute_error, mean_squared_error,root_mean_squared_error (default), r2\n",
    "predictor = TabularPredictor(label=label, path=save_path, eval_metric=metric).fit(train_data, presets='best_quality', num_bag_folds=0, num_bag_sets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_55</th>\n",
       "      <th>dim_56</th>\n",
       "      <th>dim_57</th>\n",
       "      <th>dim_58</th>\n",
       "      <th>dim_59</th>\n",
       "      <th>dim_60</th>\n",
       "      <th>dim_61</th>\n",
       "      <th>dim_62</th>\n",
       "      <th>dim_63</th>\n",
       "      <th>dim_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.262418</td>\n",
       "      <td>4.623484</td>\n",
       "      <td>5.207058</td>\n",
       "      <td>1.474604</td>\n",
       "      <td>5.149589</td>\n",
       "      <td>1.366774</td>\n",
       "      <td>7.350364</td>\n",
       "      <td>4.988407</td>\n",
       "      <td>-4.451728</td>\n",
       "      <td>2.715955</td>\n",
       "      <td>...</td>\n",
       "      <td>3.239450</td>\n",
       "      <td>0.271440</td>\n",
       "      <td>-1.450751</td>\n",
       "      <td>6.201051</td>\n",
       "      <td>2.322523</td>\n",
       "      <td>-0.651795</td>\n",
       "      <td>-2.503192</td>\n",
       "      <td>-1.329733</td>\n",
       "      <td>5.614099</td>\n",
       "      <td>4.784914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.815653</td>\n",
       "      <td>0.467063</td>\n",
       "      <td>-0.713573</td>\n",
       "      <td>-2.519432</td>\n",
       "      <td>-6.833046</td>\n",
       "      <td>3.925573</td>\n",
       "      <td>3.131740</td>\n",
       "      <td>2.785505</td>\n",
       "      <td>-0.169789</td>\n",
       "      <td>3.797550</td>\n",
       "      <td>...</td>\n",
       "      <td>8.718956</td>\n",
       "      <td>-4.833921</td>\n",
       "      <td>-1.387358</td>\n",
       "      <td>1.010315</td>\n",
       "      <td>0.467406</td>\n",
       "      <td>-7.262593</td>\n",
       "      <td>0.406025</td>\n",
       "      <td>-4.430325</td>\n",
       "      <td>-4.493312</td>\n",
       "      <td>3.210908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.128397</td>\n",
       "      <td>-0.347771</td>\n",
       "      <td>-4.491439</td>\n",
       "      <td>-0.143762</td>\n",
       "      <td>-1.139696</td>\n",
       "      <td>1.523660</td>\n",
       "      <td>4.283424</td>\n",
       "      <td>1.275272</td>\n",
       "      <td>-3.277825</td>\n",
       "      <td>-0.944104</td>\n",
       "      <td>...</td>\n",
       "      <td>2.371663</td>\n",
       "      <td>-0.860736</td>\n",
       "      <td>-2.739771</td>\n",
       "      <td>4.489244</td>\n",
       "      <td>4.020984</td>\n",
       "      <td>1.973119</td>\n",
       "      <td>3.365454</td>\n",
       "      <td>-2.144289</td>\n",
       "      <td>1.310637</td>\n",
       "      <td>-1.411514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.334880</td>\n",
       "      <td>-0.358754</td>\n",
       "      <td>-5.121314</td>\n",
       "      <td>-1.869275</td>\n",
       "      <td>0.053525</td>\n",
       "      <td>2.998930</td>\n",
       "      <td>-0.482418</td>\n",
       "      <td>1.283854</td>\n",
       "      <td>-1.355309</td>\n",
       "      <td>-0.988547</td>\n",
       "      <td>...</td>\n",
       "      <td>4.671710</td>\n",
       "      <td>-1.422821</td>\n",
       "      <td>0.833765</td>\n",
       "      <td>5.060172</td>\n",
       "      <td>3.785773</td>\n",
       "      <td>1.105779</td>\n",
       "      <td>2.158277</td>\n",
       "      <td>-2.020028</td>\n",
       "      <td>-0.746978</td>\n",
       "      <td>1.548296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.339348</td>\n",
       "      <td>2.649365</td>\n",
       "      <td>-1.346519</td>\n",
       "      <td>0.872298</td>\n",
       "      <td>-2.672357</td>\n",
       "      <td>1.983254</td>\n",
       "      <td>2.075392</td>\n",
       "      <td>6.573480</td>\n",
       "      <td>-0.809723</td>\n",
       "      <td>-2.231186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290699</td>\n",
       "      <td>-0.446408</td>\n",
       "      <td>-5.342670</td>\n",
       "      <td>5.205126</td>\n",
       "      <td>5.651273</td>\n",
       "      <td>-2.507123</td>\n",
       "      <td>1.437673</td>\n",
       "      <td>-3.640510</td>\n",
       "      <td>-1.535356</td>\n",
       "      <td>4.507751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "46   3.262418  4.623484  5.207058  1.474604  5.149589  1.366774  7.350364   \n",
       "101 -1.815653  0.467063 -0.713573 -2.519432 -6.833046  3.925573  3.131740   \n",
       "175 -0.128397 -0.347771 -4.491439 -0.143762 -1.139696  1.523660  4.283424   \n",
       "9   -0.334880 -0.358754 -5.121314 -1.869275  0.053525  2.998930 -0.482418   \n",
       "136  0.339348  2.649365 -1.346519  0.872298 -2.672357  1.983254  2.075392   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...    dim_55    dim_56    dim_57  \\\n",
       "46   4.988407 -4.451728  2.715955  ...  3.239450  0.271440 -1.450751   \n",
       "101  2.785505 -0.169789  3.797550  ...  8.718956 -4.833921 -1.387358   \n",
       "175  1.275272 -3.277825 -0.944104  ...  2.371663 -0.860736 -2.739771   \n",
       "9    1.283854 -1.355309 -0.988547  ...  4.671710 -1.422821  0.833765   \n",
       "136  6.573480 -0.809723 -2.231186  ...  0.290699 -0.446408 -5.342670   \n",
       "\n",
       "       dim_58    dim_59    dim_60    dim_61    dim_62    dim_63    dim_64  \n",
       "46   6.201051  2.322523 -0.651795 -2.503192 -1.329733  5.614099  4.784914  \n",
       "101  1.010315  0.467406 -7.262593  0.406025 -4.430325 -4.493312  3.210908  \n",
       "175  4.489244  4.020984  1.973119  3.365454 -2.144289  1.310637 -1.411514  \n",
       "9    5.060172  3.785773  1.105779  2.158277 -2.020028 -0.746978  1.548296  \n",
       "136  5.205126  5.651273 -2.507123  1.437673 -3.640510 -1.535356  4.507751  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_df.drop(columns=['i', 'name'])\n",
    "# val_data.head()\n",
    "y_val = test_data[label]\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.050034147274332794\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.050034147274332794,\n",
      "    \"mean_squared_error\": -0.002503415893469624,\n",
      "    \"mean_absolute_error\": -0.03686644413525408,\n",
      "    \"r2\": 0.1486749360125207,\n",
      "    \"pearsonr\": 0.3991500258208202,\n",
      "    \"median_absolute_error\": -0.02989608228206636\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%capture log_output\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config Application.log_level = 'DEBUG'\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "for item in y_pred:\n",
    "    print(item)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_val, y_pred=y_pred, auxiliary_metrics=True)\n",
    "print(perf)\n",
    "\n",
    "results = predictor.fit_summary(show_plot=True)\n",
    "print(results)\n",
    "print(predictor.leaderboard(test_data, silent=True))\n",
    "\n",
    "with open('./output_body.log', 'w') as f:\n",
    "    f.write(log_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is:  regression\n",
      "AutoGluon identified the following types of features:\n",
      "('float', []) : 64 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39179033041000366\n",
      "0.3810622990131378\n",
      "0.42376548051834106\n",
      "0.4437100291252136\n",
      "0.363418310880661\n",
      "0.3789520263671875\n",
      "0.403035044670105\n",
      "0.4026564955711365\n",
      "0.3625396490097046\n",
      "0.3624393939971924\n",
      "0.3658306300640106\n",
      "0.4688659608364105\n",
      "0.4446702003479004\n",
      "0.3616076707839966\n",
      "0.43355903029441833\n",
      "0.4477322995662689\n",
      "0.3285706639289856\n",
      "0.4058804214000702\n",
      "0.48094141483306885\n",
      "0.3682215213775635\n",
      "0.3887735605239868\n",
      "0.45548808574676514\n",
      "0.3750140070915222\n",
      "0.44727540016174316\n",
      "0.379258930683136\n",
      "0.42059534788131714\n",
      "0.3870013356208801\n",
      "0.4131103754043579\n",
      "0.3449678421020508\n",
      "0.3451216518878937\n",
      "0.41435036063194275\n",
      "0.43246224522590637\n",
      "0.46546685695648193\n",
      "0.45046740770339966\n",
      "0.35544735193252563\n",
      "0.36518368124961853\n",
      "0.3741125166416168\n",
      "0.41998228430747986\n",
      "0.4059818387031555\n",
      "0.3419978618621826\n",
      "0.48325586318969727\n",
      "0.37301015853881836\n",
      "0.3563459515571594\n",
      "0.36194753646850586\n",
      "0.373629629611969\n",
      "0.4176792800426483\n",
      "0.4082556962966919\n",
      "0.4265376925468445\n",
      "0.371904194355011\n",
      "0.4069189429283142\n",
      "0.38090550899505615\n",
      "0.3891986012458801\n",
      "0.351724773645401\n",
      "0.3393004238605499\n",
      "0.4386860132217407\n",
      "0.39035773277282715\n",
      "0.4195917844772339\n",
      "0.3641114830970764\n",
      "0.3356676697731018\n",
      "0.3984665870666504\n",
      "0.3862329423427582\n",
      "0.3990768790245056\n",
      "0.4336708188056946\n",
      "0.3780348598957062\n",
      "0.35709524154663086\n",
      "0.43270277976989746\n",
      "0.3989928364753723\n",
      "0.3341268002986908\n",
      "0.4349290728569031\n",
      "0.36943143606185913\n",
      "0.4094802439212799\n",
      "0.398168683052063\n",
      "0.38452884554862976\n",
      "0.3747814893722534\n",
      "0.406643807888031\n",
      "0.4055778980255127\n",
      "0.3918313980102539\n",
      "0.4384373426437378\n",
      "0.3612135648727417\n",
      "0.41370850801467896\n",
      "0.4387841522693634\n",
      "0.47993525862693787\n",
      "0.34867388010025024\n",
      "0.3437376618385315\n",
      "0.36352819204330444\n",
      "0.44293391704559326\n",
      "0.3840710520744324\n",
      "0.432060569524765\n",
      "0.4026172459125519\n",
      "0.425356388092041\n",
      "0.38211697340011597\n",
      "0.33712002635002136\n",
      "0.40696126222610474\n",
      "0.35744529962539673\n",
      "0.440382182598114\n",
      "0.36362332105636597\n",
      "0.37933456897735596\n",
      "0.3398241400718689\n",
      "0.4669704735279083\n",
      "0.3523421287536621\n",
      "0.43756529688835144\n",
      "0.3746878504753113\n",
      "0.3734244704246521\n",
      "0.4034900665283203\n",
      "0.4176720380783081\n",
      "0.35631901025772095\n",
      "0.3617863655090332\n",
      "0.45573580265045166\n",
      "0.4109218716621399\n",
      "0.459861159324646\n",
      "0.33083996176719666\n",
      "0.4205053448677063\n",
      "0.35646677017211914\n",
      "0.40360432863235474\n",
      "0.4371391534805298\n",
      "0.38796737790107727\n",
      "0.3305729329586029\n",
      "0.349462628364563\n",
      "0.48497310280799866\n",
      "0.3669013977050781\n",
      "0.4198072850704193\n",
      "0.39795982837677\n",
      "0.3819107413291931\n",
      "0.4236255884170532\n",
      "0.4028424620628357\n",
      "0.35897690057754517\n",
      "0.4477386176586151\n",
      "0.46614018082618713\n",
      "0.4040706753730774\n",
      "0.38181233406066895\n",
      "0.3870830535888672\n",
      "0.4546618163585663\n",
      "0.35890993475914\n",
      "0.429970920085907\n",
      "0.4195497930049896\n",
      "0.4261748492717743\n",
      "0.4111995995044708\n",
      "0.44467246532440186\n",
      "0.37008407711982727\n",
      "0.3732496500015259\n",
      "0.43466639518737793\n",
      "0.43387371301651\n",
      "0.361432284116745\n",
      "0.36641478538513184\n",
      "0.4060188829898834\n",
      "0.36824139952659607\n",
      "0.4192717671394348\n",
      "0.4193376898765564\n",
      "0.4605945348739624\n",
      "0.3684576153755188\n",
      "0.4385855793952942\n",
      "0.4382896423339844\n",
      "0.4738922417163849\n",
      "0.41832852363586426\n",
      "0.3815518617630005\n",
      "0.3506256937980652\n",
      "0.3955523371696472\n",
      "0.43625593185424805\n",
      "0.4251198172569275\n",
      "0.42316752672195435\n",
      "0.4599953591823578\n",
      "0.3414871096611023\n",
      "0.4475749731063843\n",
      "0.4009304344654083\n",
      "0.3519303798675537\n",
      "0.3528866171836853\n",
      "0.42258864641189575\n",
      "0.4403831958770752\n",
      "0.3302047848701477\n",
      "0.4398511052131653\n",
      "0.35242927074432373\n",
      "0.38859403133392334\n",
      "0.4083986282348633\n",
      "0.3981284201145172\n",
      "0.3674815893173218\n",
      "0.44683367013931274\n",
      "0.44052058458328247\n",
      "0.3734694719314575\n",
      "0.4343143105506897\n",
      "0.42915478348731995\n",
      "0.3920208215713501\n",
      "0.3502066731452942\n",
      "0.3892451524734497\n",
      "0.38189414143562317\n",
      "0.4068814516067505\n",
      "0.3519914448261261\n",
      "0.4287808835506439\n",
      "0.34577226638793945\n",
      "0.45402586460113525\n",
      "0.38019847869873047\n",
      "0.43842053413391113\n",
      "0.35170862078666687\n",
      "0.37990331649780273\n",
      "0.4940207600593567\n",
      "0.47117194533348083\n",
      "0.42482632398605347\n",
      "0.3909876346588135\n",
      "0.4582611918449402\n",
      "0.3853916823863983\n",
      "0.44656887650489807\n",
      "0.3983720541000366\n",
      "0.4053943157196045\n",
      "0.357313871383667\n",
      "0.3364390432834625\n",
      "0.44402188062667847\n",
      "0.36396485567092896\n",
      "0.4005166292190552\n",
      "0.41189998388290405\n",
      "0.4431803226470947\n",
      "0.392319917678833\n",
      "0.46936747431755066\n",
      "0.3638980984687805\n",
      "0.35961538553237915\n",
      "0.39887329936027527\n",
      "0.3890080451965332\n",
      "0.39134901762008667\n",
      "0.37236201763153076\n",
      "0.43158575892448425\n",
      "0.4017951488494873\n",
      "0.40765687823295593\n",
      "0.4159657955169678\n",
      "0.34243839979171753\n",
      "0.46071213483810425\n",
      "0.3712117671966553\n",
      "0.44097864627838135\n",
      "0.335355669260025\n",
      "0.3544788062572479\n",
      "0.42140090465545654\n",
      "0.390938401222229\n",
      "0.3735673427581787\n",
      "0.41216251254081726\n",
      "0.33971452713012695\n",
      "0.424628347158432\n",
      "0.38397037982940674\n",
      "0.3585411310195923\n",
      "0.34322279691696167\n",
      "0.46808040142059326\n",
      "0.3525606095790863\n",
      "0.4369989037513733\n",
      "0.36798155307769775\n",
      "0.345698744058609\n",
      "0.38643768429756165\n",
      "0.34328627586364746\n",
      "0.3986402750015259\n",
      "0.3913796544075012\n",
      "0.3798753619194031\n",
      "0.4062117338180542\n",
      "0.4403465688228607\n",
      "0.36125513911247253\n",
      "0.4003375768661499\n",
      "0.36312007904052734\n",
      "0.37295016646385193\n",
      "0.3938447833061218\n",
      "0.4087449908256531\n",
      "0.3932618200778961\n",
      "0.4471514821052551\n",
      "0.3480796217918396\n",
      "0.44845664501190186\n",
      "0.4394400715827942\n",
      "0.4088708758354187\n",
      "0.36331287026405334\n",
      "0.36194872856140137\n",
      "0.44353586435317993\n",
      "0.3341522514820099\n",
      "0.45961621403694153\n",
      "0.3844027519226074\n",
      "0.39505207538604736\n",
      "0.46563923358917236\n",
      "0.42559319734573364\n",
      "0.3932591378688812\n",
      "0.45250141620635986\n",
      "0.4429479241371155\n",
      "0.3998779356479645\n",
      "0.33358657360076904\n",
      "0.3447881042957306\n",
      "0.43362170457839966\n",
      "0.32254451513290405\n",
      "0.3652035892009735\n",
      "0.45550185441970825\n",
      "0.35779210925102234\n",
      "0.43086493015289307\n",
      "0.3576417565345764\n",
      "0.4210144281387329\n",
      "0.39647376537323\n",
      "0.3815433382987976\n",
      "0.3398491144180298\n",
      "0.33094292879104614\n",
      "0.4329127371311188\n",
      "0.4063296914100647\n",
      "0.4820114076137543\n",
      "0.4155697226524353\n",
      "0.3727315664291382\n",
      "0.3310588598251343\n",
      "0.41393351554870605\n",
      "0.36859166622161865\n",
      "0.4242568016052246\n",
      "0.35722142457962036\n",
      "0.4479307532310486\n",
      "0.3544079661369324\n",
      "0.438050240278244\n",
      "0.3842957019805908\n",
      "0.35532283782958984\n",
      "0.4010644555091858\n",
      "0.3584701418876648\n",
      "0.3164687752723694\n",
      "0.40437331795692444\n",
      "0.4051583409309387\n",
      "0.3787952661514282\n",
      "0.3692706823348999\n",
      "0.448263943195343\n",
      "0.49721527099609375\n",
      "0.48071879148483276\n",
      "0.44185906648635864\n",
      "0.3728625774383545\n",
      "0.4803334176540375\n",
      "0.44383054971694946\n",
      "0.3985189199447632\n",
      "0.4213070869445801\n",
      "0.3864481449127197\n",
      "0.40987876057624817\n",
      "0.5030591487884521\n",
      "0.3494316637516022\n",
      "0.33755818009376526\n",
      "0.37074553966522217\n",
      "0.3359028398990631\n",
      "0.45373356342315674\n",
      "0.423553466796875\n",
      "0.38614359498023987\n",
      "0.3251833915710449\n",
      "0.38112425804138184\n",
      "0.4098644256591797\n",
      "0.3258691132068634\n",
      "0.4230591356754303\n",
      "0.4578569829463959\n",
      "0.4056326448917389\n",
      "0.4515288174152374\n",
      "0.4309972822666168\n",
      "0.45651209354400635\n",
      "0.33874791860580444\n",
      "0.3685157895088196\n",
      "0.3426671028137207\n",
      "0.40123268961906433\n",
      "0.34625858068466187\n",
      "0.4308778643608093\n",
      "0.4304735064506531\n",
      "0.4681724011898041\n",
      "0.3430599570274353\n",
      "0.4329999089241028\n",
      "0.35600799322128296\n",
      "0.3972819447517395\n",
      "0.3452805280685425\n"
     ]
    }
   ],
   "source": [
    "train_data_pred = predictor.predict(train_data, model='WeightedEnsemble_L2')\n",
    "for item in train_data_pred:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3680879473686218\n",
      "0.3668398857116699\n",
      "0.4162694215774536\n",
      "0.3744370937347412\n",
      "0.39999914169311523\n",
      "0.41955405473709106\n",
      "0.43098902702331543\n",
      "0.44742828607559204\n",
      "0.3989654779434204\n",
      "0.3661521077156067\n",
      "0.38885247707366943\n",
      "0.4264290928840637\n",
      "0.3580113649368286\n",
      "0.41094690561294556\n",
      "0.41759610176086426\n",
      "0.4063419997692108\n",
      "0.37049055099487305\n",
      "0.3720453083515167\n",
      "0.36963918805122375\n",
      "0.36872708797454834\n",
      "0.42646923661231995\n",
      "0.39879709482192993\n",
      "0.36635568737983704\n",
      "0.4158444106578827\n",
      "0.442308634519577\n",
      "0.3707406520843506\n",
      "0.42948588728904724\n",
      "0.38404983282089233\n",
      "0.37580224871635437\n",
      "0.3908371329307556\n",
      "0.4203270971775055\n",
      "0.3802730441093445\n",
      "0.3808373808860779\n",
      "0.4379156231880188\n",
      "0.4138909876346588\n",
      "0.3898378610610962\n",
      "0.37178879976272583\n",
      "0.3968990445137024\n",
      "0.3959178626537323\n",
      "0.38958024978637695\n",
      "0.43402737379074097\n",
      "0.3811335563659668\n",
      "0.4189939796924591\n",
      "0.3743710517883301\n",
      "0.3966352939605713\n",
      "0.401253342628479\n",
      "0.39957699179649353\n",
      "0.37944507598876953\n",
      "0.335768461227417\n",
      "0.38161516189575195\n",
      "0.40289002656936646\n",
      "0.3390030264854431\n",
      "0.4211916923522949\n",
      "0.3930211067199707\n",
      "0.37040960788726807\n",
      "0.40101999044418335\n",
      "0.368929922580719\n",
      "0.43083256483078003\n",
      "0.3626163601875305\n",
      "0.3866497278213501\n",
      "0.40893131494522095\n",
      "0.38439199328422546\n",
      "0.3772504925727844\n",
      "0.41055381298065186\n",
      "0.3591257631778717\n",
      "0.3452332615852356\n",
      "0.4014890789985657\n",
      "0.3490194082260132\n",
      "0.3945191204547882\n",
      "0.40460291504859924\n",
      "0.386159747838974\n",
      "0.3862611651420593\n",
      "0.36590975522994995\n",
      "0.4115165174007416\n",
      "0.3706233501434326\n",
      "0.41281044483184814\n",
      "0.41003432869911194\n",
      "0.37300604581832886\n",
      "0.39678993821144104\n",
      "0.3649914264678955\n",
      "0.3774581551551819\n",
      "0.4209333062171936\n",
      "0.37249284982681274\n",
      "0.37900859117507935\n",
      "0.3868088722229004\n",
      "0.4157624840736389\n",
      "0.40901118516921997\n",
      "0.39662501215934753\n"
     ]
    }
   ],
   "source": [
    "test_data_pred = predictor.predict(test_data, model='WeightedEnsemble_L2')\n",
    "for item in test_data_pred:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
