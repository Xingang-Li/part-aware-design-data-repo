{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xli/anaconda3/envs/surrogate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_120</th>\n",
       "      <th>dim_121</th>\n",
       "      <th>dim_122</th>\n",
       "      <th>dim_123</th>\n",
       "      <th>dim_124</th>\n",
       "      <th>dim_125</th>\n",
       "      <th>dim_126</th>\n",
       "      <th>dim_127</th>\n",
       "      <th>dim_128</th>\n",
       "      <th>drag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.996583</td>\n",
       "      <td>0.253377</td>\n",
       "      <td>2.345441</td>\n",
       "      <td>-0.989086</td>\n",
       "      <td>-2.179726</td>\n",
       "      <td>4.450816</td>\n",
       "      <td>-0.333054</td>\n",
       "      <td>-0.833110</td>\n",
       "      <td>0.170525</td>\n",
       "      <td>0.681121</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321793</td>\n",
       "      <td>-0.977492</td>\n",
       "      <td>0.493258</td>\n",
       "      <td>-2.447803</td>\n",
       "      <td>-0.851386</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>-0.704168</td>\n",
       "      <td>0.301570</td>\n",
       "      <td>0.179546</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-0.107085</td>\n",
       "      <td>1.786517</td>\n",
       "      <td>1.586284</td>\n",
       "      <td>-1.288352</td>\n",
       "      <td>-1.469519</td>\n",
       "      <td>1.856284</td>\n",
       "      <td>0.022899</td>\n",
       "      <td>0.707441</td>\n",
       "      <td>0.195338</td>\n",
       "      <td>0.934119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785785</td>\n",
       "      <td>-1.589034</td>\n",
       "      <td>-3.525021</td>\n",
       "      <td>-4.223967</td>\n",
       "      <td>-2.653476</td>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.977202</td>\n",
       "      <td>-0.662360</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>-3.295154</td>\n",
       "      <td>-1.133777</td>\n",
       "      <td>-0.713116</td>\n",
       "      <td>0.946432</td>\n",
       "      <td>0.027521</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.565126</td>\n",
       "      <td>-1.224105</td>\n",
       "      <td>-0.519147</td>\n",
       "      <td>0.146645</td>\n",
       "      <td>...</td>\n",
       "      <td>2.272549</td>\n",
       "      <td>-0.996835</td>\n",
       "      <td>5.970934</td>\n",
       "      <td>0.562599</td>\n",
       "      <td>3.063003</td>\n",
       "      <td>-0.003317</td>\n",
       "      <td>1.284646</td>\n",
       "      <td>-0.388674</td>\n",
       "      <td>0.367492</td>\n",
       "      <td>0.435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.809618</td>\n",
       "      <td>-1.043127</td>\n",
       "      <td>0.594460</td>\n",
       "      <td>1.935878</td>\n",
       "      <td>0.173827</td>\n",
       "      <td>1.941216</td>\n",
       "      <td>-0.689809</td>\n",
       "      <td>-1.535557</td>\n",
       "      <td>1.745006</td>\n",
       "      <td>0.349334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602183</td>\n",
       "      <td>-0.519529</td>\n",
       "      <td>0.047703</td>\n",
       "      <td>1.788710</td>\n",
       "      <td>-0.448435</td>\n",
       "      <td>-0.003273</td>\n",
       "      <td>-0.998566</td>\n",
       "      <td>-0.302207</td>\n",
       "      <td>0.684875</td>\n",
       "      <td>0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.572277</td>\n",
       "      <td>-0.328782</td>\n",
       "      <td>0.444743</td>\n",
       "      <td>0.411545</td>\n",
       "      <td>-1.628947</td>\n",
       "      <td>3.242414</td>\n",
       "      <td>-0.008619</td>\n",
       "      <td>-0.790840</td>\n",
       "      <td>0.288297</td>\n",
       "      <td>0.432827</td>\n",
       "      <td>...</td>\n",
       "      <td>2.103731</td>\n",
       "      <td>-2.167353</td>\n",
       "      <td>-1.278066</td>\n",
       "      <td>-1.638192</td>\n",
       "      <td>2.981116</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>-0.616031</td>\n",
       "      <td>-0.505367</td>\n",
       "      <td>0.432376</td>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "61  -0.996583  0.253377  2.345441 -0.989086 -2.179726  4.450816 -0.333054   \n",
       "354 -0.107085  1.786517  1.586284 -1.288352 -1.469519  1.856284  0.022899   \n",
       "358 -3.295154 -1.133777 -0.713116  0.946432  0.027521  0.023506  0.565126   \n",
       "275 -0.809618 -1.043127  0.594460  1.935878  0.173827  1.941216 -0.689809   \n",
       "18  -0.572277 -0.328782  0.444743  0.411545 -1.628947  3.242414 -0.008619   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...   dim_120   dim_121   dim_122  \\\n",
       "61  -0.833110  0.170525  0.681121  ...  1.321793 -0.977492  0.493258   \n",
       "354  0.707441  0.195338  0.934119  ...  0.785785 -1.589034 -3.525021   \n",
       "358 -1.224105 -0.519147  0.146645  ...  2.272549 -0.996835  5.970934   \n",
       "275 -1.535557  1.745006  0.349334  ...  0.602183 -0.519529  0.047703   \n",
       "18  -0.790840  0.288297  0.432827  ...  2.103731 -2.167353 -1.278066   \n",
       "\n",
       "      dim_123   dim_124   dim_125   dim_126   dim_127   dim_128   drag  \n",
       "61  -2.447803 -0.851386  0.003659 -0.704168  0.301570  0.179546  0.375  \n",
       "354 -4.223967 -2.653476  0.012422  0.597222  0.977202 -0.662360  0.374  \n",
       "358  0.562599  3.063003 -0.003317  1.284646 -0.388674  0.367492  0.435  \n",
       "275  1.788710 -0.448435 -0.003273 -0.998566 -0.302207  0.684875  0.437  \n",
       "18  -1.638192  2.981116  0.002460 -0.616031 -0.505367  0.432376  0.367  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#surrogate models\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_file = './spvae_vectors_drags.csv'\n",
    "df = TabularDataset(data_file)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=777)\n",
    "\n",
    "#exclue the first two columns of train data\n",
    "train_data = train_df.drop(columns=['i', 'name'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 , 0.375\n",
      "354 , 0.374\n",
      "358 , 0.435\n",
      "275 , 0.437\n",
      "18 , 0.367\n",
      "107 , 0.36\n",
      "57 , 0.456\n",
      "430 , 0.386\n",
      "374 , 0.345\n",
      "179 , 0.345\n",
      "287 , 0.341\n",
      "51 , 0.55\n",
      "395 , 0.48\n",
      "133 , 0.371\n",
      "233 , 0.427\n",
      "210 , 0.478\n",
      "151 , 0.294\n",
      "375 , 0.411\n",
      "24 , 0.538\n",
      "311 , 0.356\n",
      "152 , 0.429\n",
      "369 , 0.478\n",
      "8 , 0.317\n",
      "90 , 0.439\n",
      "47 , 0.418\n",
      "305 , 0.423\n",
      "421 , 0.377\n",
      "253 , 0.418\n",
      "167 , 0.322\n",
      "315 , 0.355\n",
      "345 , 0.489\n",
      "366 , 0.42\n",
      "41 , 0.51\n",
      "359 , 0.453\n",
      "269 , 0.362\n",
      "56 , 0.375\n",
      "193 , 0.408\n",
      "283 , 0.513\n",
      "204 , 0.315\n",
      "224 , 0.333\n",
      "316 , 0.571\n",
      "19 , 0.375\n",
      "150 , 0.33\n",
      "118 , 0.298\n",
      "58 , 0.341\n",
      "223 , 0.359\n",
      "1 , 0.393\n",
      "226 , 0.403\n",
      "124 , 0.339\n",
      "232 , 0.404\n",
      "278 , 0.404\n",
      "342 , 0.344\n",
      "26 , 0.4\n",
      "390 , 0.326\n",
      "379 , 0.417\n",
      "81 , 0.384\n",
      "236 , 0.432\n",
      "100 , 0.374\n",
      "383 , 0.315\n",
      "148 , 0.334\n",
      "3 , 0.337\n",
      "371 , 0.329\n",
      "123 , 0.446\n",
      "231 , 0.348\n",
      "248 , 0.37\n",
      "39 , 0.379\n",
      "120 , 0.439\n",
      "273 , 0.317\n",
      "29 , 0.436\n",
      "378 , 0.371\n",
      "352 , 0.429\n",
      "154 , 0.362\n",
      "407 , 0.347\n",
      "131 , 0.357\n",
      "261 , 0.387\n",
      "262 , 0.402\n",
      "382 , 0.468\n",
      "10 , 0.425\n",
      "88 , 0.402\n",
      "372 , 0.445\n",
      "73 , 0.417\n",
      "234 , 0.516\n",
      "106 , 0.307\n",
      "213 , 0.334\n",
      "364 , 0.356\n",
      "206 , 0.434\n",
      "33 , 0.38\n",
      "437 , 0.456\n",
      "353 , 0.392\n",
      "176 , 0.397\n",
      "337 , 0.336\n",
      "399 , 0.304\n",
      "335 , 0.469\n",
      "140 , 0.349\n",
      "256 , 0.427\n",
      "250 , 0.344\n",
      "419 , 0.373\n",
      "98 , 0.335\n",
      "373 , 0.516\n",
      "93 , 0.335\n",
      "208 , 0.423\n",
      "149 , 0.431\n",
      "70 , 0.358\n",
      "336 , 0.416\n",
      "13 , 0.435\n",
      "293 , 0.322\n",
      "327 , 0.315\n",
      "212 , 0.441\n",
      "319 , 0.404\n",
      "289 , 0.518\n",
      "119 , 0.294\n",
      "76 , 0.418\n",
      "158 , 0.355\n",
      "320 , 0.407\n",
      "191 , 0.431\n",
      "155 , 0.384\n",
      "414 , 0.324\n",
      "240 , 0.341\n",
      "67 , 0.547\n",
      "22 , 0.357\n",
      "367 , 0.408\n",
      "246 , 0.42\n",
      "160 , 0.374\n",
      "194 , 0.425\n",
      "332 , 0.454\n",
      "249 , 0.332\n",
      "143 , 0.456\n",
      "393 , 0.49\n",
      "132 , 0.366\n",
      "15 , 0.327\n",
      "137 , 0.372\n",
      "96 , 0.465\n",
      "308 , 0.322\n",
      "201 , 0.387\n",
      "209 , 0.459\n",
      "296 , 0.437\n",
      "285 , 0.388\n",
      "156 , 0.479\n",
      "114 , 0.367\n",
      "135 , 0.337\n",
      "141 , 0.441\n",
      "279 , 0.415\n",
      "329 , 0.381\n",
      "306 , 0.364\n",
      "424 , 0.402\n",
      "267 , 0.377\n",
      "403 , 0.4\n",
      "284 , 0.397\n",
      "218 , 0.474\n",
      "159 , 0.339\n",
      "247 , 0.441\n",
      "391 , 0.453\n",
      "389 , 0.509\n",
      "270 , 0.472\n",
      "207 , 0.394\n",
      "122 , 0.35\n",
      "377 , 0.387\n",
      "310 , 0.452\n",
      "434 , 0.432\n",
      "301 , 0.365\n",
      "69 , 0.477\n",
      "230 , 0.348\n",
      "121 , 0.44\n",
      "68 , 0.411\n",
      "129 , 0.338\n",
      "197 , 0.334\n",
      "111 , 0.433\n",
      "258 , 0.441\n",
      "84 , 0.306\n",
      "219 , 0.417\n",
      "216 , 0.358\n",
      "214 , 0.403\n",
      "268 , 0.407\n",
      "139 , 0.399\n",
      "386 , 0.409\n",
      "45 , 0.449\n",
      "405 , 0.478\n",
      "163 , 0.382\n",
      "105 , 0.428\n",
      "344 , 0.437\n",
      "11 , 0.429\n",
      "43 , 0.333\n",
      "82 , 0.393\n",
      "350 , 0.38\n",
      "396 , 0.403\n",
      "102 , 0.349\n",
      "220 , 0.458\n",
      "274 , 0.337\n",
      "27 , 0.46\n",
      "299 , 0.366\n",
      "66 , 0.47\n",
      "112 , 0.347\n",
      "211 , 0.375\n",
      "370 , 0.547\n",
      "35 , 0.503\n",
      "333 , 0.436\n",
      "266 , 0.415\n",
      "180 , 0.47\n",
      "254 , 0.323\n",
      "291 , 0.456\n",
      "40 , 0.387\n",
      "2 , 0.404\n",
      "244 , 0.357\n",
      "64 , 0.327\n",
      "313 , 0.445\n",
      "14 , 0.335\n",
      "94 , 0.393\n",
      "410 , 0.422\n",
      "404 , 0.414\n",
      "83 , 0.385\n",
      "288 , 0.522\n",
      "259 , 0.373\n",
      "162 , 0.355\n",
      "203 , 0.419\n",
      "113 , 0.355\n",
      "205 , 0.345\n",
      "431 , 0.373\n",
      "385 , 0.418\n",
      "42 , 0.427\n",
      "380 , 0.421\n",
      "387 , 0.361\n",
      "104 , 0.318\n",
      "317 , 0.497\n",
      "298 , 0.407\n",
      "294 , 0.483\n",
      "348 , 0.288\n",
      "394 , 0.333\n",
      "227 , 0.393\n",
      "198 , 0.363\n",
      "435 , 0.319\n",
      "126 , 0.403\n",
      "331 , 0.321\n",
      "363 , 0.417\n",
      "164 , 0.384\n",
      "251 , 0.336\n",
      "181 , 0.339\n",
      "357 , 0.51\n",
      "49 , 0.357\n",
      "432 , 0.527\n",
      "346 , 0.358\n",
      "388 , 0.424\n",
      "425 , 0.393\n",
      "323 , 0.333\n",
      "20 , 0.4\n",
      "99 , 0.409\n",
      "413 , 0.364\n",
      "415 , 0.417\n",
      "199 , 0.427\n",
      "30 , 0.357\n",
      "297 , 0.387\n",
      "343 , 0.358\n",
      "62 , 0.375\n",
      "109 , 0.402\n",
      "91 , 0.399\n",
      "44 , 0.395\n",
      "23 , 0.472\n",
      "37 , 0.328\n",
      "314 , 0.451\n",
      "125 , 0.36\n",
      "235 , 0.341\n",
      "195 , 0.333\n",
      "34 , 0.363\n",
      "182 , 0.466\n",
      "95 , 0.321\n",
      "190 , 0.481\n",
      "85 , 0.377\n",
      "272 , 0.398\n",
      "108 , 0.495\n",
      "265 , 0.471\n",
      "264 , 0.375\n",
      "17 , 0.507\n",
      "429 , 0.478\n",
      "189 , 0.397\n",
      "427 , 0.318\n",
      "217 , 0.325\n",
      "183 , 0.431\n",
      "309 , 0.296\n",
      "0 , 0.32\n",
      "341 , 0.481\n",
      "392 , 0.382\n",
      "117 , 0.446\n",
      "356 , 0.313\n",
      "361 , 0.395\n",
      "80 , 0.394\n",
      "4 , 0.377\n",
      "330 , 0.34\n",
      "228 , 0.322\n",
      "138 , 0.494\n",
      "276 , 0.394\n",
      "225 , 0.53\n",
      "221 , 0.421\n",
      "229 , 0.368\n",
      "92 , 0.296\n",
      "245 , 0.338\n",
      "28 , 0.362\n",
      "78 , 0.435\n",
      "53 , 0.361\n",
      "38 , 0.598\n",
      "384 , 0.345\n",
      "171 , 0.428\n",
      "292 , 0.371\n",
      "115 , 0.334\n",
      "237 , 0.405\n",
      "355 , 0.335\n",
      "16 , 0.323\n",
      "130 , 0.415\n",
      "186 , 0.413\n",
      "307 , 0.36\n",
      "360 , 0.335\n",
      "128 , 0.444\n",
      "376 , 0.573\n",
      "271 , 0.541\n",
      "60 , 0.438\n",
      "202 , 0.381\n",
      "322 , 0.536\n",
      "326 , 0.433\n",
      "312 , 0.454\n",
      "324 , 0.424\n",
      "365 , 0.391\n",
      "318 , 0.395\n",
      "50 , 0.576\n",
      "418 , 0.367\n",
      "74 , 0.322\n",
      "438 , 0.351\n",
      "187 , 0.327\n",
      "31 , 0.461\n",
      "65 , 0.39\n",
      "263 , 0.409\n",
      "325 , 0.278\n",
      "340 , 0.378\n",
      "338 , 0.413\n",
      "321 , 0.299\n",
      "347 , 0.435\n",
      "32 , 0.484\n",
      "142 , 0.429\n",
      "397 , 0.468\n",
      "402 , 0.441\n",
      "295 , 0.482\n",
      "280 , 0.345\n",
      "302 , 0.367\n",
      "423 , 0.355\n",
      "116 , 0.388\n",
      "127 , 0.328\n",
      "157 , 0.417\n",
      "71 , 0.419\n",
      "433 , 0.503\n",
      "87 , 0.321\n",
      "422 , 0.437\n",
      "59 , 0.338\n",
      "303 , 0.385\n",
      "103 , 0.337\n"
     ]
    }
   ],
   "source": [
    "train_drags = train_df[\"drag\"]\n",
    "train_index = train_df[\"i\"]\n",
    "for index, drag in zip(train_index, train_drags):\n",
    "    print(index, \",\" ,drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count    351.000000\n",
      "mean       0.398513\n",
      "std        0.060013\n",
      "min        0.278000\n",
      "25%        0.353000\n",
      "50%        0.394000\n",
      "75%        0.435000\n",
      "max        0.598000\n",
      "Name: drag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label = 'drag'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"./agModels-spvae\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=0, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"./agModels-spvae/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #51-Ubuntu SMP Mon Jul 4 06:41:22 UTC 2022\n",
      "Train Data Rows:    351\n",
      "Train Data Columns: 128\n",
      "Label Column: drag\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (0.598, 0.278, 0.39851, 0.06001)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    234733.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 128 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 128 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t128 features in original data used to generate 128 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 280, Val Rows: 71\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t-0.0558\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t-0.0552\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t-0.0524\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t-0.0533\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t-0.0529\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t-0.0516\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.63s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t-0.0512\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-0.0526\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t-0.0549\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t-0.052\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t-0.0514\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.0503\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 24.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./agModels-spvae/\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "save_path = './agModels-spvae'  # specifies folder to store trained models\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "metric = 'root_mean_squared_error' #Regression:mean_absolute_error, mean_squared_error,root_mean_squared_error (default), r2\n",
    "predictor = TabularPredictor(label=label, path=save_path, eval_metric=metric).fit(train_data, presets='best_quality', num_bag_folds=0, num_bag_sets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>dim_8</th>\n",
       "      <th>dim_9</th>\n",
       "      <th>dim_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_119</th>\n",
       "      <th>dim_120</th>\n",
       "      <th>dim_121</th>\n",
       "      <th>dim_122</th>\n",
       "      <th>dim_123</th>\n",
       "      <th>dim_124</th>\n",
       "      <th>dim_125</th>\n",
       "      <th>dim_126</th>\n",
       "      <th>dim_127</th>\n",
       "      <th>dim_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.675357</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>-2.380979</td>\n",
       "      <td>-0.728630</td>\n",
       "      <td>-1.412328</td>\n",
       "      <td>1.705713</td>\n",
       "      <td>-0.913276</td>\n",
       "      <td>-0.664502</td>\n",
       "      <td>-0.036643</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785418</td>\n",
       "      <td>-0.100383</td>\n",
       "      <td>-1.418045</td>\n",
       "      <td>-5.795298</td>\n",
       "      <td>-0.239288</td>\n",
       "      <td>-3.270577</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>2.296032</td>\n",
       "      <td>-0.461153</td>\n",
       "      <td>1.085652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-0.058659</td>\n",
       "      <td>0.094974</td>\n",
       "      <td>-0.148768</td>\n",
       "      <td>-1.709858</td>\n",
       "      <td>0.231730</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>-0.203162</td>\n",
       "      <td>0.288094</td>\n",
       "      <td>-0.420197</td>\n",
       "      <td>-2.209749</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.907042</td>\n",
       "      <td>0.220606</td>\n",
       "      <td>0.879496</td>\n",
       "      <td>-1.250059</td>\n",
       "      <td>-0.911256</td>\n",
       "      <td>2.207671</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>1.828314</td>\n",
       "      <td>-0.745860</td>\n",
       "      <td>-1.381517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.408514</td>\n",
       "      <td>-0.318794</td>\n",
       "      <td>-0.195711</td>\n",
       "      <td>-1.258788</td>\n",
       "      <td>-0.074261</td>\n",
       "      <td>-3.524118</td>\n",
       "      <td>0.243094</td>\n",
       "      <td>-0.282567</td>\n",
       "      <td>0.518098</td>\n",
       "      <td>1.104262</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.164850</td>\n",
       "      <td>-2.873618</td>\n",
       "      <td>0.817046</td>\n",
       "      <td>1.877096</td>\n",
       "      <td>-1.402977</td>\n",
       "      <td>-0.992778</td>\n",
       "      <td>-0.002622</td>\n",
       "      <td>1.483371</td>\n",
       "      <td>0.380046</td>\n",
       "      <td>0.484612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.754269</td>\n",
       "      <td>-1.133549</td>\n",
       "      <td>-1.814455</td>\n",
       "      <td>0.036195</td>\n",
       "      <td>-0.159687</td>\n",
       "      <td>2.911953</td>\n",
       "      <td>-0.640023</td>\n",
       "      <td>-0.894014</td>\n",
       "      <td>0.440514</td>\n",
       "      <td>0.164435</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.664808</td>\n",
       "      <td>1.762599</td>\n",
       "      <td>-0.071357</td>\n",
       "      <td>3.233031</td>\n",
       "      <td>-0.915575</td>\n",
       "      <td>2.781991</td>\n",
       "      <td>-0.008800</td>\n",
       "      <td>0.508250</td>\n",
       "      <td>-0.724321</td>\n",
       "      <td>-0.320688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-1.054854</td>\n",
       "      <td>0.100548</td>\n",
       "      <td>0.473871</td>\n",
       "      <td>3.140375</td>\n",
       "      <td>-0.449581</td>\n",
       "      <td>2.196428</td>\n",
       "      <td>0.303511</td>\n",
       "      <td>-0.011313</td>\n",
       "      <td>0.375206</td>\n",
       "      <td>-0.866014</td>\n",
       "      <td>...</td>\n",
       "      <td>3.855908</td>\n",
       "      <td>0.273155</td>\n",
       "      <td>-2.094910</td>\n",
       "      <td>-3.537341</td>\n",
       "      <td>1.143077</td>\n",
       "      <td>-2.242801</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.217993</td>\n",
       "      <td>-0.597952</td>\n",
       "      <td>1.817741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dim_1     dim_2     dim_3     dim_4     dim_5     dim_6     dim_7  \\\n",
       "46  -1.675357  0.774387 -2.380979 -0.728630 -1.412328  1.705713 -0.913276   \n",
       "101 -0.058659  0.094974 -0.148768 -1.709858  0.231730  0.003423 -0.203162   \n",
       "175  0.408514 -0.318794 -0.195711 -1.258788 -0.074261 -3.524118  0.243094   \n",
       "9   -2.754269 -1.133549 -1.814455  0.036195 -0.159687  2.911953 -0.640023   \n",
       "136 -1.054854  0.100548  0.473871  3.140375 -0.449581  2.196428  0.303511   \n",
       "\n",
       "        dim_8     dim_9    dim_10  ...   dim_119   dim_120   dim_121  \\\n",
       "46  -0.664502 -0.036643  0.697300  ...  2.785418 -0.100383 -1.418045   \n",
       "101  0.288094 -0.420197 -2.209749  ... -2.907042  0.220606  0.879496   \n",
       "175 -0.282567  0.518098  1.104262  ... -2.164850 -2.873618  0.817046   \n",
       "9   -0.894014  0.440514  0.164435  ... -2.664808  1.762599 -0.071357   \n",
       "136 -0.011313  0.375206 -0.866014  ...  3.855908  0.273155 -2.094910   \n",
       "\n",
       "      dim_122   dim_123   dim_124   dim_125   dim_126   dim_127   dim_128  \n",
       "46  -5.795298 -0.239288 -3.270577  0.005218  2.296032 -0.461153  1.085652  \n",
       "101 -1.250059 -0.911256  2.207671 -0.007707  1.828314 -0.745860 -1.381517  \n",
       "175  1.877096 -1.402977 -0.992778 -0.002622  1.483371  0.380046  0.484612  \n",
       "9    3.233031 -0.915575  2.781991 -0.008800  0.508250 -0.724321 -0.320688  \n",
       "136 -3.537341  1.143077 -2.242801 -0.000889  0.217993 -0.597952  1.817741  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_df.drop(columns=['i', 'name'])\n",
    "# val_data.head()\n",
    "y_val = test_data[label]\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 , 0.548\n",
      "101 , 0.37\n",
      "175 , 0.38\n",
      "9 , 0.431\n",
      "136 , 0.38\n",
      "381 , 0.486\n",
      "239 , 0.435\n",
      "63 , 0.496\n",
      "401 , 0.414\n",
      "238 , 0.314\n",
      "168 , 0.326\n",
      "409 , 0.39\n",
      "97 , 0.361\n",
      "368 , 0.541\n",
      "215 , 0.429\n",
      "200 , 0.483\n",
      "172 , 0.426\n",
      "169 , 0.409\n",
      "282 , 0.369\n",
      "134 , 0.32\n",
      "243 , 0.398\n",
      "145 , 0.331\n",
      "166 , 0.327\n",
      "286 , 0.373\n",
      "6 , 0.431\n",
      "165 , 0.371\n",
      "75 , 0.435\n",
      "146 , 0.336\n",
      "25 , 0.388\n",
      "184 , 0.419\n",
      "12 , 0.445\n",
      "161 , 0.35\n",
      "89 , 0.364\n",
      "362 , 0.413\n",
      "72 , 0.413\n",
      "77 , 0.41\n",
      "147 , 0.32\n",
      "222 , 0.389\n",
      "412 , 0.401\n",
      "110 , 0.347\n",
      "290 , 0.631\n",
      "21 , 0.375\n",
      "178 , 0.436\n",
      "420 , 0.395\n",
      "257 , 0.406\n",
      "304 , 0.357\n",
      "52 , 0.407\n",
      "48 , 0.435\n",
      "177 , 0.363\n",
      "252 , 0.403\n",
      "436 , 0.375\n",
      "260 , 0.374\n",
      "406 , 0.464\n",
      "153 , 0.41\n",
      "281 , 0.394\n",
      "426 , 0.35\n",
      "400 , 0.456\n",
      "174 , 0.395\n",
      "144 , 0.345\n",
      "170 , 0.325\n",
      "192 , 0.339\n",
      "349 , 0.424\n",
      "36 , 0.361\n",
      "242 , 0.414\n",
      "185 , 0.395\n",
      "398 , 0.364\n",
      "241 , 0.309\n",
      "277 , 0.336\n",
      "86 , 0.365\n",
      "408 , 0.446\n",
      "351 , 0.438\n",
      "339 , 0.374\n",
      "417 , 0.446\n",
      "416 , 0.453\n",
      "54 , 0.417\n",
      "188 , 0.38\n",
      "79 , 0.39\n",
      "255 , 0.341\n",
      "411 , 0.355\n",
      "328 , 0.454\n",
      "300 , 0.376\n",
      "196 , 0.428\n",
      "7 , 0.307\n",
      "173 , 0.398\n",
      "5 , 0.372\n",
      "55 , 0.44\n",
      "428 , 0.365\n",
      "334 , 0.398\n"
     ]
    }
   ],
   "source": [
    "test_drags = test_df[\"drag\"]\n",
    "test_index = test_df[\"i\"]\n",
    "for index, drag in zip(test_index, test_drags):\n",
    "    print(index, \",\" ,drag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: root_mean_squared_error on test data: -0.052029913458336574\n",
      "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"root_mean_squared_error\": -0.052029913458336574,\n",
      "    \"mean_squared_error\": -0.002707111894481993,\n",
      "    \"mean_absolute_error\": -0.03948913191665302,\n",
      "    \"r2\": 0.07940497909157618,\n",
      "    \"pearsonr\": 0.3018611278861623,\n",
      "    \"median_absolute_error\": -0.03252647447586057\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%capture log_output\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%config Application.log_level = 'DEBUG'\n",
    "%config IPCompleter.greedy = True\n",
    "\n",
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "for item in y_pred:\n",
    "    print(item)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_val, y_pred=y_pred, auxiliary_metrics=True)\n",
    "print(perf)\n",
    "\n",
    "results = predictor.fit_summary(show_plot=True)\n",
    "print(results)\n",
    "print(predictor.leaderboard(test_data, silent=True))\n",
    "\n",
    "with open('./output_spvae.log', 'w') as f:\n",
    "    f.write(log_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoGluon infers problem type is:  regression\n",
      "AutoGluon identified the following types of features:\n",
      "('float', []) : 64 | ['dim_1', 'dim_2', 'dim_3', 'dim_4', 'dim_5', ...]\n"
     ]
    }
   ],
   "source": [
    "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
    "print(\"AutoGluon identified the following types of features:\")\n",
    "print(predictor.feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37740638852119446\n",
      "0.3842927813529968\n",
      "0.4455946683883667\n",
      "0.43261516094207764\n",
      "0.3714680075645447\n",
      "0.3733121156692505\n",
      "0.4496154189109802\n",
      "0.39431801438331604\n",
      "0.388127863407135\n",
      "0.35969555377960205\n",
      "0.3628774881362915\n",
      "0.49429309368133545\n",
      "0.4561493992805481\n",
      "0.379244327545166\n",
      "0.42710548639297485\n",
      "0.4014332890510559\n",
      "0.3208616375923157\n",
      "0.4093305468559265\n",
      "0.4826950430870056\n",
      "0.36669355630874634\n",
      "0.3920172452926636\n",
      "0.4538956880569458\n",
      "0.3439823389053345\n",
      "0.4421924352645874\n",
      "0.4135732054710388\n",
      "0.4233205318450928\n",
      "0.41290485858917236\n",
      "0.41970711946487427\n",
      "0.3335917592048645\n",
      "0.35587185621261597\n",
      "0.45180416107177734\n",
      "0.41224437952041626\n",
      "0.4739586114883423\n",
      "0.43735337257385254\n",
      "0.35707011818885803\n",
      "0.3750622570514679\n",
      "0.4152054190635681\n",
      "0.397990882396698\n",
      "0.33967575430870056\n",
      "0.3503302037715912\n",
      "0.47259077429771423\n",
      "0.3731432259082794\n",
      "0.3528730273246765\n",
      "0.33467307686805725\n",
      "0.3519686460494995\n",
      "0.41324102878570557\n",
      "0.39402568340301514\n",
      "0.40569254755973816\n",
      "0.3491412401199341\n",
      "0.38379764556884766\n",
      "0.3961864709854126\n",
      "0.3768957853317261\n",
      "0.39923709630966187\n",
      "0.3434215784072876\n",
      "0.40201616287231445\n",
      "0.37392720580101013\n",
      "0.405214786529541\n",
      "0.3567756712436676\n",
      "0.3466331958770752\n",
      "0.37672579288482666\n",
      "0.3949683904647827\n",
      "0.36286699771881104\n",
      "0.43615996837615967\n",
      "0.3462795317173004\n",
      "0.37705761194229126\n",
      "0.38097018003463745\n",
      "0.42683786153793335\n",
      "0.3501471281051636\n",
      "0.3888964056968689\n",
      "0.37436407804489136\n",
      "0.4244721531867981\n",
      "0.3755185008049011\n",
      "0.3793182373046875\n",
      "0.3653848469257355\n",
      "0.4064372777938843\n",
      "0.40489935874938965\n",
      "0.4054079055786133\n",
      "0.4394664764404297\n",
      "0.43068891763687134\n",
      "0.43868565559387207\n",
      "0.4210751950740814\n",
      "0.4777117669582367\n",
      "0.32500123977661133\n",
      "0.33394235372543335\n",
      "0.3630543351173401\n",
      "0.43973684310913086\n",
      "0.3871690630912781\n",
      "0.43707171082496643\n",
      "0.39421719312667847\n",
      "0.36436688899993896\n",
      "0.37196969985961914\n",
      "0.3498333990573883\n",
      "0.41568976640701294\n",
      "0.34838196635246277\n",
      "0.43414032459259033\n",
      "0.35074180364608765\n",
      "0.3775043487548828\n",
      "0.3482573330402374\n",
      "0.47929832339286804\n",
      "0.3733985424041748\n",
      "0.4002353250980377\n",
      "0.3918115794658661\n",
      "0.38469433784484863\n",
      "0.45985591411590576\n",
      "0.421503484249115\n",
      "0.37120428681373596\n",
      "0.3771928548812866\n",
      "0.4410576820373535\n",
      "0.4181640148162842\n",
      "0.44345349073410034\n",
      "0.3327256739139557\n",
      "0.41634833812713623\n",
      "0.3745846748352051\n",
      "0.3916800022125244\n",
      "0.3792892098426819\n",
      "0.3835332989692688\n",
      "0.3343803584575653\n",
      "0.3462773859500885\n",
      "0.4932466149330139\n",
      "0.3733710050582886\n",
      "0.3949023485183716\n",
      "0.4242554306983948\n",
      "0.43246883153915405\n",
      "0.42564019560813904\n",
      "0.43367713689804077\n",
      "0.36592257022857666\n",
      "0.44134750962257385\n",
      "0.4442976713180542\n",
      "0.3815402388572693\n",
      "0.3405877649784088\n",
      "0.3782338500022888\n",
      "0.4524788558483124\n",
      "0.37300437688827515\n",
      "0.39493972063064575\n",
      "0.4076787829399109\n",
      "0.41792213916778564\n",
      "0.39851149916648865\n",
      "0.44244813919067383\n",
      "0.3709551692008972\n",
      "0.36000609397888184\n",
      "0.4443433880805969\n",
      "0.4229828119277954\n",
      "0.38681405782699585\n",
      "0.3848536014556885\n",
      "0.4114118814468384\n",
      "0.36825650930404663\n",
      "0.40025752782821655\n",
      "0.3963700532913208\n",
      "0.45443636178970337\n",
      "0.342620313167572\n",
      "0.4309619665145874\n",
      "0.4398551285266876\n",
      "0.46843671798706055\n",
      "0.43660223484039307\n",
      "0.3742368221282959\n",
      "0.3542701303958893\n",
      "0.3834732174873352\n",
      "0.44848668575286865\n",
      "0.43251192569732666\n",
      "0.370656818151474\n",
      "0.45778006315231323\n",
      "0.35918307304382324\n",
      "0.4339199960231781\n",
      "0.40393030643463135\n",
      "0.3471807837486267\n",
      "0.3569606840610504\n",
      "0.4024215340614319\n",
      "0.4374765157699585\n",
      "0.34797415137290955\n",
      "0.4271467328071594\n",
      "0.36786434054374695\n",
      "0.4100536108016968\n",
      "0.40996384620666504\n",
      "0.4005358815193176\n",
      "0.3903389573097229\n",
      "0.43675875663757324\n",
      "0.45669877529144287\n",
      "0.37764713168144226\n",
      "0.43656933307647705\n",
      "0.41435950994491577\n",
      "0.40908101201057434\n",
      "0.3872683048248291\n",
      "0.391997367143631\n",
      "0.39495396614074707\n",
      "0.4004635214805603\n",
      "0.3682902455329895\n",
      "0.41992872953414917\n",
      "0.3353531062602997\n",
      "0.4398980438709259\n",
      "0.3714018166065216\n",
      "0.4164905250072479\n",
      "0.36525779962539673\n",
      "0.3786798417568207\n",
      "0.47263795137405396\n",
      "0.45956701040267944\n",
      "0.431921124458313\n",
      "0.4066368341445923\n",
      "0.444011390209198\n",
      "0.404479444026947\n",
      "0.4417363405227661\n",
      "0.38943707942962646\n",
      "0.4099881649017334\n",
      "0.3649596571922302\n",
      "0.36403244733810425\n",
      "0.4384969472885132\n",
      "0.3862033188343048\n",
      "0.41557830572128296\n",
      "0.4145755171775818\n",
      "0.40077871084213257\n",
      "0.3859100341796875\n",
      "0.47253847122192383\n",
      "0.3805408477783203\n",
      "0.36619728803634644\n",
      "0.43489593267440796\n",
      "0.3914671838283539\n",
      "0.3701987862586975\n",
      "0.38090384006500244\n",
      "0.4221023917198181\n",
      "0.41805708408355713\n",
      "0.42835545539855957\n",
      "0.4069908857345581\n",
      "0.3354259431362152\n",
      "0.465209424495697\n",
      "0.43593865633010864\n",
      "0.43184471130371094\n",
      "0.3799537718296051\n",
      "0.36720508337020874\n",
      "0.40293312072753906\n",
      "0.38283535838127136\n",
      "0.3774893283843994\n",
      "0.39399394392967224\n",
      "0.3446725606918335\n",
      "0.39784613251686096\n",
      "0.39050814509391785\n",
      "0.36073940992355347\n",
      "0.3475441336631775\n",
      "0.46775344014167786\n",
      "0.3642585277557373\n",
      "0.38605159521102905\n",
      "0.3702511191368103\n",
      "0.3755462169647217\n",
      "0.39174336194992065\n",
      "0.348608136177063\n",
      "0.4007229804992676\n",
      "0.41673141717910767\n",
      "0.39365071058273315\n",
      "0.4044434130191803\n",
      "0.4313429296016693\n",
      "0.405606210231781\n",
      "0.3961673974990845\n",
      "0.3758685290813446\n",
      "0.3973393440246582\n",
      "0.3766830265522003\n",
      "0.4037964940071106\n",
      "0.3970891237258911\n",
      "0.4136608839035034\n",
      "0.3598769009113312\n",
      "0.44403159618377686\n",
      "0.39135709404945374\n",
      "0.37046152353286743\n",
      "0.3487907350063324\n",
      "0.37492841482162476\n",
      "0.44868046045303345\n",
      "0.35437464714050293\n",
      "0.46549421548843384\n",
      "0.39067620038986206\n",
      "0.3560211956501007\n",
      "0.470321387052536\n",
      "0.40891772508621216\n",
      "0.37984034419059753\n",
      "0.46665680408477783\n",
      "0.45594513416290283\n",
      "0.39973723888397217\n",
      "0.352225661277771\n",
      "0.36758163571357727\n",
      "0.4141007661819458\n",
      "0.32347577810287476\n",
      "0.36313074827194214\n",
      "0.4594579339027405\n",
      "0.38474923372268677\n",
      "0.4399018883705139\n",
      "0.3878355026245117\n",
      "0.39532458782196045\n",
      "0.39891308546066284\n",
      "0.3858170211315155\n",
      "0.34263962507247925\n",
      "0.34084552526474\n",
      "0.41611582040786743\n",
      "0.39535754919052124\n",
      "0.49052202701568604\n",
      "0.42071035504341125\n",
      "0.38979625701904297\n",
      "0.3410561680793762\n",
      "0.3906916081905365\n",
      "0.37079504132270813\n",
      "0.43804195523262024\n",
      "0.36407899856567383\n",
      "0.4350264072418213\n",
      "0.36100760102272034\n",
      "0.43035638332366943\n",
      "0.41099607944488525\n",
      "0.3555210530757904\n",
      "0.4134432077407837\n",
      "0.34984517097473145\n",
      "0.34278127551078796\n",
      "0.41506993770599365\n",
      "0.39341625571250916\n",
      "0.36647000908851624\n",
      "0.34743940830230713\n",
      "0.45348024368286133\n",
      "0.48944607377052307\n",
      "0.4709741175174713\n",
      "0.43668437004089355\n",
      "0.38060063123703003\n",
      "0.4719262719154358\n",
      "0.42895495891571045\n",
      "0.42927980422973633\n",
      "0.40947383642196655\n",
      "0.39116907119750977\n",
      "0.39921021461486816\n",
      "0.4770963788032532\n",
      "0.3738735318183899\n",
      "0.3296613097190857\n",
      "0.36965495347976685\n",
      "0.3646514415740967\n",
      "0.44867172837257385\n",
      "0.3898221552371979\n",
      "0.4122314155101776\n",
      "0.33754029870033264\n",
      "0.3799123167991638\n",
      "0.41115251183509827\n",
      "0.3261936902999878\n",
      "0.42806267738342285\n",
      "0.4493267238140106\n",
      "0.4266102612018585\n",
      "0.4519496560096741\n",
      "0.4358862042427063\n",
      "0.44969305396080017\n",
      "0.34385859966278076\n",
      "0.37727582454681396\n",
      "0.3584916293621063\n",
      "0.3774888515472412\n",
      "0.37498340010643005\n",
      "0.4230504035949707\n",
      "0.42995598912239075\n",
      "0.47965145111083984\n",
      "0.34459131956100464\n",
      "0.4188928008079529\n",
      "0.35271525382995605\n",
      "0.3917601704597473\n",
      "0.3591077923774719\n"
     ]
    }
   ],
   "source": [
    "train_data_pred = predictor.predict(train_data, model='WeightedEnsemble_L2')\n",
    "for item in train_data_pred:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37827032804489136\n",
      "0.38917893171310425\n",
      "0.37587660551071167\n",
      "0.44189774990081787\n",
      "0.37875816226005554\n",
      "0.39651840925216675\n",
      "0.40124717354774475\n",
      "0.41791316866874695\n",
      "0.43207114934921265\n",
      "0.3840968906879425\n",
      "0.4002808928489685\n",
      "0.40972644090652466\n",
      "0.3700295090675354\n",
      "0.428059458732605\n",
      "0.3960338234901428\n",
      "0.4286670386791229\n",
      "0.3689090609550476\n",
      "0.4227966070175171\n",
      "0.40865519642829895\n",
      "0.36048954725265503\n",
      "0.43040257692337036\n",
      "0.41173142194747925\n",
      "0.3879980742931366\n",
      "0.38057172298431396\n",
      "0.4181513786315918\n",
      "0.3717370629310608\n",
      "0.41384339332580566\n",
      "0.38663816452026367\n",
      "0.3804423213005066\n",
      "0.3811838626861572\n",
      "0.42607587575912476\n",
      "0.3678780794143677\n",
      "0.40069785714149475\n",
      "0.4320749044418335\n",
      "0.38979214429855347\n",
      "0.37909385561943054\n",
      "0.37798306345939636\n",
      "0.3945416808128357\n",
      "0.38940465450286865\n",
      "0.38216638565063477\n",
      "0.43809235095977783\n",
      "0.3774615526199341\n",
      "0.4280720055103302\n",
      "0.4132728576660156\n",
      "0.4227043390274048\n",
      "0.42129242420196533\n",
      "0.36194470524787903\n",
      "0.3706870377063751\n",
      "0.4311797320842743\n",
      "0.4209950566291809\n",
      "0.3846205472946167\n",
      "0.3554178476333618\n",
      "0.3996891975402832\n",
      "0.3987707495689392\n",
      "0.37210512161254883\n",
      "0.3927472233772278\n",
      "0.3972650170326233\n",
      "0.3840867280960083\n",
      "0.41190260648727417\n",
      "0.3934358060359955\n",
      "0.3988819122314453\n",
      "0.4166586697101593\n",
      "0.36987221240997314\n",
      "0.3922364115715027\n",
      "0.38190850615501404\n",
      "0.4378635883331299\n",
      "0.38589048385620117\n",
      "0.3997667133808136\n",
      "0.37070661783218384\n",
      "0.36433282494544983\n",
      "0.3671126067638397\n",
      "0.3825869560241699\n",
      "0.3742908239364624\n",
      "0.3818489611148834\n",
      "0.37810802459716797\n",
      "0.4297342896461487\n",
      "0.40224289894104004\n",
      "0.37365037202835083\n",
      "0.4019010663032532\n",
      "0.4271131753921509\n",
      "0.4110907316207886\n",
      "0.4226844906806946\n",
      "0.3806142508983612\n",
      "0.3854581415653229\n",
      "0.4313127100467682\n",
      "0.4270803928375244\n",
      "0.3842257261276245\n",
      "0.4226515293121338\n"
     ]
    }
   ],
   "source": [
    "test_data_pred = predictor.predict(test_data, model='WeightedEnsemble_L2')\n",
    "for item in test_data_pred:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
